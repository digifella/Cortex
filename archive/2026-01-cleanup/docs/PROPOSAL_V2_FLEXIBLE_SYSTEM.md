# Proposal System v2.0 - Flexible, Hint-Based Approach

## üéØ The Problem We Solved

### Old System (Rigid)
- ‚úÖ Worked... but only if your template was EXACTLY right
- ‚ùå Required manual insertion of `[INSTRUCTION_TYPE::param]` tags
- ‚ùå Failed if tags were missing or malformed
- ‚ùå No flexibility for different tender formats
- ‚ùå User had to know exact instruction types upfront

**Example of required format:**
```
Section: Technical Approach
[GENERATE_FROM_KB_AND_PROPOSAL]
## Explain our technical methodology

Section: Resources
[GENERATE_RESOURCES]
```

### New System (Flexible)
- ‚úÖ Works with **ANY tender document** structure
- ‚úÖ **Auto-detects** sections, questions, tables
- ‚úÖ User provides **natural language hints**
- ‚úÖ MoE assistance available **anywhere**
- ‚úÖ No tags required!

**Example - just upload the tender as-is:**
```
3. Technical Approach

Please describe your technical methodology for delivering this project.

[User selects this and types: "Answer using our AI capabilities from the knowledge base"]
```

---

## üèóÔ∏è New Architecture

### Core Components

#### 1. **FlexibleTemplateParser**
**Purpose:** Parse any .docx tender document without rigid format requirements.

**What it detects:**
- üìã **Headings:** Traditional heading styles (Heading 1, 2, 3...)
- üî¢ **Numbered sections:** "1.1", "1.2.3", etc.
- ‚ùì **Questions:** Paragraphs starting with "What", "How", "Describe", etc.
- ‚úÖ **Requirements:** Text with "must", "shall", "required"
- üìä **Tables:** Each cell can be a section
- ‚ö†Ô∏è **Blanks/Placeholders:** `[INSERT]`, `TBD`, `___`, etc.

**What it identifies:**
- Status: Empty, Placeholder, Partial, Complete
- Complexity: Simple, Moderate, Complex
- Needs Work: Auto-detected based on content
- Suggested Approach: What AI recommends

**Code:**
```python
from cortex_engine.proposals import FlexibleTemplateParser

# Parse any tender document
parser = FlexibleTemplateParser()
sections = parser.parse_document(doc)

# Get sections that need work
needs_work = parser.get_sections_needing_work()
```

#### 2. **HintBasedAssistant**
**Purpose:** Provide MoE assistance based on user's natural language hints.

**Modes of assistance:**
- `GENERATE_NEW` - Create content from scratch
- `ANSWER_QUESTION` - Answer specific questions
- `REFINE_EXISTING` - Improve what's already there
- `BRAINSTORM` - Creative ideation
- `EXPAND_BRIEF` - Expand brief notes
- `REWRITE_PROFESSIONAL` - Polish content
- `ADD_EVIDENCE` - Add citations/examples

**How it works:**
1. User selects a section
2. User provides hint: *"Answer this question using our AI capabilities and case studies"*
3. System analyzes intent (creativity needed? evidence needed? technical depth?)
4. Routes to optimal model(s) - single or MoE
5. Retrieves relevant KB context
6. Generates content
7. (If MoE) Synthesizes multiple expert outputs

**Code:**
```python
from cortex_engine.proposals import HintBasedAssistant, AssistanceRequest, AssistanceMode

# Create request
request = AssistanceRequest(
    section=section,
    user_hint="Answer using our AI capabilities and provide case studies",
    mode=AssistanceMode.ANSWER_QUESTION,
    use_moe=True,  # Use multiple experts
    creativity=0.7
)

# Get assistance
result = await assistant.assist(request, progress_callback=st.info)

# Use result
if result.success:
    st.write(result.content)
    st.info(f"Models used: {result.models_used}")
```

---

## üöÄ How to Use

### 1. Test the POC

**Start Streamlit:**
```bash
streamlit run pages/Proposal_Copilot_v2_POC.py
```

**Upload a tender document:**
- Any .docx format
- No special tags required
- Can have questions, tables, numbered sections, etc.

**See what happens:**
- System auto-detects all sections
- Shows you what needs work
- Suggests approaches
- Lets you provide hints in natural language

### 2. Try Different Scenarios

#### Scenario A: Tender with Questions
```
Upload tender with sections like:
"3.1 Describe your technical approach to delivering this project"

System detects:
- Type: QUESTION
- Status: EMPTY
- Complexity: COMPLEX
- Suggested: "Answer the question using knowledge base and expertise"

You add hint:
"Use our AI capabilities from the knowledge base, provide 2-3 examples"

Click Generate with MoE
```

#### Scenario B: Tender with Tables
```
Upload tender with resource table:
Role | Name | Experience
_____|______|___________

System detects each cell as section:
- Type: TABLE
- Status: PLACEHOLDER
- Needs Work: TRUE

You add hint for each cell:
"Fill this with our team member details from KB"
```

#### Scenario C: Methodology Section
```
Upload tender with:
"Provide detailed methodology for implementation"

System detects:
- Type: NUMBERED (if numbered like "4.2")
- Status: EMPTY
- Complexity: COMPLEX
- Suggested: "Generate detailed technical approach with MoE"

You add hint:
"Describe agile methodology with AI integration, emphasize innovation"

Click Generate with MoE ‚Üí Uses 2-3 expert models
```

---

## üìä Comparison: Old vs New

### Old System Workflow
```
1. Open template in Word
2. Manually insert [GENERATE_FROM_KB] tags
3. Save template
4. Upload to Proposal Copilot
5. Parse (fails if tags wrong)
6. For each section:
   - Select creativity (green/orange/red)
   - Click Generate
   - Wait (single model, sequential)
7. Assemble document
```

### New System Workflow
```
1. Upload tender as-is (no modifications)
2. System auto-detects sections
3. Review detected sections
4. For sections needing work:
   - Add natural language hint
   - Choose mode (Generate/Answer/Refine/etc.)
   - Optionally enable MoE
   - Adjust creativity
   - Click Generate
5. (Future) Batch generate all sections in parallel
6. Assemble document
```

**Time saved:** ~50-60% reduction in prep work

---

## üîß Technical Details

### FlexibleSection Data Model

```python
@dataclass
class FlexibleSection:
    # Identification
    section_id: str                    # Unique ID
    section_type: SectionType          # HEADING, QUESTION, TABLE, etc.

    # Content
    heading: str                       # Section title
    content: str                       # Current content (may be empty)

    # Context
    parent_heading: Optional[str]      # Parent section
    numbering: Optional[str]           # e.g., "1.2.3"
    level: int                         # Nesting level

    # Status
    status: ContentStatus              # EMPTY, PLACEHOLDER, PARTIAL, COMPLETE
    needs_work: bool                   # Auto-detected

    # User input
    user_hint: Optional[str]           # Natural language guidance
    priority: int                      # 0-5

    # AI metadata
    complexity: str                    # simple/moderate/complex
    suggested_approach: str            # What AI recommends
```

### Pattern Detection

**Placeholder Detection:**
```python
PLACEHOLDER_PATTERNS = [
    r'\[.*?\]',          # [INSERT], [COMPANY NAME]
    r'<.*?>',            # <insert text>
    r'_{3,}',            # _____
    r'\.{3,}',           # .....
    r'\bTBD\b',          # TBD
    r'\bXXX\b',          # XXX
]
```

**Question Detection:**
```python
QUESTION_PATTERNS = [
    r'^\s*(how|what|when|where|why|describe|explain)',
    r'\?$',              # Ends with ?
]
```

**Requirement Detection:**
```python
REQUIREMENT_PATTERNS = [
    r'\b(must|shall|should|required|mandatory)\b',
]
```

### MoE Decision Logic

**When to use MoE:**
1. User explicitly requests it (checkbox)
2. Section complexity is "complex"
3. User hint indicates need (e.g., "comprehensive", "detailed")

**Expert Selection:**
```python
# For complex sections:
Expert 1: Best model (qwen2.5:72b-instruct-q4_K_M)
Expert 2: Balanced model (llama3.3:70b-instruct-q4_K_M)
Expert 3: (Optional) Fast model for efficiency

# Synthesis:
Meta-LLM: Balanced model (mistral-small3.2)
```

---

## üß™ Testing Recommendations

### Test Case 1: Simple Tender (RFP)
**Upload:** 5-section RFP with questions
**Expected:**
- All questions detected
- Status: EMPTY for each
- Suggested approaches accurate
- Single model sufficient for most

### Test Case 2: Complex Tender (Technical)
**Upload:** 15-section technical proposal template
**Expected:**
- Methodology sections marked COMPLEX
- MoE recommended for technical sections
- Numbering correctly extracted
- Parent-child relationships accurate

### Test Case 3: Table-Heavy Tender
**Upload:** Tender with resource tables
**Expected:**
- Each cell detected as section
- Placeholders identified (_____)
- Table context preserved
- Can fill cells individually

### Test Case 4: Mixed Format Tender
**Upload:** Tender with questions + tables + paragraphs
**Expected:**
- All formats detected
- Types correctly classified
- No sections missed
- Suggested approaches appropriate

---

## üìù Next Steps

### Phase 1: POC Validation (Current)
- ‚úÖ Flexible parser built
- ‚úÖ Hint-based assistant designed
- ‚úÖ POC UI created
- ‚è≥ Test with real tenders (YOU DO THIS)
- ‚è≥ Validate auto-detection accuracy

### Phase 2: MoE Integration (Week 1-2)
- Connect HintBasedAssistant to AdaptiveModelManager
- Implement actual KB context building
- Add streaming generation with progress
- Test with real models

### Phase 3: Full Features (Week 3-4)
- Parallel batch generation
- Quality validation
- Document assembly
- Version comparison
- Export with evidence tracking

---

## üêõ Known Limitations (POC)

1. **Not Connected to Real Models:** POC shows UI only, doesn't call LLMs yet
2. **No KB Integration:** Context building stubbed out
3. **No Document Assembly:** Can't replace content in original document yet
4. **No Persistence:** Section edits not saved

These will be implemented in Phase 2-3.

---

## üí° Key Benefits

### For Users
- ‚úÖ **No template prep work** - upload tenders as-is
- ‚úÖ **Natural language control** - say what you want in plain English
- ‚úÖ **Flexible workflow** - work on any section in any order
- ‚úÖ **Quality transparency** - see which models, why, confidence scores

### For System
- ‚úÖ **Any document structure** - no rigid requirements
- ‚úÖ **Intelligent routing** - right model for right task
- ‚úÖ **MoE when needed** - quality where it matters
- ‚úÖ **Extensible** - easy to add new modes/patterns

---

## üîó Related Files

**Core System:**
- `/cortex_engine/proposals/flexible_parser.py` - Auto-detection
- `/cortex_engine/proposals/hint_assistant.py` - MoE assistance
- `/cortex_engine/proposals/__init__.py` - Module exports

**UI:**
- `/pages/Proposal_Copilot_v2_POC.py` - Proof of concept page

**Documentation:**
- This file

**Old System (for reference):**
- `/cortex_engine/instruction_parser.py` - Rigid parser
- `/cortex_engine/task_engine.py` - Old execution engine
- `/pages/Proposal_Copilot.py` - Old UI

---

## ‚ùì FAQ

### Q: Do I need to modify my tender documents?
**A:** No! Upload them exactly as you receive them. The system auto-detects structure.

### Q: What if the system misses a section?
**A:** You can manually add sections (coming in Phase 2). The flexible design allows this.

### Q: Can I use the old `[INSTRUCTION]` tags?
**A:** Yes! The system will detect them as placeholders and offer assistance.

### Q: When should I use MoE vs single model?
**A:** MoE for complex sections (methodology, technical approach). Single model for simple (boilerplate, brief answers).

### Q: How long does generation take?
**A:** Single model: 30-60s. MoE: 2-4 minutes (2-3 experts + synthesis).

### Q: Can I batch generate everything?
**A:** Phase 3 feature - will generate all sections in parallel with dependency resolution.

---

**Ready to test? Run:** `streamlit run pages/Proposal_Copilot_v2_POC.py`
