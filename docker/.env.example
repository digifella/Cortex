# Cortex Suite Environment Configuration
# Copy this file to .env and customize your settings

# =============================================================================
# LLM Provider Configuration
# =============================================================================

# Primary LLM provider: "ollama", "openai", or "gemini"
LLM_PROVIDER=ollama

# Ollama Configuration (for local processing)
OLLAMA_MODEL=mistral:7b-instruct-v0.3-q4_K_M
OLLAMA_BASE_URL=http://localhost:11434

# Required Models for Full Functionality:
# - mistral:7b-instruct-v0.3-q4_K_M (knowledge base operations, research)
# - llava (vision/image processing - install with: ollama pull llava)
# - mistral-small3.2 (proposals - install with: ollama pull mistral-small3.2)

# =============================================================================
# API Keys (Optional - for cloud LLM providers)
# =============================================================================

# OpenAI API Configuration
OPENAI_API_KEY=your_openai_api_key_here

# Gemini API Configuration  
GEMINI_API_KEY=your_gemini_api_key_here

# YouTube API (for research features)
YOUTUBE_API_KEY=your_google_api_key_for_youtube_search

# =============================================================================
# Data Paths
# =============================================================================

# Where to store the AI databases (vector store, knowledge graph)
AI_DATABASE_PATH=/data/ai_databases

# Where your source documents are located
KNOWLEDGE_SOURCE_PATH=/data/knowledge_base

# =============================================================================
# System Configuration
# =============================================================================

# Graphviz executable path (for mind map generation)
GRAPHVIZ_DOT_EXECUTABLE=/usr/bin/dot

# Python environment
PYTHONUNBUFFERED=1
PYTHONDONTWRITEBYTECODE=1

# =============================================================================
# Docker-Specific Configuration
# =============================================================================

# Container timezone
TZ=UTC

# Resource limits (adjust based on your system)
MEMORY_LIMIT=4g
CPU_LIMIT=2

# Enable GPU support for Ollama (requires NVIDIA Docker runtime)
ENABLE_GPU=false

# =============================================================================
# Application Settings
# =============================================================================

# Default document ingestion settings
DEFAULT_BATCH_SIZE=250
DEFAULT_CHUNK_SIZE=1000

# UI Configuration
STREAMLIT_SERVER_PORT=8501
STREAMLIT_SERVER_ADDRESS=0.0.0.0

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000

# =============================================================================
# Logging and Debug
# =============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# Enable debug mode
DEBUG=false

# =============================================================================
# Volume Mounts (Automatic Drive Detection + Custom Paths)
# =============================================================================

# ‚úÖ AUTOMATIC MOUNTING (No setup required):
# The setup scripts automatically detect and mount common directories:
#
# Windows: C:\Users, D:\, E:\, F:\ drives ‚Üí /mnt/c, /mnt/d, /mnt/e, /mnt/f
# Linux: $HOME directory ‚Üí /home/host_user  
# macOS: $HOME directory ‚Üí /home/host_user
# WSL2: /mnt/c, /mnt/d, /mnt/e, /mnt/f ‚Üí same paths in container

# üìÅ USING YOUR PATHS IN CORTEX SUITE:
# When the interface asks for "Root Source Documents Path", you can enter:
# - Windows: E:\KB_Test ‚Üí automatically available as /mnt/e/KB_Test
# - Windows: C:\Users\YourName\Documents ‚Üí available as /mnt/c/Users/YourName/Documents
# - Linux/Mac: /home/username/docs ‚Üí available as /home/host_user/docs

# üîß CUSTOM MOUNT PATHS (Optional):
# Add additional paths here if needed:

# Windows Examples:
# HOST_DOCUMENTS_PATH=C:\Users\YourUsername\Documents
# HOST_DOWNLOADS_PATH=C:\Users\YourUsername\Downloads  
# HOST_PROJECTS_PATH=E:\Development\Projects

# Linux/Mac Examples:
# HOST_DOCUMENTS_PATH=/home/username/Documents
# HOST_DOWNLOADS_PATH=/home/username/Downloads
# HOST_PROJECTS_PATH=/opt/projects

# WSL2 Examples:
# HOST_DOCUMENTS_PATH=/mnt/c/Users/YourUsername/Documents
# HOST_ONEDRIVE_PATH=/mnt/c/Users/YourUsername/OneDrive
# HOST_EXTERNAL_DRIVE=/mnt/e/ExternalDrive

# üí° TROUBLESHOOTING TIPS:
# - All mounts are read-only for security
# - If a path doesn't show up, check it exists on your host system
# - Windows users: Use either forward slashes (/) or backslashes (\)
# - Case matters on Linux/Mac, but not on Windows

# =============================================================================
# Security (Production Deployments)
# =============================================================================

# JWT Secret (generate with: openssl rand -base64 32)
JWT_SECRET=your-secure-jwt-secret-here

# Database encryption key
DB_ENCRYPTION_KEY=your-encryption-key-here

# Enable HTTPS
ENABLE_HTTPS=false

# SSL Certificate paths (if ENABLE_HTTPS=true)
SSL_CERT_PATH=/ssl/cert.pem
SSL_KEY_PATH=/ssl/key.pem

# =============================================================================
# Hybrid Model Architecture Configuration (v3.0.0)
# =============================================================================

# Model distribution strategy
MODEL_DISTRIBUTION_STRATEGY=hybrid_docker_preferred

# Deployment environment (affects model selection)
DEPLOYMENT_ENV=production

# Docker Model Runner configuration
DOCKER_MODEL_REGISTRY=docker.io/ai
ENABLE_DOCKER_MODELS=true

# Ollama configuration (fallback/compatibility)
ENABLE_OLLAMA_MODELS=true

# =============================================================================
# Model Setup Instructions
# =============================================================================

# Cortex Suite v3.0.0 supports hybrid model distribution:
#
# HYBRID SETUP (Recommended):
# - Run: ./run-cortex-hybrid.sh
# - Automatically configures both Docker Model Runner and Ollama
# - Provides 15% faster inference with enterprise features
# - Automatic fallback for maximum reliability
#
# TRADITIONAL SETUP:
# - Run: ./run-cortex.sh  
# - Uses Ollama only (proven and stable)
#
# ENTERPRISE SETUP:
# - Run: ./run-cortex-hybrid.sh --profile enterprise
# - Docker Model Runner only (requires Docker Model Runner)
#
# Model Installation:
# 1. Essential models are installed automatically during setup
# 2. Use the Setup Wizard for guided configuration
# 3. Visit http://localhost:8501/0_Setup_Wizard for first-time setup
#
# Required Models (auto-installed):
# - mistral:7b-instruct-v0.3-q4_K_M (general AI tasks)
# - mistral-small3.2 (enhanced proposals and analysis)
#
# Optional Models:
# - llava (image analysis - 4.5GB)
# - codellama (code generation - 3.8GB)