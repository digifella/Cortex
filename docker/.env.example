# Cortex Suite Environment Configuration
# Copy this file to .env and customize your settings

# =============================================================================
# LLM Provider Configuration
# =============================================================================

# Primary LLM provider: "ollama", "openai", or "gemini"
LLM_PROVIDER=ollama

# Ollama Configuration (for local processing)
OLLAMA_MODEL=mistral:7b-instruct-v0.3-q4_K_M
OLLAMA_BASE_URL=http://localhost:11434

# Required Models for Full Functionality:
# - mistral:7b-instruct-v0.3-q4_K_M (knowledge base operations, research)
# - llava (vision/image processing - install with: ollama pull llava)
# - mistral-small3.2 (proposals - install with: ollama pull mistral-small3.2)

# =============================================================================
# API Keys (Optional - for cloud LLM providers)
# =============================================================================

# OpenAI API Configuration
OPENAI_API_KEY=your_openai_api_key_here

# Gemini API Configuration  
GEMINI_API_KEY=your_gemini_api_key_here

# YouTube API (for research features)
YOUTUBE_API_KEY=your_google_api_key_for_youtube_search

# =============================================================================
# Data Paths
# =============================================================================

# Where to store the AI databases (vector store, knowledge graph)
AI_DATABASE_PATH=/data/ai_databases

# Where your source documents are located
KNOWLEDGE_SOURCE_PATH=/data/knowledge_base

# =============================================================================
# System Configuration
# =============================================================================

# Graphviz executable path (for mind map generation)
GRAPHVIZ_DOT_EXECUTABLE=/usr/bin/dot

# Python environment
PYTHONUNBUFFERED=1
PYTHONDONTWRITEBYTECODE=1

# =============================================================================
# Docker-Specific Configuration
# =============================================================================

# Container timezone
TZ=UTC

# Resource limits (adjust based on your system)
MEMORY_LIMIT=4g
CPU_LIMIT=2

# Enable GPU support for Ollama (requires NVIDIA Docker runtime)
ENABLE_GPU=false

# =============================================================================
# Application Settings
# =============================================================================

# Default document ingestion settings
DEFAULT_BATCH_SIZE=250
DEFAULT_CHUNK_SIZE=1000

# UI Configuration
STREAMLIT_SERVER_PORT=8501
STREAMLIT_SERVER_ADDRESS=0.0.0.0

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000

# Host port mappings for Docker compose (change if ports are already used)
CORTEX_UI_PORT=8501
CORTEX_API_PORT=8000
CORTEX_OLLAMA_PORT=11434

# =============================================================================
# Qwen3-VL Multimodal Embeddings (GPU Required)
# =============================================================================

# Enable Qwen3-VL for multimodal embeddings (text + images in same vector space)
# Requires: transformers>=4.57.0, ~5-16GB VRAM depending on model size
QWEN3_VL_ENABLED=false

# Model size: auto (based on available VRAM), 2B (~5GB), or 8B (~16GB)
QWEN3_VL_MODEL_SIZE=auto

# Neural reranker for improved search precision (uses additional VRAM)
QWEN3_VL_RERANKER_ENABLED=false
QWEN3_VL_RERANKER_SIZE=auto
QWEN3_VL_RERANKER_TOP_K=20
QWEN3_VL_RERANKER_CANDIDATES=50

# Flash Attention 2 for memory efficiency (recommended if available)
QWEN3_VL_USE_FLASH_ATTENTION=true

# Batch sizes (reduce if running out of VRAM)
QWEN3_VL_EMBED_BATCH_SIZE=8
QWEN3_VL_RERANK_BATCH_SIZE=4

# =============================================================================
# Logging and Debug
# =============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# Enable debug mode
DEBUG=false

# =============================================================================
# STORAGE MODE CONFIGURATION
# =============================================================================
#
# Cortex Suite supports two storage modes:
#
# ============================================================================
# MODE 1: PORTABLE (Default) - Fully Containerized
# ============================================================================
# All data stored inside Docker volumes - fully transportable!
#
# Start with:
#   docker compose up -d
#
# Benefits:
#   - Zero configuration required
#   - Export entire setup: docker save + volume export
#   - Works identically on any system
#   - No path issues between Windows/Linux/Mac
#
# Backup portable volumes:
#   docker run --rm -v cortex_ai_databases:/data -v $(pwd):/backup alpine tar czf /backup/cortex_backup.tar.gz /data
#
# Restore portable volumes:
#   docker run --rm -v cortex_ai_databases:/data -v $(pwd):/backup alpine tar xzf /backup/cortex_backup.tar.gz -C /
#
# ============================================================================
# MODE 2: EXTERNAL STORAGE - Host Filesystem
# ============================================================================
# Data stored on your host filesystem for easy access/backup.
#
# Start with:
#   docker compose -f docker-compose.yml -f docker-compose.external.yml up -d
#
# Or use helper scripts:
#   Windows: run-compose.bat --external
#   Linux/Mac: ./run-compose.sh --external
#
# REQUIRED: Uncomment and set these paths for external mode:
# EXTERNAL_AI_DATABASE_PATH=C:\ai_databases
# EXTERNAL_KNOWLEDGE_PATH=C:\KB_Test
#
# WSL2/Linux paths:
# EXTERNAL_AI_DATABASE_PATH=/mnt/f/ai_databases
# EXTERNAL_KNOWLEDGE_PATH=/mnt/e/KB_Test
#
# Benefits:
#   - Direct file access from host OS
#   - Easy manual backups (just copy folders)
#   - Use existing databases from previous installations
#   - External tools can access the data
#
# ============================================================================

# =============================================================================
# Volume Mounts (Additional Paths for Knowledge Source)
# =============================================================================

# ðŸ’¡ TROUBLESHOOTING TIPS:
# - Knowledge source mounts are read-only for security
# - If a path doesn't show up, check it exists on your host system
# - Windows users: Use either forward slashes (/) or backslashes (\)
# - Case matters on Linux/Mac, but not on Windows

# =============================================================================
# Security (Production Deployments)
# =============================================================================

# JWT Secret (generate with: openssl rand -base64 32)
JWT_SECRET=your-secure-jwt-secret-here

# Database encryption key
DB_ENCRYPTION_KEY=your-encryption-key-here

# Enable HTTPS
ENABLE_HTTPS=false

# SSL Certificate paths (if ENABLE_HTTPS=true)
SSL_CERT_PATH=/ssl/cert.pem
SSL_KEY_PATH=/ssl/key.pem

# =============================================================================
# Hybrid Model Architecture Configuration (v3.0.0)
# =============================================================================

# Model distribution strategy
MODEL_DISTRIBUTION_STRATEGY=hybrid_docker_preferred

# Deployment environment (affects model selection)
DEPLOYMENT_ENV=production

# Docker Model Runner configuration
DOCKER_MODEL_REGISTRY=docker.io/ai
ENABLE_DOCKER_MODELS=true

# Ollama configuration (fallback/compatibility)
ENABLE_OLLAMA_MODELS=true

# =============================================================================
# Model Setup Instructions
# =============================================================================

# Cortex Suite Docker launcher:
# - Run: ./run-compose.sh (Linux/Mac)
# - Run: run-compose.bat (Windows)
# - Use --external for host-bound storage mode
#
# Model Installation:
# 1. Essential models are installed automatically during setup
# 2. Use the Setup Wizard for guided configuration
# 3. Visit http://localhost:8501/0_Setup_Wizard for first-time setup
#
# Required Models (auto-installed):
# - mistral:7b-instruct-v0.3-q4_K_M (general AI tasks)
# - mistral-small3.2 (enhanced proposals and analysis)
#
# Optional Models:
# - llava (image analysis - 4.5GB)
# - codellama (code generation - 3.8GB)
