# Cortex Suite Environment Configuration
# Copy this file to .env and customize your settings

# =============================================================================
# LLM Provider Configuration
# =============================================================================

# Primary LLM provider: "ollama", "openai", or "gemini"
LLM_PROVIDER=ollama

# Ollama Configuration (for local processing)
OLLAMA_MODEL=mistral:7b-instruct-v0.3-q4_K_M
OLLAMA_BASE_URL=http://localhost:11434

# Required Models for Full Functionality:
# - mistral:7b-instruct-v0.3-q4_K_M (knowledge base operations, research)
# - llava (vision/image processing - install with: ollama pull llava)
# - mistral-small3.2 (proposals - install with: ollama pull mistral-small3.2)

# =============================================================================
# API Keys (Optional - for cloud LLM providers)
# =============================================================================

# OpenAI API Configuration
OPENAI_API_KEY=your_openai_api_key_here

# Gemini API Configuration  
GEMINI_API_KEY=your_gemini_api_key_here

# YouTube API (for research features)
YOUTUBE_API_KEY=your_google_api_key_for_youtube_search

# =============================================================================
# Data Paths
# =============================================================================

# Where to store the AI databases (vector store, knowledge graph)
AI_DATABASE_PATH=/data/ai_databases

# Where your source documents are located
KNOWLEDGE_SOURCE_PATH=/data/knowledge_base

# =============================================================================
# System Configuration
# =============================================================================

# Graphviz executable path (for mind map generation)
GRAPHVIZ_DOT_EXECUTABLE=/usr/bin/dot

# Python environment
PYTHONUNBUFFERED=1
PYTHONDONTWRITEBYTECODE=1

# =============================================================================
# Docker-Specific Configuration
# =============================================================================

# Container timezone
TZ=UTC

# Resource limits (adjust based on your system)
MEMORY_LIMIT=4g
CPU_LIMIT=2

# Enable GPU support for Ollama (requires NVIDIA Docker runtime)
ENABLE_GPU=false

# =============================================================================
# Application Settings
# =============================================================================

# Default document ingestion settings
DEFAULT_BATCH_SIZE=250
DEFAULT_CHUNK_SIZE=1000

# UI Configuration
STREAMLIT_SERVER_PORT=8501
STREAMLIT_SERVER_ADDRESS=0.0.0.0

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000

# =============================================================================
# Logging and Debug
# =============================================================================

# Log level: DEBUG, INFO, WARNING, ERROR
LOG_LEVEL=INFO

# Enable debug mode
DEBUG=false

# =============================================================================
# Volume Mounts (Customize for your system)
# =============================================================================

# Example paths - uncomment and modify for your system:

# Windows Examples:
# HOST_DOCUMENTS_PATH=C:\Users\YourUsername\Documents
# HOST_DOWNLOADS_PATH=C:\Users\YourUsername\Downloads

# Linux/Mac Examples:
# HOST_DOCUMENTS_PATH=/home/username/Documents
# HOST_DOWNLOADS_PATH=/home/username/Downloads

# WSL2 Examples:
# HOST_DOCUMENTS_PATH=/mnt/c/Users/YourUsername/Documents
# HOST_ONEDRIVE_PATH=/mnt/c/Users/YourUsername/OneDrive

# =============================================================================
# Security (Production Deployments)
# =============================================================================

# JWT Secret (generate with: openssl rand -base64 32)
JWT_SECRET=your-secure-jwt-secret-here

# Database encryption key
DB_ENCRYPTION_KEY=your-encryption-key-here

# Enable HTTPS
ENABLE_HTTPS=false

# SSL Certificate paths (if ENABLE_HTTPS=true)
SSL_CERT_PATH=/ssl/cert.pem
SSL_KEY_PATH=/ssl/key.pem

# =============================================================================
# Model Setup Instructions
# =============================================================================

# After starting Cortex Suite, you may need to install additional models:
#
# 1. For image processing (optional but recommended):
#    docker exec cortex-ollama ollama pull llava
#
# 2. For better proposal generation:
#    docker exec cortex-ollama ollama pull mistral-small3.2
#
# 3. Check model status in Cortex Suite main page sidebar
#
# Note: The system will automatically detect missing models and provide
# installation instructions in the UI.