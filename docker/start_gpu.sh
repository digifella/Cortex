#!/bin/bash
set -e

echo "üöÄ Starting Cortex Suite (GPU-Enabled)"
python -c "from cortex_engine.version_config import VERSION_DISPLAY; print('Version:', VERSION_DISPLAY)"
echo "üìÖ $(date)"
echo "üíª Platform: $(uname -m) ($(uname -s))"
echo "üê≥ Docker Environment: $([ -f /.dockerenv ] && echo 'Yes' || echo 'No')"
echo "üéÆ GPU Support: CUDA-Enabled"
echo ""

# Prefer host-mounted volumes when present
PREFERRED_AI_DB="/data/ai_databases"
if [ -d "$PREFERRED_AI_DB" ]; then
    echo "üìÇ Detected external AI database mount at $PREFERRED_AI_DB"
    export AI_DATABASE_PATH="$PREFERRED_AI_DB"
else
    echo "üìÇ Using internal AI database path at $AI_DATABASE_PATH"
    mkdir -p "$AI_DATABASE_PATH"
fi

PREFERRED_SOURCE="/data/knowledge_base"
if [ -d "$PREFERRED_SOURCE" ]; then
    echo "üìÇ Detected external knowledge source mount at $PREFERRED_SOURCE"
    export KNOWLEDGE_SOURCE_PATH="$PREFERRED_SOURCE"
else
    echo "üìÇ Using internal knowledge source path at $KNOWLEDGE_SOURCE_PATH"
    mkdir -p "$KNOWLEDGE_SOURCE_PATH"
fi

# Verify GPU access
echo "üîç Checking GPU availability..."
python -c "import torch; avail=torch.cuda.is_available(); print('CUDA available:', avail); count=torch.cuda.device_count() if avail else 0; print('GPU count:', count)"
echo ""

echo "ü§ñ Starting Ollama service..."
export OLLAMA_HOST=0.0.0.0:11434
export OLLAMA_ORIGINS=*
ollama serve &
OLLAMA_PID=$!

echo "‚è≥ Waiting for Ollama to initialize..."
for i in {1..60}; do
    if curl -s http://localhost:11434/api/tags >/dev/null 2>&1; then
        echo "‚úÖ Ollama is ready!"
        break
    fi
    if [ $i -eq 60 ]; then
        echo "‚ö†Ô∏è Ollama taking longer than expected, continuing..."
        break
    fi
    echo "   ... attempt $i/60"
    sleep 3
done

echo "üîó Starting API server..."
uvicorn api.main:app --host 0.0.0.0 --port 8000 --workers 1 &
API_PID=$!

echo "üñ•Ô∏è Starting Streamlit UI..."
streamlit run Cortex_Suite.py --server.port 8501 --server.address 0.0.0.0 --server.enableCORS false --server.enableXsrfProtection false &
STREAMLIT_PID=$!

echo "‚è≥ Waiting for services to start..."
sleep 10

echo ""
echo "üéâ Cortex Suite (GPU-Enabled) is now accessible!"
echo "üåê Access at: http://localhost:8501"
echo "üîó API docs: http://localhost:8000/docs"
echo "üéÆ GPU Acceleration: ENABLED"
echo ""

{
    echo "üì¶ Starting AI model downloads..."
    if ! ollama list 2>/dev/null | grep -q "mistral:latest"; then
        echo "‚¨áÔ∏è Downloading Mistral model (4.4GB)..."
        ollama pull mistral:latest
        echo "‚úÖ Mistral model ready!"
    fi
    
    if ! ollama list 2>/dev/null | grep -q "mistral-small3.2"; then
        echo "‚¨áÔ∏è Downloading Mistral Small model (15GB)..."
        ollama pull mistral-small3.2
        echo "‚úÖ Mistral Small model ready!"
    fi
    
    echo "üéØ All AI models are now ready!"
    echo "üöÄ Full functionality (with GPU) is now available at http://localhost:8501"
} &
MODEL_DOWNLOAD_PID=$!

cleanup() {
    echo ""
    echo "üõë Shutting down Cortex Suite..."
    kill $OLLAMA_PID $API_PID $STREAMLIT_PID $MODEL_DOWNLOAD_PID 2>/dev/null || true
    echo "‚úÖ Shutdown complete"
    exit 0
}

trap cleanup SIGTERM SIGINT

while true; do
    if ! kill -0 $STREAMLIT_PID 2>/dev/null; then
        echo "‚ùå Streamlit process died, restarting..."
        streamlit run Cortex_Suite.py --server.port 8501 --server.address 0.0.0.0 --server.enableCORS false --server.enableXsrfProtection false &
        STREAMLIT_PID=$!
    fi
    
    if ! kill -0 $API_PID 2>/dev/null; then
        echo "‚ùå API process died, restarting..."
        uvicorn api.main:app --host 0.0.0.0 --port 8000 --workers 1 &
        API_PID=$!
    fi
    
    if ! kill -0 $OLLAMA_PID 2>/dev/null; then
        echo "‚ùå Ollama process died, restarting..."
        ollama serve &
        OLLAMA_PID=$!
    fi
    
    sleep 30
done
