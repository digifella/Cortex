--- FILE: README.md ---
# **`README.md`**

**Version:** 94.0.0 (GraphRAG Integration Release)
**Date:** 2025-07-23

**Prelim:**
Please be aware that the system operates in a WSL2 environment, so all paths should support linux and windows.

### 1. System Status: GraphRAG Integration Complete

This document marks a major milestone with the **Project Cortex Suite** now featuring integrated knowledge graph capabilities. The system can extract entities (people, organizations, projects) and their relationships during ingestion, enabling powerful relationship-based queries.

*   **[NEW] GraphRAG Integration: Entity and Relationship Extraction**
    *   **Status:** The ingestion pipeline now automatically extracts consultants, clients, projects, and their relationships using spaCy NER and pattern matching. This data is stored in a NetworkX graph alongside the vector embeddings.
    *   **Capabilities:** The system can now answer queries like "What projects did consultant X work on?", "Who collaborated with person Y?", and "What types of work has client Z requested?"

*   **[RESOLVED] Knowledge Search: Combined Metadata and Collection Scope Search**
    *   **Status:** Fixed. The search logic constructs native ChromaDB `where` clauses directly.

*   **[RESOLVED] AI Research: Sources Found During Foundational Search**
    *   **Status:** Fixed. The foundational query functions work correctly.

*   **[RESOLVED] AI Research: Correct UI Step Numbering**
    *   **Status:** Fixed. UI uses centralized dictionary for step headers.

*   **[RESOLVED] AI Research: Citations in Deep Research Report**
    *   **Status:** Fixed. Enhanced prompts ensure proper citation formatting.

### 2. MANDATORY: Database Migration (v70.0.0+)

**To resolve critical stability issues and pathing errors, the database structure was updated. If you are upgrading from a version prior to 70.0.0, you MUST perform the following one-time action before first use:**

1.  Navigate to your primary AI databases folder (e.g., `/mnt/f/ai_databases/`).
2.  **Permanently delete the entire `knowledge_hub_db` directory.**
3.  The system will automatically recreate this directory with the correct, stable format the next time you run the ingestion process.

### 3. Usage & Licensing Disclaimer

**This system is developed for private, non-commercial use only.** All components are integrated under the assumption of personal, fair-use for research and development on a local machine.

### 4. Introduction

This document provides a comprehensive overview of the **Project Cortex Suite**. The suite is an integrated workbench designed to build a high-quality, human-verified knowledge base with graph-based relationship tracking and use it to produce AI-assisted proposals.

### 5. System Vision: The Knowledge Graph Workflow

The Cortex Suite now incorporates a knowledge graph that captures relationships between people, organizations, projects, and documents. This enables relationship-based queries and enhanced context understanding.

```mermaid
graph TD
    subgraph "Phase 1: Knowledge Base Curation with Graph"
        Z[<b>AI Assisted Research</b><br/>Multi-agent 'Double Diamond' research] --> Y[External Research<br>Folder];
        Y --> A;
        A[<b>Knowledge Ingest UI</b><br/>3-stage ingestion with<br/>entity & relationship extraction] --> H((Main Knowledge Base<br/>ChromaDB + Graph));
        A --> G((Knowledge Graph<br/>NetworkX));
        I[<b>Knowledge Search UI</b><br/>Vector + Graph queries] -->|Searches & Prunes| H;
        I -->|Leverages relationships| G;
        I -->|Curates search results into| J[<b>Working Collections</b>];
        J --> K[<b>Collection Management UI</b>];
    end

    subgraph "Phase 2: Proposal Lifecycle Management"
        subgraph "2a: Template Preparation"
        T_IN[Raw Document<br>e.g., Client RFP .docx] --> T[<b>Proposal Step 1 Prep UI</b>]
        T --> T_OUT[<b>Tagged Template .docx</b>]
        end

        subgraph "2b: Content Generation"
        L[<b>Proposal Step 2 Make UI</b>] -->|Creates/Loads| M[<b>Proposal State</b>];
        T_OUT --> N[<b>Proposal Co-pilot UI</b><br/>Uses knowledge + graph context]
        M --> N
        N -->|Selects & Uses| J;
        N -->|Generates from| H;
        N -->|Context from| G;
        N --> O[‚úÖ Final Proposal Document];
        end
    end

### 6. Installation & Operation

**1. System-Level Dependencies (for Mind Maps):**
The AI Research Assistant requires the Graphviz system command `dot` to be installed and accessible in the system's PATH.
-   **Debian/Ubuntu:** `sudo apt-get install graphviz`
-   **MacOS (Homebrew):** `brew install graphviz`
-   **Windows:** Download from the official site and add the `bin` directory to your system's `PATH`.

**2. Python Environment (Python 3.11 Required):**
This system is stabilized on Python 3.11. If you are using a different version, you must set up a 3.11 environment.
```bash
# For Ubuntu users, if 3.11 is not available:
sudo add-apt-repository ppa:deadsnakes/ppa
sudo apt update
sudo apt install python3.11 python3.11-venv

# Create and activate the virtual environment
python3.11 -m venv venv
source venv/bin/activate
```

**3. Install Python Dependencies:**
```bash
pip install -r requirements.txt```
# Download spaCy language model for entity extraction
python -m spacy download en_core_web_sm

**4. Set Up Environment Variables:**
Create a `.env` file in the project root with your API keys.```# .env file
# For the AI Research module, choose 'openai', 'ollama', or 'gemini'.
# Other modules use the settings defined in their respective files.
# .env file
LLM_PROVIDER="gemini"  # or "ollama" or "openai"
OLLAMA_MODEL="mistral:7b-instruct-v0.3-q4_K_M"
OPENAI_API_KEY="your_openai_api_key_here"
GEMINI_API_KEY="your_gemini_api_key_here"
YOUTUBE_API_KEY="your_google_api_key_for_youtube_search"
GRAPHVIZ_DOT_EXECUTABLE="/usr/bin/dot"

# (Optional) Explicit path to Graphviz 'dot' executable to help resolve mind map issues.
# Find this path by running `which dot` in your Ubuntu/WSL terminal.
GRAPHVIZ_DOT_EXECUTABLE="/usr/bin/dot"
```

**5. Run the Streamlit Application:**```bash
streamlit run Cortex_Suite.py```

### 7. Version History & Future Roadmap

Entity Extraction
During ingestion, the system automatically identifies:

People: Consultants, authors, team members
Organizations: Clients, partners, vendors
Projects: From document content and thematic tags
Documents: All ingested files with metadata

Relationship Mapping
The system tracks relationships such as:

authored: Person ‚Üí Document
client_of: Organization ‚Üí Document
collaborated_with: Person ‚Üî Person
mentioned_in: Entity ‚Üí Document
documented_in: Project ‚Üí Document

Graph Queries (In Development)

Find all projects a consultant worked on
Identify collaborators on documents
Track work done for specific clients
Analyze document type preferences by client

8. Version History & Future Roadmap
Completed Sprints

Sprint 1-20.5: Core system development through search bugfix. ‚úÖ
Sprint 21: Multi-Modal Knowledge Ingestion (Images) - VLM integration for image description. ‚úÖ
Sprint 22: GraphRAG Foundation - Entity extraction and knowledge graph building during ingestion. ‚úÖ Done

In Progress

Sprint 23: Graph-Enhanced Retrieval - Implement hybrid vector + graph search queries. üìù Next Up

Planned Sprints

Sprint 24: Graph Visualization - Interactive graph exploration UI
Sprint 25: Advanced Graph Queries - Complex relationship traversal
Sprint 26: Agentic Tool Use - [GENERATE_TABLE_FROM_KB] with graph context
Sprint 27: Full Co-pilot Upgrade with graph-aware generation

9. Troubleshooting
spaCy Model Download Issues
If you encounter errors with spaCy, ensure the model is downloaded:
bashpython -m spacy download en_core_web_sm
MuPDF Color Profile Warnings
You may see warnings like MuPDF error: format error: cmsOpenProfileFromMem failed. These are harmless and relate to PDF color profiles. Text extraction continues normally.
Inspecting the Knowledge Graph
Use the cortex_inspector tool to examine graph contents:
bashpython scripts/cortex_inspector.py --db-path /mnt/f/ai_databases --stats

10. Logs and Data Locations

Ingestion Log: logs/ingestion.log - Detailed processing information
Ingested Files Log: <db_path>/knowledge_hub_db/ingested_files.log - List of processed files
Knowledge Graph: <db_path>/knowledge_cortex.gpickle - NetworkX graph with entities and relationships
Vector Store: <db_path>/knowledge_hub_db/ - ChromaDB embeddings

11. Core Components Update

cortex_engine/entity_extractor.py: NEW - Extracts people, organizations, and projects using spaCy NER and pattern matching
cortex_engine/graph_manager.py: ENHANCED - Now EnhancedGraphManager with relationship queries
cortex_engine/ingest_cortex.py: v13.0.0 - Integrated entity extraction during analysis phase
cortex_engine/graph_query.py: NEW (Coming in Sprint 23) - Hybrid vector + graph search

12. Dependencies Update
Key additions to requirements.txt:

spacy>=3.5.0,<3.8.0 - For entity extraction
numpy<2.0.0,>=1.22.5 - Pinned for compatibility

The system now builds a comprehensive knowledge graph during ingestion, laying the foundation for powerful relationship-based queries and enhanced context understanding in proposal generation.

#### **Handling `:Zone.Identifier` Files in WSL**

If you see files ending with `:Zone.Identifier`, these are harmless metadata artifacts from Windows. You can safely remove all of them from your project by running the following command from your `cortex_suite` root directory in your WSL terminal:

```bash
find . -type f -name "*:Zone.Identifier" -delete
```

This command finds all files (`-type f`) whose names (`-name`) end with `":Zone.Identifier"` and deletes them.

### 9. Final Code Manifest

-   `Cortex_Suite.py`: Main entrypoint for the unified Streamlit application.
-   `requirements.txt`: Frozen Python dependencies for a stable Python 3.11 environment.
-   `.env`: For storing API keys and optional executable paths.
-   `.gitignore`: Specifies which files and directories to ignore for version control.
-   `boilerplate.json`: Stores reusable boilerplate text snippets.
-   `cortex_config.json`: File to store last-used paths and other persistent settings.
-   `working_collections.json`: Stores user-curated document collections.
-   `staging_ingestion.json`: A temporary file holding AI-suggested metadata for user review.

-   **`cortex_engine/`**: The core backend logic of the application.
    -   `__init__.py`: Makes the engine a Python package.
    -   `boilerplate_manager.py`: Manages boilerplate text snippets.
    -   `collection_manager.py`: Handles CRUD operations for Working Collections.
    -   `config.py`: Central configuration for paths, models, and default settings.
    -   `config_manager.py`: Manages persistent user settings file (`cortex_config.json`).
    -   `graph_extraction_worker.py`: Subprocess worker for knowledge graph extraction.
    -   `graph_manager.py`: Manages the knowledge graph file.
    -   `ingest_cortex.py`: Core ingestion script with validation and progress reporting.
    -   `instruction_parser.py`: Parses `.docx` files for Cortex instructions.
    -   `proposal_manager.py`: Manages the lifecycle of proposals.
    -   `query_cortex.py`: Provides models and prompts for querying the knowledge base.
    -   `session_state.py`: Manages Streamlit session state.
    -   `synthesise.py`: Backend for the AI Assisted Research agent.
    -   `task_engine.py`: Backend for AI task execution in the Proposal Co-pilot.
    -   `utils.py`: Shared, low-dependency helper functions.

-   **`pages/`**: The individual Streamlit UI pages.
    -   `1_AI_Assisted_Research.py`: UI for the multi-agent research engine.
    -   `2_Knowledge_Ingest.py`: UI for the three-stage document ingestion process.
    -   `3_Knowledge_Search.py`: UI for searching the knowledge base. Includes a robust fix for complex filter combinations.
    -   `4_Collection_Management.py`: UI for managing Working Collections.
    -   `5_Proposal_Step_1_Prep.py`: UI for creating proposal templates.
    -   `6_Proposal_Step_2_Make.py`: UI for creating and loading proposals.
    -   `Proposal_Copilot.py`: The core UI for drafting proposals.

-   **`scripts/`**: Standalone utility and diagnostic scripts.
    -   `__init__.py`: Makes scripts a Python package.
    -   `cortex_inspector.py`: A command-line tool to inspect the knowledge base.

-   **`proposals/`**: Default directory to store all saved proposal data.
-   **`external_research/`**: Default location for synthesized research notes from the AI Research agent.
-   **`template_maps/`**: Default directory to store saved progress from the Template Editor.
-   **`logs/`**: Directory containing log files like `ingestion.log` and `query.log`.
    -   `ingested_files.log`: Note: This specific log is stored inside the `knowledge_hub_db` directory, not in the main `logs` folder.
```


--- FILE: ./Cortex_Suite.py ---
# ## File: Cortex_Suite.py
# Version: 62.0.0 (UI Page Renaming)
# Date: 2025-07-15
# Purpose: A central Streamlit launchpad for the integrated Cortex Suite.

import streamlit as st

# --- Page Setup ---
st.set_page_config(
    page_title="Cortex Suite",
    page_icon="üöÄ",
    layout="wide"
)

st.title("üöÄ Welcome to the Project Cortex Suite")
st.caption("Version 62.0.0 (UI Page Renaming)")

st.markdown("""
This is the central hub for the Cortex Suite, an integrated workbench for building a knowledge base and using it for AI-assisted proposal development.

Please select a tool from the sidebar on the left to begin your workflow:

-   **ü§ñ AI Assisted Research:** Use a multi-agent system to perform external research on a topic and synthesize the findings into a "Discovery Note".

-   **üß† Knowledge Ingest:** Ingest new documents (your own files or generated Discovery Notes) into the central knowledge base.

-   **üî¨ Knowledge Search:** Explore the entire knowledge base, build curated "Working Collections", and prune unwanted documents from the KB.

-   **üìö Collection Management:** View the contents of, rename, export, or delete your Working Collections.

-   **üìù Proposal Step 1 Prep:** (Formerly Template Editor) Create and modify `.docx` templates with interactive Cortex instructions.

-   **üóÇÔ∏è Proposal Step 2 Make:** (Formerly Proposal Management) Create new proposals or load, manage, and delete existing ones.

This unified interface provides a seamless workflow from initial research to final document generation.
""")

st.divider()

st.info("To get started, click on one of the pages in the navigation sidebar.")


--- FILE: ./cortex_engine/__init__.py ---



--- FILE: ./cortex_engine/boilerplate_manager.py ---
# ## File: cortex_engine/boilerplate_manager.py
# Version: 1.0.0 (Initial Release)
# Date: 2025-07-14
# Purpose: Manages loading and accessing boilerplate text snippets from a JSON file.

import json
from pathlib import Path
import os

# Define the path to the boilerplate file in the project root
BOILERPLATE_FILE = Path(__file__).parent.parent / "boilerplate.json"

class BoilerplateManager:
    """
    Handles loading and providing access to boilerplate text blocks
    stored in a central JSON file.
    """
    def __init__(self):
        """Initializes the manager and loads the boilerplate data."""
        self.boilerplate_data = self._load()

    def _load(self) -> dict:
        """Loads the boilerplate content from the JSON file."""
        if not BOILERPLATE_FILE.exists():
            # Create a default file if it doesn't exist to guide the user
            default_content = {
                "about_us": "This is the default 'About Us' section. Please edit this in boilerplate.json.",
                "legals": "This is the default 'Legal Terms' section. Please edit this in boilerplate.json."
            }
            self._save(default_content)
            return default_content

        try:
            with open(BOILERPLATE_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, IOError):
            # Return a default error message if the file is corrupt
            return {"error": "Could not load boilerplate.json. Please check the file format."}

    def _save(self, data: dict):
        """Saves data to the boilerplate JSON file."""
        try:
            with open(BOILERPLATE_FILE, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=4)
        except IOError as e:
            print(f"Error saving boilerplate file: {e}")

    def get_boilerplate(self, name: str) -> str:
        """
        Retrieves a specific boilerplate text by its name (key).

        Args:
            name: The key of the boilerplate text to retrieve.

        Returns:
            The boilerplate text as a string, or an error message if not found.
        """
        return self.boilerplate_data.get(name, f"[BOILERPLATE ERROR: Key '{name}' not found in boilerplate.json]")

    def get_boilerplate_names(self) -> list:
        """
        Returns a list of all available boilerplate names (keys).
        """
        return list(self.boilerplate_data.keys())


--- FILE: ./cortex_engine/collection_manager.py ---
# ## File: cortex_engine/collection_manager.py
# Version: 5.2.0 (Import Cleanup)
# Date: 2025-07-16
# Purpose: Manages CRUD operations for collections.
#          - FIX (v5.2.0): Removed unnecessary sys.path manipulation to improve
#            code hygiene and prevent potential import side-effects.

import json
import os
import shutil
from pathlib import Path
import sys
import re
from datetime import datetime

# Add project root to path to allow finding sibling modules
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from cortex_engine.utils import get_file_hash

COLLECTIONS_FILE = str(PROJECT_ROOT / "working_collections.json")

def convert_windows_to_wsl_path(win_path):
    if not isinstance(win_path, str) or win_path.startswith('/mnt/'): return win_path
    path_str = str(win_path).replace('\\', '/')
    match = re.match(r"^([a-zA-Z]):/(.*)", path_str)
    if match:
        drive, rest = match.groups()
        return f"/mnt/{drive.lower()}/{rest}"
    return win_path

class WorkingCollectionManager:
    """Manages CRUD operations for working collections stored in a JSON file."""

    def __init__(self):
        self.collections = self._load()

    def _load(self):
        collections = {}
        if os.path.exists(COLLECTIONS_FILE):
            try:
                with open(COLLECTIONS_FILE, 'r') as f:
                    collections = json.load(f)
            except (json.JSONDecodeError, IOError):
                collections = {}

        now_iso = datetime.now().isoformat()
        for name, data in collections.items():
            if isinstance(data, dict):
                if 'created_at' not in data: data['created_at'] = now_iso
                if 'modified_at' not in data: data['modified_at'] = data.get('created_at', now_iso)
            else:
                collections[name] = {"name": name, "doc_ids": data, "created_at": now_iso, "modified_at": now_iso}

        if "default" not in collections:
            collections["default"] = {"name": "default", "doc_ids": [], "created_at": now_iso, "modified_at": now_iso}
        return collections

    def _save(self):
        try:
            with open(COLLECTIONS_FILE, 'w') as f:
                json.dump(self.collections, f, indent=4)
            return True
        except IOError:
            return False

    def get_collection_names(self) -> list:
        return list(self.collections.keys())

    def get_doc_ids_by_name(self, name: str) -> list:
        return self.collections.get(name, {}).get("doc_ids", [])

    def create_collection(self, name: str) -> bool:
        if name and name not in self.collections:
            now_iso = datetime.now().isoformat()
            self.collections[name] = {"name": name, "doc_ids": [], "created_at": now_iso, "modified_at": now_iso}
            self._save()
            return True
        return False

    def add_docs_by_id_to_collection(self, name: str, doc_ids: list):
        if not doc_ids: return
        if name not in self.collections: self.create_collection(name)
        existing_ids = set(self.collections[name].get("doc_ids", []))
        ids_added = False
        for doc_id in doc_ids:
            if doc_id not in existing_ids:
                self.collections[name]["doc_ids"].append(doc_id)
                existing_ids.add(doc_id)
                ids_added = True
        if ids_added:
            self.collections[name]["modified_at"] = datetime.now().isoformat()
            self._save()

    def remove_from_collection(self, name: str, doc_ids_to_remove: list):
        if name in self.collections:
            initial_count = len(self.collections[name].get("doc_ids", []))
            self.collections[name]["doc_ids"] = [doc_id for doc_id in self.collections[name]["doc_ids"] if doc_id not in doc_ids_to_remove]
            if len(self.collections[name]["doc_ids"]) < initial_count:
                self.collections[name]['modified_at'] = datetime.now().isoformat()
                self._save()

    def rename_collection(self, old_name: str, new_name: str):
        if old_name in self.collections and new_name not in self.collections and old_name != "default":
            self.collections[new_name] = self.collections.pop(old_name)
            self.collections[new_name]['name'] = new_name
            self.collections[new_name]['modified_at'] = datetime.now().isoformat()
            self._save()

    def delete_collection(self, name: str):
        if name in self.collections and name != "default":
            del self.collections[name]
            self._save()

    def merge_collections(self, source_name: str, dest_name: str) -> bool:
        """Merges the source collection into the destination and deletes the source."""
        if source_name not in self.collections or dest_name not in self.collections or source_name == dest_name:
            return False

        source_doc_ids = self.get_doc_ids_by_name(source_name)
        self.add_docs_by_id_to_collection(dest_name, source_doc_ids)
        self.delete_collection(source_name)
        return True

    def export_collection_files(self, name: str, output_dir: str, vector_collection) -> tuple:
        doc_ids = self.get_doc_ids_by_name(name)
        if not doc_ids: return [], []
        wsl_output_dir = convert_windows_to_wsl_path(output_dir)
        Path(wsl_output_dir).mkdir(parents=True, exist_ok=True)
        results = vector_collection.get(where={"doc_id": {"$in": doc_ids}}, include=["metadatas"])
        source_paths = set(meta['doc_posix_path'] for meta in results['metadatas'] if 'doc_posix_path' in meta)
        copied_files, failed_files = [], []
        for src_path_str in source_paths:
            src_path = Path(src_path_str)
            dest_path = Path(wsl_output_dir) / src_path.name
            try:
                if src_path.exists():
                    shutil.copy(src_path, dest_path)
                    copied_files.append(str(dest_path))
                else:
                    raise FileNotFoundError
            except (FileNotFoundError, Exception) as e:
                failed_files.append(f"{src_path_str} (Reason: {e})")
        return copied_files, failed_files


--- FILE: ./cortex_engine/config.py ---
# ## File: config.py
# Version: 2.7.0 (Enable Image Ingestion)
# Date: 2025-07-22
# Purpose: Central configuration file for Project Cortex.
#          - CHANGE (v2.7.0): Removed image extensions from the default
#            exclusion patterns to allow them to be discovered by the
#            ingestion scanner.

import os
from pathlib import Path

# --- Core Paths ---
# This is now a FALLBACK. The scripts will accept a path argument to override this.
BASE_DATA_PATH = "/mnt/f/ai_databases"

# The following paths are placeholders; they will be dynamically set by scripts.
# They are derived from the BASE_DATA_PATH by default.
CHROMA_DB_PATH = os.path.join(BASE_DATA_PATH, "knowledge_hub_db")
GRAPH_FILE_PATH = os.path.join(BASE_DATA_PATH, "knowledge_cortex.gpickle")
IMAGE_STORE_PATH = os.path.join(CHROMA_DB_PATH, "images")

# --- Log Files & Staging ---
# Go up one level from this file's directory (cortex_engine) to the project root.
PROJECT_ROOT = Path(__file__).parent.parent
LOGS_DIR = PROJECT_ROOT / "logs"
LOGS_DIR.mkdir(exist_ok=True) # Ensure the logs directory exists

# UNIFIED LOG: Tracks all processed or excluded files. Stored inside the DB dir.
INGESTED_FILES_LOG = "ingested_files.log"
STAGING_INGESTION_FILE = str(PROJECT_ROOT / "staging_ingestion.json")

# Centralize logs into the 'logs' directory
INGESTION_LOG_PATH = str(LOGS_DIR / "ingestion.log")
QUERY_LOG_PATH = str(LOGS_DIR / "query.log")


# --- ChromaDB and VectorStore Settings ---
COLLECTION_NAME = "knowledge_hub_collection"

# --- Model Names ---
EMBED_MODEL = "BAAI/bge-base-en-v1.5"
LLM_MODEL = "mistral-cortex-24"
VLM_MODEL = "llava"

# --- UI Defaults ---
# SPRINT 21 CHANGE: Removed image files from default exclusions.
DEFAULT_EXCLUSION_PATTERNS_STR = (
    # Office temp files
    "~$*.docx\n~$*.xlsx\n~$*.pptx\n"
    # Common junk
    "*.tmp\n*.lnk\n"
    # Web & code files
    "*.css\n*.html\n*.js\n*.py\n*.json\n"
    # Data & archives
    "*.xls\n*.xlsx\n*.csv\n*.zip\n*.ppt\n*.pptx\n"
    # Multimedia files (excluding images)
    "*.mp4\n*.mov\n*.avi\n*.mp3\n*.wav\n*.raf\n"
    # Business document types to ignore by default
    "*invoice*\n*timesheet*\n*contract*\n*receipt*"
)


--- FILE: ./cortex_engine/config_manager.py ---
# ## File: cortex_engine/config_manager.py
# Version: 2.0.0 (Class-based Refactor)
# Date: 2025-07-12
# Purpose: Manages persistent user configuration settings using a class structure.
#          - MAJOR CHANGE (v2.0.0): Refactored from standalone functions to a
#            ConfigManager class to align with modern UI practices and fix
#            ImportErrors in dependent pages.

import json
from pathlib import Path

# The config file is stored in the project's root directory.
CONFIG_FILE_PATH = Path(__file__).parent.parent / "cortex_config.json"

class ConfigManager:
    """
    Handles loading and saving of persistent user settings to a JSON file.
    This provides a simple way to remember user inputs across sessions,
    such as last-used directory paths.
    """
    def __init__(self):
        """Initializes the manager and loads the current config."""
        self.config_path = CONFIG_FILE_PATH
        self.config = self._load_config()

    def _load_config(self) -> dict:
        """Loads the configuration from the JSON file."""
        if not self.config_path.exists():
            return {}  # Return an empty dict if the file doesn't exist
        try:
            with open(self.config_path, 'r') as f:
                return json.load(f)
        except (json.JSONDecodeError, IOError):
            return {} # Return an empty dict on error

    def get_config(self) -> dict:
        """Returns the current configuration dictionary."""
        return self.config

    def update_config(self, new_settings: dict):
        """
        Updates the configuration with new settings and saves it to the file.
        """
        self.config.update(new_settings)
        try:
            with open(self.config_path, 'w') as f:
                json.dump(self.config, f, indent=4)
        except IOError as e:
            # In a Streamlit app, printing might go to the console.
            # This is acceptable for a non-critical error.
            print(f"Warning: Could not save configuration. Error: {e}")


--- FILE: ./cortex_engine/entity_extractor.py ---
# cortex_engine/entity_extractor.py
import re
from typing import List, Dict, Tuple, Set
from pydantic import BaseModel, Field
import spacy
from collections import defaultdict
import logging

class ExtractedEntity(BaseModel):
    name: str
    entity_type: str  # 'person', 'organization', 'project', 'report'
    aliases: List[str] = Field(default_factory=list)
    
class ExtractedRelationship(BaseModel):
    source: str
    target: str
    relationship_type: str  # 'worked_on', 'authored', 'client_of', 'collaborated_with'
    context: str = ""

class EntityExtractor:
    def __init__(self):
        # Load spaCy model for NER
        try:
            self.nlp = spacy.load("en_core_web_sm")
            logging.info("Loaded spaCy model successfully")
        except:
            logging.warning("spaCy model not found. Attempting to download...")
            import subprocess
            subprocess.run(["python", "-m", "spacy", "download", "en_core_web_sm"])
            self.nlp = spacy.load("en_core_web_sm")
        
        # Pattern matching for common document patterns
        self.consultant_patterns = [
            r"(?:consultant|author|prepared by|written by|compiled by|lead|manager)[:;\s]+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)",
            r"([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+\((?:consultant|author|lead|manager)\)",
            r"(?:Project Manager|Technical Lead|Consultant)[:;\s]+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)",
        ]
        
        self.client_patterns = [
            r"(?:client|customer|for|prepared for|submitted to)[:;\s]+([A-Z][a-z]+(?:\s+(?:&\s+)?[A-Z][a-z]+)*)",
            r"([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+\(client\)",
        ]
        
        self.project_patterns = [
            r"(?:project|initiative|program)[:;\s]+([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)",
            r"([A-Z][a-z]+(?:\s+[A-Z][a-z]+)*)\s+(?:Project|Initiative|Program)",
        ]
    
    def normalize_entity_name(self, name: str) -> str:
        """Normalize entity names to handle variations."""
        # Remove extra whitespace
        name = ' '.join(name.split())
        # Remove common suffixes
        name = re.sub(r'\s+(Inc\.|LLC|Ltd\.|Limited|Corporation|Corp\.)$', '', name, flags=re.IGNORECASE)
        return name.strip()
    
    def extract_entities_and_relationships(self, 
                                         text: str, 
                                         metadata: Dict) -> Tuple[List[ExtractedEntity], List[ExtractedRelationship]]:
        """Extract entities and relationships from document text and metadata."""
        entities = []
        relationships = []
        entity_map = {}  # name -> ExtractedEntity
        
        # Use filename and metadata for initial context
        doc_type = metadata.get('document_type', '')
        filename = metadata.get('file_name', '')
        summary = metadata.get('summary', '')
        
        # Extract from structured metadata
        if 'thematic_tags' in metadata:
            for tag in metadata['thematic_tags']:
                if any(keyword in tag.lower() for keyword in ['project', 'initiative', 'program']):
                    entity = ExtractedEntity(
                        name=tag,
                        entity_type='project'
                    )
                    entities.append(entity)
                    entity_map[tag] = entity
        
        # Combine text sources for extraction
        combined_text = f"{filename}\n{summary}\n{text[:10000]}"
        
        # Extract using NER
        try:
            doc = self.nlp(combined_text)
            
            persons = set()
            organizations = set()
            
            for ent in doc.ents:
                if ent.label_ == "PERSON":
                    name = self.normalize_entity_name(ent.text)
                    if len(name.split()) <= 4:  # Avoid false positives with very long names
                        persons.add(name)
                elif ent.label_ == "ORG":
                    name = self.normalize_entity_name(ent.text)
                    organizations.add(name)
        except Exception as e:
            logging.error(f"spaCy NER failed: {e}")
        
        # Extract using patterns
        text_for_patterns = combined_text[:5000]  # Focus on document header
        
        for pattern in self.consultant_patterns:
            matches = re.findall(pattern, text_for_patterns, re.IGNORECASE | re.MULTILINE)
            for match in matches:
                name = self.normalize_entity_name(match)
                if len(name.split()) <= 4:
                    persons.add(name)
        
        for pattern in self.client_patterns:
            matches = re.findall(pattern, text_for_patterns, re.IGNORECASE | re.MULTILINE)
            for match in matches:
                name = self.normalize_entity_name(match)
                organizations.add(name)
        
        for pattern in self.project_patterns:
            matches = re.findall(pattern, text_for_patterns, re.IGNORECASE | re.MULTILINE)
            for match in matches:
                name = self.normalize_entity_name(match)
                if name not in entity_map:
                    entity = ExtractedEntity(
                        name=name,
                        entity_type='project'
                    )
                    entities.append(entity)
                    entity_map[name] = entity
        
        # Create entities for persons
        for person in persons:
            if person not in entity_map:
                entity = ExtractedEntity(
                    name=person,
                    entity_type='person'
                )
                entities.append(entity)
                entity_map[person] = entity
        
        # Create entities for organizations  
        for org in organizations:
            if org not in entity_map:
                entity = ExtractedEntity(
                    name=org,
                    entity_type='organization'
                )
                entities.append(entity)
                entity_map[org] = entity
        
        # Extract relationships based on document type
        if doc_type in ['Project Plan', 'Final Report', 'Draft Report', 'Proposal/Quote', 'Technical Documentation']:
            # Link consultants to documents
            for person in persons:
                relationships.append(ExtractedRelationship(
                    source=person,
                    target=filename,
                    relationship_type='authored'
                ))
            
            # Link organizations as clients
            for org in organizations:
                # Try to determine if it's a client based on context
                org_pattern = re.escape(org)
                if re.search(rf"(?:for|client|customer)[\s:]*{org_pattern}", text_for_patterns, re.IGNORECASE):
                    relationships.append(ExtractedRelationship(
                        source=org,
                        target=filename,
                        relationship_type='client_of'
                    ))
        
        # Extract collaboration relationships
        if len(persons) > 1:
            person_list = list(persons)
            for i in range(len(person_list)):
                for j in range(i+1, len(person_list)):
                    relationships.append(ExtractedRelationship(
                        source=person_list[i],
                        target=person_list[j],
                        relationship_type='collaborated_with',
                        context=filename
                    ))
        
        # Link projects to documents
        for entity in entity_map.values():
            if entity.entity_type == 'project':
                relationships.append(ExtractedRelationship(
                    source=entity.name,
                    target=filename,
                    relationship_type='documented_in'
                ))
        
        return entities, relationships


--- FILE: ./cortex_engine/graph_extraction_worker.py ---
# ## File: graph_extraction_worker.py
# Version: 2.1.1 ("Verbose Debugging")
# Date: 2025-07-08
# Purpose: A robust worker script using the outlines library.
#          - FIX (v2.1.1): Added verbose print statements to stderr for debugging
#            the inputs and outputs of the extraction process.

import sys
import argparse
import json
import ollama
from outlines.models.ollama import Ollama
from outlines import Generator
from outlines.types import JsonSchema

def main():
    """
    Accepts model name and schema via arguments, reads the prompt from stdin,
    performs guided generation, and prints the resulting JSON to stdout.
    """
    parser = argparse.ArgumentParser()
    parser.add_argument("--model-name", required=True, type=str, help="The name of the Ollama model to use.")
    parser.add_argument("--schema-str", required=True, type=str, help="The Pydantic JSON schema as a string.")
    args = parser.parse_args()

    try:
        prompt = sys.stdin.read()
        client = ollama.Client()

        # --- START: v2.1.1 DEBUGGING ADDITION ---
        # Print all the inputs this worker script received to the error stream for debugging
        print("--- GRAPH EXTRACTION WORKER (v2.1.1) INPUTS ---", file=sys.stderr)
        print(f"Model Name: {args.model_name}", file=sys.stderr)
        print(f"Schema: {args.schema_str}", file=sys.stderr)
        print(f"Prompt Text (first 500 chars): {prompt[:500]}...", file=sys.stderr)
        print("-------------------------------------------------", file=sys.stderr)
        # --- END: v2.1.1 DEBUGGING ADDITION ---

        model = Ollama(model_name=args.model_name, client=client)

        generator = Generator(model, JsonSchema(args.schema_str))
        generated_json_str = generator(prompt)

        # --- START: v2.1.1 DEBUGGING ADDITION ---
        # Print the raw generated output before sending it to stdout
        print("--- RAW LLM EXTRACTION OUTPUT ---", file=sys.stderr)
        print(generated_json_str, file=sys.stderr)
        print("---------------------------------", file=sys.stderr)
        # --- END: v2.1.1 DEBUGGING ADDITION ---

        print(generated_json_str) # This is the actual output for the parent script
        sys.exit(0)

    except Exception as e:
        print(f"Worker script failed: {e}", file=sys.stderr)
        sys.exit(1)

if __name__ == "__main__":
    main()


--- FILE: ./cortex_engine/graph_manager.py ---
# cortex_engine/graph_manager.py (enhanced version)
import os
import networkx as nx
import pickle
from typing import List, Dict, Set, Optional
from collections import defaultdict
import logging

class EnhancedGraphManager:
    def __init__(self, graph_file_path: str):
        self.graph_file_path = graph_file_path
        logging.info(f"Initializing EnhancedGraphManager with path: {graph_file_path}")
        
        if os.path.exists(self.graph_file_path):
            try:
                with open(self.graph_file_path, 'rb') as f:
                    self.graph = pickle.load(f)
                logging.info(f"Loaded existing graph with {self.graph.number_of_nodes()} nodes")
            except Exception as e:
                logging.error(f"Failed to load graph: {e}")
                self.graph = nx.DiGraph()
        else:
            self.graph = nx.DiGraph()
            logging.info("Created new empty graph")
        
        # Initialize indices for efficient querying
        self._build_indices()
    
    def _build_indices(self):
        """Build indices for efficient querying."""
        self.entity_index = defaultdict(set)  # entity_type -> set of node_ids
        self.relationship_index = defaultdict(set)  # relationship_type -> set of edge tuples
        
        for node, data in self.graph.nodes(data=True):
            if 'entity_type' in data:
                self.entity_index[data['entity_type']].add(node)
        
        for source, target, data in self.graph.edges(data=True):
            if 'relationship_type' in data:
                self.relationship_index[data['relationship_type']].add((source, target))
    
    def add_entity(self, entity_id: str, entity_type: str, **attributes):
        """Add an entity node with automatic indexing."""
        self.graph.add_node(entity_id, entity_type=entity_type, **attributes)
        self.entity_index[entity_type].add(entity_id)
    
    def add_relationship(self, source: str, target: str, relationship_type: str, **attributes):
        """Add a relationship with automatic indexing."""
        self.graph.add_edge(source, target, relationship_type=relationship_type, **attributes)
        self.relationship_index[relationship_type].add((source, target))
    
    def query_consultant_projects(self, consultant_name: str) -> List[Dict]:
        """Find all projects and reports a consultant worked on."""
        results = []
        
        # Find by exact match
        consultant_id = f"person:{consultant_name}"
        
        if consultant_id in self.graph:
            # Find all documents authored by this consultant
            for neighbor in self.graph.neighbors(consultant_id):
                edge_data = self.graph.edges[consultant_id, neighbor]
                if edge_data.get('relationship_type') == 'authored':
                    if neighbor in self.graph:
                        node_data = self.graph.nodes[neighbor]
                        results.append({
                            'document': neighbor,
                            'type': node_data.get('document_type', 'Unknown'),
                            'metadata': node_data
                        })
        
        return results
    
    def query_consultant_collaborators(self, consultant_name: str) -> Set[str]:
        """Find all people who collaborated with a consultant."""
        collaborators = set()
        consultant_id = f"person:{consultant_name}"
        
        if consultant_id in self.graph:
            for neighbor in self.graph.neighbors(consultant_id):
                edge_data = self.graph.edges[consultant_id, neighbor]
                if edge_data.get('relationship_type') == 'collaborated_with':
                    collaborators.add(neighbor)
            
            # Also check reverse relationships
            for predecessor in self.graph.predecessors(consultant_id):
                edge_data = self.graph.edges[predecessor, consultant_id]
                if edge_data.get('relationship_type') == 'collaborated_with':
                    collaborators.add(predecessor)
        
        return collaborators
    
    def query_client_projects(self, client_name: str) -> List[Dict]:
        """Find all work done for a specific client."""
        results = []
        client_id = f"organization:{client_name}"
        
        if client_id in self.graph:
            # Find all documents that have this organization as a client
            for neighbor in self.graph.neighbors(client_id):
                edge_data = self.graph.edges[client_id, neighbor]
                if edge_data.get('relationship_type') == 'client_of':
                    if neighbor in self.graph:
                        node_data = self.graph.nodes[neighbor]
                        results.append({
                            'document': neighbor,
                            'metadata': node_data
                        })
        
        return results
    
    def get_entity_by_name(self, name: str, entity_type: Optional[str] = None) -> Optional[str]:
        """Find entity ID by name, optionally filtered by type."""
        if entity_type:
            potential_id = f"{entity_type}:{name}"
            if potential_id in self.graph:
                return potential_id
        else:
            # Search all entity types
            for etype in ['person', 'organization', 'project']:
                potential_id = f"{etype}:{name}"
                if potential_id in self.graph:
                    return potential_id
        return None
    
    def get_popular_document_types_by_client(self) -> Dict[str, Dict[str, int]]:
        """Analyze which document types are most common for different clients."""
        client_doc_types = defaultdict(lambda: defaultdict(int))
        
        for node in self.entity_index['organization']:
            projects = self.query_client_projects(node.split(':', 1)[1])
            for project in projects:
                doc_type = project['metadata'].get('document_type', 'Unknown')
                client_doc_types[node][doc_type] += 1
        
        return dict(client_doc_types)
    
    def save_graph(self):
        """Save the graph to a pickle file."""
        try:
            with open(self.graph_file_path, 'wb') as f:
                pickle.dump(self.graph, f)
            logging.info(f"Graph saved successfully to {self.graph_file_path}")
        except Exception as e:
            logging.error(f"Failed to save graph: {e}")
    
    def get_graph_stats(self) -> Dict:
        """Get statistics about the graph."""
        stats = {
            'total_nodes': self.graph.number_of_nodes(),
            'total_edges': self.graph.number_of_edges(),
            'entity_counts': {etype: len(nodes) for etype, nodes in self.entity_index.items()},
            'relationship_counts': {rtype: len(edges) for rtype, edges in self.relationship_index.items()}
        }
        return stats


--- FILE: ./cortex_engine/graph_query.py ---
# cortex_engine/graph_query.py
# 23_07_25
# V1.0 Claude Code
from typing import List, Dict, Optional
from cortex_engine.graph_manager import EnhancedGraphManager

class GraphQueryEngine:
    def __init__(self, graph_manager: EnhancedGraphManager, vector_index):
        self.graph = graph_manager
        self.vector_index = vector_index
    
    def hybrid_search(self, query: str, use_graph_context: bool = True) -> List[Dict]:
        """Perform hybrid search using both vector similarity and graph relationships."""
        # First, do standard vector search
        vector_results = self.vector_index.as_retriever(similarity_top_k=10).retrieve(query)
        
        if not use_graph_context:
            return vector_results
        
        # Enhance results with graph context
        enhanced_results = []
        
        for result in vector_results:
            doc_id = result.metadata.get('doc_id')
            
            # Get graph context
            graph_context = {
                'authors': [],
                'clients': [],
                'related_docs': [],
                'collaborators': []
            }
            
            if doc_id in self.graph.graph:
                # Find authors
                for predecessor in self.graph.graph.predecessors(doc_id):
                    edge_data = self.graph.graph.edges[predecessor, doc_id]
                    if edge_data.get('relationship_type') == 'authored':
                        graph_context['authors'].append(predecessor)
                
                # Find related documents through shared entities
                for author in graph_context['authors']:
                    related = self.graph.query_consultant_projects(author)
                    graph_context['related_docs'].extend([
                        r['document'] for r in related if r['document'] != doc_id
                    ])
            
            result.metadata['graph_context'] = graph_context
            enhanced_results.append(result)
        
        return enhanced_results


--- FILE: ./cortex_engine/ingest_cortex.py ---
# ## File: ingest_cortex.py
# Version: 13.0.0 (GraphRAG Integration by Claude Opus)
# Date: 2025-07-23
# Purpose: Core ingestion script for Project Cortex with integrated knowledge graph extraction.
#          - FEATURE (v13.0.0): Integrated entity extraction and knowledge graph building
#            during the ingestion process. The system now extracts people, organizations,
#            projects, and their relationships while maintaining backward compatibility.

import os
import sys
os.environ['ANONYMIZED_TELEMETRY'] = 'False'

import argparse
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional, Literal, Tuple
import re

from pydantic import BaseModel, Field, ValidationError
from llama_index.core import Document
from llama_index.core.settings import Settings
from llama_index.llms.ollama import Ollama
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core import VectorStoreIndex, StorageContext
import chromadb
from chromadb.config import Settings as ChromaSettings
import hashlib

from llama_index.readers.file import (
    DocxReader,
    PptxReader,
    PyMuPDFReader,
    FlatReader
)

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from cortex_engine.config import INGESTION_LOG_PATH, STAGING_INGESTION_FILE, INGESTED_FILES_LOG, COLLECTION_NAME, EMBED_MODEL
from cortex_engine.utils import get_file_hash
from cortex_engine.query_cortex import describe_image_with_vlm_for_ingestion
from cortex_engine.entity_extractor import EntityExtractor, ExtractedEntity, ExtractedRelationship
from cortex_engine.graph_manager import EnhancedGraphManager

LOG_FILE = INGESTION_LOG_PATH
STAGING_FILE = STAGING_INGESTION_FILE
COLLECTIONS_FILE = str(project_root / "working_collections.json")

# Define supported image extensions
IMAGE_EXTENSIONS = {".png", ".jpg", ".jpeg"}

os.makedirs(os.path.dirname(LOG_FILE), exist_ok=True)

def initialize_script():
    """Sets up robust logging and configures LlamaIndex models."""
    log_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    root_logger = logging.getLogger()

    if root_logger.hasHandlers():
        root_logger.handlers.clear()

    root_logger.setLevel(logging.INFO)

    file_handler = logging.FileHandler(LOG_FILE, mode='a', encoding='utf-8')
    file_handler.setFormatter(log_formatter)
    root_logger.addHandler(file_handler)

    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(log_formatter)
    root_logger.addHandler(console_handler)

    logging.info("--- Script Initialized: Configuring models... ---")
    Settings.llm = Ollama(model="mistral", request_timeout=120.0)
    Settings.embed_model = HuggingFaceEmbedding(model_name=EMBED_MODEL)
    logging.info(f"Models configured (Embed: {EMBED_MODEL}).")

class RichMetadata(BaseModel):
    document_type: Literal[
        "Project Plan", "Technical Documentation", "Proposal/Quote", "Case Study / Trophy",
        "Final Report", "Draft Report", "Presentation", "Contract/SOW",
        "Meeting Minutes", "Financial Report", "Research Paper", "Email Correspondence", "Image/Diagram", "Other"
    ] = Field(..., description="The primary category of the document.")
    proposal_outcome: Literal["Won", "Lost", "Pending", "N/A"] = Field(..., description="The outcome of the proposal, if applicable.")
    summary: str = Field(..., description="A concise, 1-3 sentence summary of the document's content and purpose.")
    thematic_tags: List[str] = Field(default_factory=list, description="A list of 3-5 key themes, topics, or technologies discussed.")

class DocumentMetadata(BaseModel):
    doc_id: str
    doc_posix_path: str
    file_name: str
    last_modified_date: str
    rich_metadata: Optional[RichMetadata] = None
    exclude_from_final: bool = False
    extracted_entities: List[Dict] = Field(default_factory=list)
    extracted_relationships: List[Dict] = Field(default_factory=list)

def load_processed_files_log(log_path: str) -> Dict[str, str]:
    if not os.path.exists(log_path): return {}
    with open(log_path, 'r') as f:
        try:
            log_data = json.load(f)
            return {k: (v[0] if isinstance(v, list) else v) for k, v in log_data.items()}
        except json.JSONDecodeError: return {}

def manual_load_documents(file_paths: List[str]) -> List[Document]:
    documents = []
    reader_map = {".pdf": PyMuPDFReader(), ".docx": DocxReader(), ".pptx": PptxReader()}
    default_reader = FlatReader()
    
    # Suppress PyMuPDF warnings
    import fitz
    fitz.TOOLS.mupdf_display_errors(False)
    
    for file_path in file_paths:
        try:
            path = Path(file_path)
            extension = path.suffix.lower()
            
            # Handle image files using the VLM
            if extension in IMAGE_EXTENSIONS:
                logging.info(f"Loading '{file_path}' with VLM Image Processor")
                description = describe_image_with_vlm_for_ingestion(file_path)
                doc = Document(text=description)
                doc.metadata['file_path'] = str(path.as_posix())
                doc.metadata['file_name'] = path.name
                doc.metadata['source_type'] = 'image'
                documents.append(doc)
                continue

            # Original text-based document handling
            reader = reader_map.get(extension, default_reader)
            logging.info(f"Loading '{file_path}' with reader: {reader.__class__.__name__}")
            
            # For PDFs, add extra error handling
            if isinstance(reader, PyMuPDFReader):
                try:
                    docs_from_file = reader.load_data(file_path=path)
                except Exception as pdf_error:
                    # If PDF fails completely, log it but continue
                    if "cmsOpenProfileFromMem" not in str(pdf_error):
                        logging.warning(f"PDF processing warning for {path.name}: {pdf_error}")
                    docs_from_file = reader.load_data(file_path=path)
            else:
                docs_from_file = reader.load_data(file=path)
                
            for doc in docs_from_file:
                doc.metadata['file_path'] = str(path.as_posix())
                doc.metadata['file_name'] = path.name
                doc.metadata['source_type'] = 'document'
            documents.extend(docs_from_file)
            
        except Exception as e:
            logging.error(f"Failed to load file {file_path}: {e}", exc_info=True)
            documents.append(Document(
                text=f"Error reading this document. Could not load content from {Path(file_path).name}. Reason: {e}",
                metadata={'file_path': str(Path(file_path).as_posix()), 'file_name': Path(file_path).name}
            ))
    return documents

def extract_entities_and_relationships(doc_text: str, metadata: Dict, entity_extractor: EntityExtractor) -> Tuple[List[Dict], List[Dict]]:
    """Extract entities and relationships from document and convert to serializable format."""
    try:
        entities, relationships = entity_extractor.extract_entities_and_relationships(doc_text, metadata)
        
        # Convert to dictionaries for JSON serialization
        entities_dict = [entity.model_dump() for entity in entities]
        relationships_dict = [rel.model_dump() for rel in relationships]
        
        return entities_dict, relationships_dict
    except Exception as e:
        logging.error(f"Failed to extract entities: {e}")
        return [], []

def analyze_documents(include_paths: List[str], fresh_start: bool):
    logging.info(f"--- Starting Stage 2: Document Analysis with Graph Extraction (Cortex v13.0.0) ---")
    if fresh_start and os.path.exists(STAGING_FILE): os.remove(STAGING_FILE)
    
    # Initialize entity extractor
    entity_extractor = EntityExtractor()
    
    docs = manual_load_documents(include_paths)
    logging.info(f"Loaded {len(docs)} document objects from {len(include_paths)} files.")
    unique_docs = list({doc.metadata['file_path']: doc for doc in docs}.values())
    logging.info(f"--- Found {len(unique_docs)} unique files to process. ---")

    staged_docs = []
    schema_json_str = json.dumps(RichMetadata.model_json_schema(), indent=2)
    prompt_lines = [
        "Analyze the document content and its file path to return a single, valid JSON object that strictly conforms to this Pydantic schema:", "```json", "{schema}", "```", "---",
        "**SPECIAL INSTRUCTIONS:**",
        '- If the source is an image (`source_type` is "image"), you **MUST** set `document_type` to "Image/Diagram".',
        '- If the file is a `.pptx`, you **MUST** set `document_type` to "Presentation".',
        '- If the `file_path` contains "trophy" or the filename mentions "Case Study", you **MUST** set `document_type` to "Case Study / Trophy".',
        '- If the filename contains the word "Final", you should strongly prefer the "Final Report" type.', '- If the filename contains the word "Draft", you should strongly prefer the "Draft Report" type.',
        '- If no other category seems appropriate, you **MUST** use "Other" as a fallback.', "---", "File Path: {file_path}", "Source Type: {source_type}", "Document Content (first 8000 characters):",
        "-----------------", "{text}", "-----------------", "IMPORTANT: Your response must be ONLY the JSON object itself, with no extra text, explanations, or wrapper keys."
    ]
    metadata_prompt_template = "\n".join(prompt_lines)

    for i, doc in enumerate(unique_docs):
        rich_metadata = None
        file_path_str, file_name = doc.metadata.get('file_path', ''), doc.metadata.get('file_name', 'Unknown File')
        source_type = doc.metadata.get('source_type', 'document')
        logging.info(f"--- ({i+1}/{len(unique_docs)}) Analyzing: {file_name} ({source_type.upper()}) ---")
        
        # Print machine-readable progress for the UI
        print(f"CORTEX_PROGRESS::{i+1}/{len(unique_docs)}::{file_name}", flush=True)
        
        try:
            if not doc.text.strip(): raise ValueError("Document is empty or could not be read.")
            prompt = metadata_prompt_template.format(schema=schema_json_str, text=doc.get_content()[:8000], file_path=file_path_str, source_type=source_type)
            logging.info("Sending prompt to LLM for metadata extraction...")
            response_str = str(Settings.llm.complete(prompt))
            logging.info("Received raw response from LLM. Cleaning and parsing JSON...")
            json_match = re.search(r'\{.*\}', response_str, re.DOTALL)
            if not json_match:
                error_snippet = response_str.strip().replace('\n', ' ')[:200]
                logging.error(f"LLM did not return valid JSON for {file_name}. Response: {error_snippet}...")
                raise ValueError("LLM did not return a valid JSON object.")

            clean_json_str = json_match.group(0)
            metadata_json = json.loads(clean_json_str)

            # Normalize common validation fields before Pydantic
            if 'proposal_outcome' in metadata_json and isinstance(metadata_json['proposal_outcome'], str):
                metadata_json['proposal_outcome'] = metadata_json['proposal_outcome'].title()

            try: 
                rich_metadata = RichMetadata.model_validate(metadata_json)
            except ValidationError as e:
                logging.warning(f"Initial validation failed for {file_name}. Retrying with nested key check...")
                if isinstance(metadata_json, dict) and len(metadata_json) == 1:
                    nested_key = list(metadata_json.keys())[0]
                    if isinstance(metadata_json[nested_key], dict): 
                        rich_metadata = RichMetadata.model_validate(metadata_json[nested_key])
                    else: 
                        raise e
                else: 
                    raise e
            logging.info(f"Successfully parsed and validated metadata for {file_name}.")
            
            # Extract entities and relationships
            logging.info(f"Extracting entities and relationships from {file_name}...")
            entities_dict, relationships_dict = extract_entities_and_relationships(
                doc.get_content()[:8000],
                {
                    'document_type': rich_metadata.document_type,
                    'file_name': file_name,
                    'summary': rich_metadata.summary,
                    'thematic_tags': rich_metadata.thematic_tags
                },
                entity_extractor
            )
            logging.info(f"Extracted {len(entities_dict)} entities and {len(relationships_dict)} relationships.")
            
        except Exception as e:
            logging.error(f"CRITICAL ERROR analyzing {file_name}: {e}", exc_info=True)
            default_doc_type = "Image/Diagram" if source_type == 'image' else 'Other'
            rich_metadata = RichMetadata(
                document_type=default_doc_type, 
                proposal_outcome="N/A", 
                summary=f"ERROR: Could not analyze document. Reason: {e}", 
                thematic_tags=["error", "analysis-failed"]
            )
            entities_dict = []
            relationships_dict = []

        doc_meta = DocumentMetadata(
            doc_id=get_file_hash(file_path_str), 
            doc_posix_path=Path(file_path_str).as_posix(),
            file_name=doc.metadata.get('file_name'), 
            last_modified_date=str(datetime.fromtimestamp(os.path.getmtime(file_path_str))),
            rich_metadata=rich_metadata,
            extracted_entities=entities_dict,
            extracted_relationships=relationships_dict
        )
        staged_docs.append(doc_meta.model_dump())

    with open(STAGING_FILE, 'w') as f: 
        json.dump(staged_docs, f, indent=2)
    logging.info(f"--- Analysis complete. {len(staged_docs)} documents written to staging file. ---")

def finalize_ingestion(db_path: str):
    logging.info(f"--- Starting Stage 3: Finalize from Staging with Graph Building (Cortex v13.0.0) ---")
    if not os.path.exists(STAGING_FILE): 
        logging.error("Staging file not found.")
        return
    
    chroma_db_path = os.path.join(db_path, "knowledge_hub_db")
    os.makedirs(chroma_db_path, exist_ok=True)
    
    # Initialize graph manager
    graph_file_path = os.path.join(db_path, "knowledge_cortex.gpickle")
    graph_manager = EnhancedGraphManager(graph_file_path)
    
    with open(STAGING_FILE, 'r') as f: 
        docs_to_process = [DocumentMetadata(**data) for data in json.load(f)]

    docs_to_index_paths, metadata_map, doc_ids_to_add_to_default = [], {}, []
    processed_log_path = os.path.join(chroma_db_path, INGESTED_FILES_LOG)
    
    for doc_meta in docs_to_process:
        if doc_meta.exclude_from_final:
            logging.info(f"User excluded {doc_meta.file_name}. Skipping.")
            write_to_processed_log(processed_log_path, doc_meta.doc_posix_path, doc_meta.doc_id)
            continue
        if doc_meta.rich_metadata and "ERROR:" in doc_meta.rich_metadata.summary:
            logging.warning(f"Skipping finalization for {doc_meta.file_name} due to prior analysis error.")
            continue
        
        docs_to_index_paths.append(doc_meta.doc_posix_path)
        metadata_map[doc_meta.doc_posix_path] = doc_meta
        doc_ids_to_add_to_default.append(doc_meta.doc_id)
        
        # Add document to graph
        logging.info(f"Adding {doc_meta.file_name} to knowledge graph...")
        graph_manager.add_entity(
            doc_meta.doc_id,
            'Document',
            file_name=doc_meta.file_name,
            document_type=doc_meta.rich_metadata.document_type if doc_meta.rich_metadata else 'Unknown',
            summary=doc_meta.rich_metadata.summary if doc_meta.rich_metadata else '',
            last_modified=doc_meta.last_modified_date,
            posix_path=doc_meta.doc_posix_path
        )
        
        # Add entities and relationships to graph
        entity_map = {}  # Track entity names to node IDs
        
        for entity_dict in doc_meta.extracted_entities:
            entity_id = f"{entity_dict['entity_type']}:{entity_dict['name']}"
            entity_map[entity_dict['name']] = entity_id
            
            # Add entity if it doesn't exist
            if entity_id not in graph_manager.graph:
                graph_manager.add_entity(
                    entity_id,
                    entity_dict['entity_type'],
                    name=entity_dict['name'],
                    aliases=entity_dict.get('aliases', [])
                )
            
            # Link entity to document
            if entity_dict['entity_type'] == 'person':
                graph_manager.add_relationship(
                    entity_id,
                    doc_meta.doc_id,
                    'authored'
                )
            elif entity_dict['entity_type'] == 'organization':
                graph_manager.add_relationship(
                    entity_id,
                    doc_meta.doc_id,
                    'client_of'
                )
            else:
                graph_manager.add_relationship(
                    entity_id,
                    doc_meta.doc_id,
                    'mentioned_in'
                )
        
        # Add extracted relationships
        for rel_dict in doc_meta.extracted_relationships:
            source_id = entity_map.get(rel_dict['source'], rel_dict['source'])
            target_id = entity_map.get(rel_dict['target'], rel_dict['target'])
            
            # Handle special case where target might be the document
            if rel_dict['target'] == doc_meta.file_name:
                target_id = doc_meta.doc_id
            
            graph_manager.add_relationship(
                source_id,
                target_id,
                rel_dict['relationship_type'],
                context=rel_dict.get('context', '')
            )

    if not docs_to_index_paths:
        logging.warning("No new, valid documents to ingest. Finalization complete.")
        if os.path.exists(STAGING_FILE): 
            os.remove(STAGING_FILE)
        return

    # Save the graph
    graph_manager.save_graph()
    logging.info(f"Knowledge graph saved to {graph_file_path}")

    # Continue with regular vector indexing
    documents_for_indexing = manual_load_documents(docs_to_index_paths)
    for doc in documents_for_indexing:
        path_key = doc.metadata['file_path']
        if path_key in metadata_map:
            doc_meta = metadata_map[path_key]
            doc.doc_id = doc_meta.doc_id
            flat_metadata = {
                "doc_id": doc_meta.doc_id, 
                "file_name": doc_meta.file_name, 
                "doc_posix_path": doc_meta.doc_posix_path, 
                "last_modified_date": doc_meta.last_modified_date
            }
            if doc_meta.rich_metadata:
                flat_metadata.update(doc_meta.rich_metadata.model_dump())
                flat_metadata['thematic_tags'] = ', '.join(flat_metadata.get('thematic_tags', []))
            doc.metadata = flat_metadata

    db_settings = ChromaSettings(anonymized_telemetry=False)
    chroma_client = chromadb.PersistentClient(path=chroma_db_path, settings=db_settings)
    chroma_collection = chroma_client.get_or_create_collection(COLLECTION_NAME)
    vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
    storage_context = StorageContext.from_defaults(vector_store=vector_store)
    index = VectorStoreIndex.from_documents(documents_for_indexing, storage_context=storage_context, show_progress=True)
    logging.info(f"Persisting index to disk at {chroma_db_path}...")
    index.storage_context.persist(persist_dir=chroma_db_path)
    
    for doc in documents_for_indexing: 
        write_to_processed_log(processed_log_path, doc.metadata['doc_posix_path'], doc.metadata['doc_id'])
    os.remove(STAGING_FILE)

    if doc_ids_to_add_to_default:
        logging.info(f"Adding {len(doc_ids_to_add_to_default)} new documents to the 'default' collection.")
        try:
            collections_data = {}
            if os.path.exists(COLLECTIONS_FILE):
                with open(COLLECTIONS_FILE, 'r') as f: 
                    collections_data = json.load(f)
            if "default" not in collections_data: 
                collections_data["default"] = {"name": "default", "doc_ids": []}
            existing_ids = set(collections_data["default"].get("doc_ids", []))
            new_ids_added = 0
            for doc_id in doc_ids_to_add_to_default:
                if doc_id not in existing_ids: 
                    collections_data["default"]["doc_ids"].append(doc_id)
                    existing_ids.add(doc_id)
                    new_ids_added += 1
            if new_ids_added > 0:
                with open(COLLECTIONS_FILE, 'w') as f: 
                    json.dump(collections_data, f, indent=4)
                logging.info(f"Successfully updated 'default' collection.")
        except Exception as e: 
            logging.error(f"Could not automatically add documents to default collection: {e}")
    
    logging.info("--- Finalization complete. Knowledge base and graph are up to date. ---")

def main():
    parser = argparse.ArgumentParser(description="Cortex Ingestion Engine with GraphRAG")
    parser.add_argument("--db-path", type=str, required=True)
    parser.add_argument("--include", type=str, nargs='*')
    parser.add_argument("--analyze-only", action="store_true")
    parser.add_argument("--finalize-from-staging", action="store_true")
    parser.add_argument("--fresh", action="store_true")
    args = parser.parse_args()

    initialize_script()

    if args.analyze_only:
        if not args.include: 
            logging.error("--include paths are required.")
            sys.exit(1)
        analyze_documents(args.include, args.fresh)
    elif args.finalize_from_staging:
        finalize_ingestion(args.db_path)
    else:
        logging.error("Specify either --analyze-only or --finalize-from-staging.")
        sys.exit(1)

if __name__ == "__main__":
    main()


--- FILE: ./cortex_engine/instruction_parser.py ---
# ## File: cortex_engine/instruction_parser.py
# Version: 4.0.0 (Unified Parser)
# Date: 2025-07-15
# Purpose: A unified module to parse .docx files and define instruction structures.
#          - CRITICAL FIX (v4.0.0): Restored the 'parse_template_for_instructions'
#            function required by the Proposal Co-pilot, which was erroneously
#            removed during previous refactoring. This file now contains all
#            necessary parsing functions for the entire suite.

import re
import docx
import io
from typing import List, Optional, NamedTuple, Dict

# This is the instruction object used by the original, powerful co-pilot.
class CortexInstruction(NamedTuple):
    section_heading: str
    instruction_raw: str
    task_type: str
    parameter: str
    placeholder_paragraph: docx.text.paragraph.Paragraph
    sub_instruction: Optional[str] = None

INSTRUCTION_REGEX = re.compile(r"\[([A-Z_]+)(?:::(.*?))?\]")

def parse_template_for_instructions(doc: docx.document.Document) -> List[CortexInstruction]:
    """
    (RESTORED) Parses a docx Document object to find Cortex instructions
    and sub-instructions for the Proposal Co-pilot.
    """
    instructions: List[CortexInstruction] = []
    paragraphs = list(doc.paragraphs)

    for i, p in enumerate(paragraphs):
        p_text = p.text.strip()
        match = INSTRUCTION_REGEX.search(p_text)

        if match:
            task_type, parameter = match.groups()
            parameter = parameter or ""
            heading_text = "Unknown Section"
            sub_instruction_text = None

            if (i + 1) < len(paragraphs):
                next_p_text = paragraphs[i + 1].text.strip()
                if next_p_text.startswith("##"):
                    sub_instruction_text = next_p_text.lstrip('#- ').strip()

            for j in range(i - 1, -1, -1):
                prev_p = paragraphs[j]
                prev_p_text = prev_p.text.strip()
                is_heading_style = prev_p.style.name.startswith('Heading')
                is_heading_text_marker = prev_p_text.lower().startswith('section:')

                if is_heading_style or is_heading_text_marker:
                    heading_text_raw = prev_p_text.split(':', 1)[1] if is_heading_text_marker else prev_p_text
                    heading_text = heading_text_raw.strip().strip(' "\'‚ñ†\t*-')
                    break

            instruction = CortexInstruction(
                section_heading=heading_text,
                instruction_raw=p_text,
                task_type=task_type.strip(),
                parameter=parameter.strip(),
                placeholder_paragraph=p,
                sub_instruction=sub_instruction_text
            )
            instructions.append(instruction)

    return instructions

# This function is used by the new Template Editor.
def iter_block_items(parent):
    """
    Yield each paragraph and table child within *parent*, in document order.
    """
    if isinstance(parent, docx.document.Document):
        parent_elm = parent.element.body
    elif isinstance(parent, docx.table._Cell):
        parent_elm = parent._tc
    else:
        try:
            parent_elm = parent._element
        except AttributeError:
            raise ValueError("Unsupported parent type for iter_block_items")

    for child in parent_elm.iterchildren():
        if isinstance(child, docx.oxml.text.paragraph.CT_P):
            yield docx.text.paragraph.Paragraph(child, parent)
        elif isinstance(child, docx.oxml.table.CT_Tbl):
            yield docx.table.Table(child, parent)


--- FILE: ./cortex_engine/proposal_manager.py ---
# ## File: cortex_engine/proposal_manager.py
# Version: 1.1.0 (Save/Resume Fix)
# Date: 2025-07-13
# Purpose: Manages the lifecycle of proposals, including creating, saving,
#          loading, listing, and deleting.
#          - FIX (v1.1.0): Made the save_proposal function robust by handling
#            both bytes and io.BytesIO objects for generated documents. This
#            resolves a TypeError during the save process after document assembly.

import os
import json
import shutil
import uuid
import io
from pathlib import Path
from datetime import datetime

# Define the directory where all proposal data will be stored
PROPOSALS_DIR = Path(__file__).parent.parent / "proposals"

class ProposalManager:
    """Handles all file-based operations for managing proposal state."""

    def __init__(self):
        """Ensures the main proposals directory exists."""
        PROPOSALS_DIR.mkdir(exist_ok=True)

    def list_proposals(self) -> list:
        """
        Scans the proposals directory and returns a list of all proposals
        with their metadata.
        """
        proposals = []
        for proposal_dir in PROPOSALS_DIR.iterdir():
            if proposal_dir.is_dir():
                state_file = proposal_dir / "state.json"
                if state_file.exists():
                    try:
                        # <<<--- LOADING from "state.json"
                        with open(state_file, 'r') as f:
                            state_data = json.load(f)
                        proposals.append({
                            "id": proposal_dir.name,
                            "name": state_data.get("name", "Untitled Proposal"),
                            "status": state_data.get("status", "Drafting"),
                            "last_modified": datetime.fromtimestamp(state_file.stat().st_mtime),
                        })
                    except (json.JSONDecodeError, IOError):
                        continue # Skip corrupted state files
        proposals.sort(key=lambda p: p['last_modified'], reverse=True)
        return proposals

    def create_proposal(self, name: str) -> str:
        """
        Creates a new proposal directory and an initial state file.
        Returns the unique ID of the new proposal.
        """
        proposal_id = str(uuid.uuid4())
        proposal_dir = PROPOSALS_DIR / proposal_id
        proposal_dir.mkdir() # <<<--- CREATING directory on disk

        initial_state = {
            "name": name,
            "status": "Drafting",
            "created_at": datetime.now().isoformat(),
            "session_state": {}
        }
        # <<<--- SAVING to "state.json"
        with open(proposal_dir / "state.json", 'w') as f:
            json.dump(initial_state, f, indent=4)

        return proposal_id

    def save_proposal(self, proposal_id: str, session_data: dict, template_bytes: bytes, generated_doc_bytes: bytes = None):
        """
        Saves the complete state of a proposal, including the session data
        and the template/draft .docx files.
        """
        proposal_dir = PROPOSALS_DIR / proposal_id
        if not proposal_dir.exists():
            raise FileNotFoundError(f"Proposal with ID {proposal_id} not found.")

        state_file = proposal_dir / "state.json"

        with open(state_file, 'r') as f:
            state_data = json.load(f)

        state_data['session_state'] = session_data
        state_data['status'] = "Drafting"

        # <<<--- SAVING to "state.json"
        with open(state_file, 'w') as f:
            json.dump(state_data, f, indent=4)

        if template_bytes:
            # <<<--- SAVING to "template.docx"
            with open(proposal_dir / "template.docx", "wb") as f:
                f.write(template_bytes)

        if generated_doc_bytes:
            # FIX: Check if the object is a BytesIO stream and get its value if so.
            doc_bytes_to_write = generated_doc_bytes.getvalue() if isinstance(generated_doc_bytes, io.BytesIO) else generated_doc_bytes
            
            if doc_bytes_to_write:
                # <<<--- SAVING to "draft.docx"
                with open(proposal_dir / "draft.docx", "wb") as f:
                    f.write(doc_bytes_to_write)

    def load_proposal(self, proposal_id: str) -> dict:
        """
        Loads all data for a given proposal ID and returns it in a dictionary.
        """
        proposal_dir = PROPOSALS_DIR / proposal_id
        if not proposal_dir.exists():
            return None

        state_file = proposal_dir / "state.json"
        template_file = proposal_dir / "template.docx"
        draft_file = proposal_dir / "draft.docx"

        loaded_data = {}
        # <<<--- LOADING from "state.json"
        with open(state_file, 'r') as f:
            loaded_data['state'] = json.load(f)

        if template_file.exists():
             # <<<--- LOADING from "template.docx"
             with open(template_file, "rb") as f:
                loaded_data['template_bytes'] = f.read()

        if draft_file.exists():
             # <<<--- LOADING from "draft.docx"
             with open(draft_file, "rb") as f:
                loaded_data['generated_doc_bytes'] = f.read()

        return loaded_data

    def delete_proposal(self, proposal_id: str):
        """Permanently deletes a proposal's directory."""
        proposal_dir = PROPOSALS_DIR / proposal_id
        if proposal_dir.exists():
            shutil.rmtree(proposal_dir) # <<<--- DELETING directory from disk

    def update_proposal_status(self, proposal_id: str, new_status: str):
        """Updates just the status of a proposal."""
        proposal_dir = PROPOSALS_DIR / proposal_id
        state_file = proposal_dir / "state.json"
        if state_file.exists():
            with open(state_file, 'r') as f:
                state_data = json.load(f)
            state_data['status'] = new_status
            # <<<--- SAVING to "state.json"
            with open(state_file, 'w') as f:
                json.dump(state_data, f, indent=4)


--- FILE: ./cortex_engine/query_cortex.py ---
# ## File: query_cortex.py
# Version: 5.0.0 (VLM Utility for Ingestion)
# Date: 2025-07-22
# Purpose: Backend module providing models and prompts.
#          - FEATURE (v5.0.0): Re-introduced a VLM utility function, specifically
#            scoped for use during the ingestion process. This allows images to
#            be described by a local model like Llava without complicating the
#            main query engine.

from llama_index.core import Settings
from llama_index.llms.ollama import Ollama
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
import torch
import ollama
import base64

from .config import LLM_MODEL, EMBED_MODEL, VLM_MODEL

# --- Prompt Templates ---

FINAL_SYNTHESIS_PROMPT = """
You are a helpful AI assistant for Project Cortex. Your task is to synthesize a clear and concise answer to the user's question based *only* on the provided context, which may contain both text and image descriptions.
Do not use any prior knowledge. If the context does not contain the answer, state that clearly.
Reference the source documents by their filenames when possible.

QUESTION:
{question}

CONTEXT:
---
{context}
---

ANSWER:
"""

AUTHOR_SUMMARY_PROMPT = """
You are a helpful AI assistant for Project Cortex. You have been asked to provide a summary of works by a specific author based on the provided context.
Synthesize a summary of the documents and topics associated with this author.
List the document titles you found. Do not use any prior knowledge.

AUTHOR:
{author_name}

CONTEXT:
---
{context}
---

SUMMARY OF WORKS:
"""

VLM_QA_PROMPT = """
You are a helpful AI assistant for Project Cortex. Your task is to synthesize a clear and concise answer to the user's question based on the provided context, which includes text and descriptions of images.
When an image is relevant, explicitly mention it in your answer (e.g., "According to the image description of the Venn diagram...").
Do not use any prior knowledge. If the context does not contain the answer, state that clearly.

QUESTION:
{question}

CONTEXT:
---
{context}
---

ANSWER:
"""

# --- Model Setup ---

def setup_models():
    """Configures and initializes the LLM and embedding model."""
    print("--- Configuring query models ---")
    device = "cuda" if torch.cuda.is_available() else "cpu"

    Settings.llm = Ollama(model=LLM_MODEL, request_timeout=300.0)
    Settings.embed_model = HuggingFaceEmbedding(model_name=EMBED_MODEL, device=device)

    print(f"‚úÖ Query models configured (LLM: {LLM_MODEL}, Embed: {EMBED_MODEL}).")


# --- SPRINT 21: VLM Utility for Ingestion ---
def describe_image_with_vlm_for_ingestion(image_path: str) -> str:
    """
    Uses a local VLM via Ollama to generate a text description for an image file.
    This function is intended to be called ONLY during the ingestion process.

    Args:
        image_path: The file path to the image.

    Returns:
        A text description of the image, or an error message.
    """
    try:
        print(f"  -> VLM Ingestion: Describing '{image_path}' with model '{VLM_MODEL}'...")
        with open(image_path, "rb") as image_file:
            encoded_image = base64.b64encode(image_file.read()).decode('utf-8')

        # This uses the official ollama client, which is robust for this task.
        client = ollama.Client()
        response = client.chat(
            model=VLM_MODEL,
            messages=[
                {
                    'role': 'user',
                    'content': 'Describe this image in detail for a searchable knowledge base. Be specific about any text, diagrams, or data presented. What is the key information conveyed by this image?',
                    'images': [encoded_image]
                }
            ],
            options={"temperature": 0.2} # Lower temperature for more factual descriptions
        )
        description = response['message']['content']
        print(f"  -> VLM Ingestion: Success. Description length: {len(description)} chars.")
        return description
    except FileNotFoundError:
        error_msg = f"VLM Error: Image file not found at {image_path}"
        print(f"  -> {error_msg}")
        return error_msg
    except Exception as e:
        error_msg = f"VLM Error: An unexpected error occurred while processing {image_path}. Is Ollama running and the '{VLM_MODEL}' model pulled? Error: {e}"
        print(f"  -> {error_msg}")
        return error_msg


--- FILE: ./cortex_engine/session_state.py ---
# ## File: cortex_engine/session_state.py
# Version: 3.2.0 (Circular Import Fix)
# Date: 2025-07-16
# Purpose: Centralized session state management for the Cortex Suite Streamlit app.
#          - CRITICAL FIX (v3.2.0): Resolved a circular import error by moving
#            the `WorkingCollectionManager` import inside the function where it
#            is used (deferred import).

import streamlit as st
from .config import DEFAULT_EXCLUSION_PATTERNS_STR
from .config_manager import ConfigManager
# CRITICAL FIX: The import below has been removed from the top level.
# from .collection_manager import WorkingCollectionManager

def initialize_app_session_state():
    """
    Initializes and refreshes all shared session state variables.
    Paths are always re-loaded from the persistent config file to ensure
    they are up-to-date across pages.
    """
    # CRITICAL FIX: Import is deferred to runtime inside the function.
    from .collection_manager import WorkingCollectionManager

    config_manager = ConfigManager()
    persistent_config = config_manager.get_config()

    # --- Paths (Always refresh from config file) ---
    st.session_state.db_path_input = persistent_config.get("ai_database_path", "")
    st.session_state.knowledge_source_path = persistent_config.get("knowledge_source_path", "")

    # --- Collections (Always refresh from file) ---
    st.session_state.collections = WorkingCollectionManager().collections

    # --- Other state (Initialize only if not present) ---
    defaults = {
        "model_provider": "Local",
        "openai_api_key": "",
        "selected_collection": "default",
        "new_collection_name": "",
    }
    for key, val in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = val


--- FILE: ./cortex_engine/synthesise.py ---
# ## File: cortex_engine/synthesise.py
# Version: 5.0.0 (Regression Fix & Citation Enhancement)
# Date: 2025-07-22
# Purpose: Backend engine for the multi-agent AI Research Assistant.
#          - CRITICAL FIX (v5.0.0): Reverted the foundational query agent and
#            source fetching logic to the more robust implementation from v3.1.0.
#            This resolves a critical regression where no foundational sources
#            were being found.
#          - FEATURE (v5.0.0): Significantly improved the prompt for the
#            `agent_deep_researcher` to explicitly instruct the LLM on how
#            to create and use a numbered reference list, fixing the bug where
#            citations were missing in the final report.

import os
import openai
import graphviz
import time
import threading
import json
import re
import requests
import ollama
import shutil
import subprocess
import random
from pathlib import Path
from dotenv import load_dotenv
from youtube_transcript_api import YouTubeTranscriptApi
from googleapiclient.discovery import build
from pydantic import BaseModel, Field, ValidationError
from typing import List, Dict, Any, Type, Tuple

from llama_index.llms.ollama import Ollama as LlamaOllama
from llama_index.llms.openai import OpenAI as LlamaOpenAI
from llama_index.core.llms import LLM

# --- Configuration & Schemas ---
OUTPUT_DIR = Path(__file__).parent.parent / "external_research"
OUTPUT_DIR.mkdir(exist_ok=True)
load_dotenv()

# RESTORED from v3.1.0 to fix regression
class FoundationalQueries(BaseModel):
    """Queries for foundational, authoritative papers."""
    highly_cited_query: str = Field(..., description="A single, precise query for finding the most cited papers on the topic.")
    review_queries: List[str] = Field(..., description="A list of queries targeting systematic reviews, literature reviews, or meta-analyses.")

class ExploratoryQueries(BaseModel):
    """Queries for broad, exploratory research."""
    scholar_queries: List[str] = Field(..., description="A list of diverse, general-purpose academic search queries.")
    youtube_queries: List[str] = Field(..., description="A list of accessible search queries for YouTube videos.")


class ThemeList(BaseModel):
    """Data model for a list of themes."""
    themes: List[str] = Field(..., description="A list of distinct, high-level thematic categories.")

def _convert_windows_to_wsl_path(win_path):
    if not isinstance(win_path, str) or win_path.startswith('/mnt/'): return win_path
    path_str = str(win_path).replace('\\', '/')
    match = re.match(r"^([a-zA-Z]):/(.*)", path_str)
    if match:
        drive, rest = match.groups()
        return f"/mnt/{drive.lower()}/{rest}"
    return win_path

def _normalize_json_keys(data: Dict[str, Any], model: Type[BaseModel]) -> Dict[str, Any]:
    normalized_data = {}
    model_fields = list(model.model_fields.keys())
    field_map = {re.sub(r'[\s-]', '_', f).lower(): f for f in model_fields}
    for key, value in data.items():
        normalized_key = re.sub(r'[\s-]', '_', key).lower()
        if normalized_key in field_map:
            correct_key = field_map[normalized_key]
            normalized_data[correct_key] = value
    return normalized_data

# --- Setup & LLM Completion ---

class GeminiRest:
    """A robust wrapper to call the Gemini API via a direct REST request."""
    def __init__(self, api_key: str, model_name: str = "gemini-1.5-flash"):
        self.api_key = api_key
        self.model_name = model_name
        self.api_url = f"https://generativelanguage.googleapis.com/v1/models/{self.model_name}:generateContent"

    def complete(self, prompt: str, generation_config: dict = None) -> str:
        headers = {'Content-Type': 'application/json'}
        params = {'key': self.api_key}
        payload = {"contents": [{"parts": [{"text": prompt}]}]}
        try:
            response = requests.post(self.api_url, headers=headers, params=params, json=payload, timeout=180) # Increased timeout
            response.raise_for_status()
            response_json = response.json()
            if 'candidates' in response_json and response_json['candidates']:
                return response_json['candidates'][0]['content']['parts'][0]['text']
            else:
                return f'{{"error": "API returned no candidates. Prompt may have been blocked.", "response": {response.text}}}'
        except requests.exceptions.RequestException as e:
            error_details = str(e)
            print(f"Error during Gemini REST API call: {error_details}")
            return f'{{"error": "Failed to connect to Gemini API.", "details": "{error_details}"}}'

LLM_PROVIDER = os.getenv("LLM_PROVIDER", "gemini").lower()
llm: Any = None
GRAPHVIZ_DOT_EXECUTABLE = os.getenv("GRAPHVIZ_DOT_EXECUTABLE")
YOUTUBE_API_KEY = os.getenv("YOUTUBE_API_KEY")
MAX_PAPERS_PER_QUERY, MAX_VIDEOS_PER_QUERY = 3, 1

def get_llm(status_callback=print):
    global llm
    if llm is not None:
        return llm

    status_callback(f"üöÄ Initializing LLM provider: {LLM_PROVIDER.upper()}")
    if LLM_PROVIDER == "gemini":
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key: raise ValueError("GEMINI_API_KEY not found in .env file.")
        llm = GeminiRest(api_key=api_key)
        status_callback(f"‚úÖ Gemini LLM (gemini-1.5-flash via Direct REST API v1) is ready.")
    elif LLM_PROVIDER == "openai":
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key: raise ValueError("OPENAI_API_KEY not found in .env file.")
        llm = LlamaOpenAI(model="gpt-4o-mini", api_key=api_key)
        status_callback(f"‚úÖ OpenAI LLM (gpt-4o-mini) is ready.")
    elif LLM_PROVIDER == "ollama":
        model_name = os.getenv("OLLAMA_MODEL", "mistral:7b-instruct-v0.3-q4_K_M")
        llm = LlamaOllama(model=model_name, request_timeout=120.0)
        status_callback(f"‚úÖ Ollama LLM ({model_name}) is ready.")
    else:
        raise ValueError(f"Unsupported LLM_PROVIDER: {LLM_PROVIDER}.")
    return llm

def llm_completion(prompt: str, status_callback=print, is_json=False) -> str:
    status_callback(f"üß† Generating text with {LLM_PROVIDER.upper()}...")
    try:
        llm_instance = get_llm(status_callback)
        response = llm_instance.complete(prompt)
        if response.strip().startswith('{"error"'):
             status_callback(f"‚ùå LLM provider error: Received an error object from the API wrapper.")
             status_callback(f"   RAW ERROR: {response}")
             return ""
        return str(response)
    except Exception as e:
        status_callback(f"\n‚ùå Unhandled exception in llm_completion: {e}")
        return ""

# --- Agent Definitions ---
# RESTORED from v3.1.0 to fix regression
def agent_foundational_query_crafter(topic: str, status_callback=print) -> dict:
    status_callback(f"ü§ñ Agent Foundational Query Crafter: Seeking authoritative sources for '{topic}'...")
    json_prompt = f"""You are an expert academic researcher. For the given topic, create a JSON object with two keys:
1.  `highly_cited_query`: A single, precise query string to find the most influential, highly-cited papers.
2.  `review_queries`: A list of 2-3 query strings to find "systematic review", "literature review", or "meta-analysis" papers.
Topic: "{topic}"
Return ONLY a valid JSON object. Do not add any other text.
Example Format:
{{
  "highly_cited_query": "highly cited papers on AI in healthcare",
  "review_queries": ["systematic review of AI in medicine", "literature review of machine learning in diagnostics"]
}}"""
    try:
        llm_output_str = llm_completion(json_prompt, status_callback=status_callback, is_json=True)
        cleaned_json_str = re.sub(r'```json\s*(.*)\s*```', r'\1', llm_output_str, flags=re.DOTALL).strip()
        raw_data = json.loads(cleaned_json_str)
        normalized_data = _normalize_json_keys(raw_data, FoundationalQueries)
        query_object = FoundationalQueries.model_validate(normalized_data)
        queries = query_object.model_dump()
        status_callback(f"‚úÖ Crafted Foundational Queries (validated): {queries}")
        return queries
    except Exception as e:
        status_callback(f"‚ùå Critical failure during foundational query generation. Error: {e}")
        return {"highly_cited_query": f"highly cited papers on {topic}", "review_queries": [f"review of {topic}"]}

def agent_exploratory_query_crafter(topic: str, status_callback=print) -> dict:
    status_callback(f"ü§ñ Agent Exploratory Query Crafter: Broadening search for '{topic}'...")
    json_prompt = f'You are an expert query generator. Based on the following topic, create a JSON object containing two lists of search queries: one for general academic papers and one for YouTube videos.\nTopic: "{topic}"\n\nReturn ONLY a valid JSON object in the following format. Do not add any other text.\nExample Format:\n{{\n  "scholar_queries": ["academic query 1", "academic query 2"],\n  "youtube_queries": ["video query a", "video query b"]\n}}'
    try:
        llm_output_str = llm_completion(json_prompt, status_callback=status_callback, is_json=True)
        cleaned_json_str = re.sub(r'```json\s*(.*)\s*```', r'\1', llm_output_str, flags=re.DOTALL).strip()
        raw_data = json.loads(cleaned_json_str)
        normalized_data = _normalize_json_keys(raw_data, ExploratoryQueries)
        query_object = ExploratoryQueries.model_validate(normalized_data)
        queries = query_object.model_dump()
        status_callback(f"‚úÖ Crafted Exploratory Queries (validated): {queries}")
        return queries
    except Exception as e:
        status_callback(f"‚ùå Critical failure during exploratory query generation. Error: {e}")
        return {"scholar_queries": [topic], "youtube_queries": [topic]}


def agent_thematic_analyser(context: str, status_callback=print, existing_themes: List[str] = None) -> List[str]:
    status_callback("üî¨ Agent Thematic Analyser: Identifying and structuring core themes...")
    llm_output_str = ""
    default_error_response = ["Could not determine themes from the provided text."]
    if existing_themes:
        status_callback("  -> Finding net-new themes to ADD to the existing list...")
        existing_themes_str = "\n".join(f"- {theme}" for theme in existing_themes)
        additive_prompt = f"""You are a research analyst. Your task is to identify ONLY the new, high-level themes from the 'CONTEXT TO ANALYZE' that are NOT already present in the 'EXISTING THEMES' list.
**EXISTING THEMES (to avoid duplicating):**
{existing_themes_str}
**CONTEXT TO ANALYZE:**
---
{context}
---
**YOUR INSTRUCTIONS:**
1.  Analyze the 'CONTEXT TO ANALYZE' for its main themes.
2.  Compare these themes against the 'EXISTING THEMES' list.
3.  Return a JSON object containing a list of **ONLY THE NEW THEMES** that you found.
4.  If no new themes are found, return a JSON object with an empty list: {{"themes": []}}.
5.  Do NOT include the existing themes in your response.
Return ONLY a valid JSON object with a "themes" key. Do not add any other text.
"""
        prompt = additive_prompt
    else:
        status_callback("  -> Identifying themes from scratch...")
        prompt = f"""Analyze the provided research context to identify 3-5 distinct, high-level themes. Each theme must be a high-level concept, not a specific detail. Themes must not overlap.
If you analyze the context and cannot determine at least 3 clear, distinct themes, you MUST return a JSON object with a single theme that says: "Could not determine themes from the provided text."
Context to analyze:
---
{context}
---
Return ONLY a valid JSON object with a "themes" key. Do not add any other text.
Example Format: {{"themes": ["First theme", "Second theme", "Third theme"]}}
"""
    try:
        llm_output_str = llm_completion(prompt, status_callback=status_callback, is_json=True)
        if not llm_output_str or not llm_output_str.strip():
            raise ValueError("LLM returned an empty response.")
        cleaned_json_str = re.sub(r'```json\s*(.*)\s*```', r'\1', llm_output_str, flags=re.DOTALL).strip()
        raw_data = json.loads(cleaned_json_str)
        if not isinstance(raw_data, dict) or not raw_data:
            return [] if existing_themes else default_error_response
        normalized_data = _normalize_json_keys(raw_data, ThemeList)
        if 'themes' not in normalized_data or not normalized_data['themes']:
            return [] if existing_themes else default_error_response
        theme_object = ThemeList.model_validate(normalized_data)
        themes_list = theme_object.themes
        status_callback(f"‚úÖ Identified and validated themes: {themes_list}")
        return themes_list
    except Exception as e:
        status_callback(f"‚ùå Critical failure during theme generation. Error: {e}\nRAW LLM OUTPUT:\n{llm_output_str}")
        return [] if existing_themes else default_error_response

# --- Data Retrieval Agents ---
def agent_paper_retriever(query: str, sort_by: str = None, status_callback=print) -> Tuple[bool, list or str]:
    search_type = f"'{sort_by}'" if sort_by else "standard"
    status_callback(f"üìö Agent Paper Retriever ({search_type} search): Searching for '{query}'...")
    base_url = "https://api.semanticscholar.org/graph/v1/paper/search"
    params = {'query': query, 'limit': MAX_PAPERS_PER_QUERY, 'fields': 'title,url,abstract,citationCount'}
    if sort_by:
        params['sort'] = sort_by

    MAX_RETRIES, INITIAL_BACKOFF = 5, 2
    for attempt in range(MAX_RETRIES):
        try:
            if attempt > 0:
                backoff_time = INITIAL_BACKOFF * (2 ** (attempt-1)) + random.uniform(0, 1)
                status_callback(f"   - Rate limit hit for '{query}'. Retrying in {backoff_time:.2f} seconds...")
                time.sleep(backoff_time)
            else:
                 time.sleep(1.5) # Initial delay

            response = requests.get(base_url, params=params, timeout=20)
            response.raise_for_status()

            data, papers = response.json(), []
            for item in data.get('data', []):
                papers.append({
                    "source_type": "paper", "title": item.get('title'),
                    "text": item.get('abstract') or 'No abstract available.',
                    "url": item.get('url'), "citations": item.get('citationCount', 0)
                })
            if not papers: status_callback(f"üü° Paper search for '{query}' was successful but returned no results.")
            return True, papers

        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 429 and attempt < MAX_RETRIES - 1:
                continue
            else:
                error_message = f"Paper search for '{query}' failed. Reason: {e}"
                status_callback(f"‚ùå {error_message}")
                return False, error_message
        except requests.exceptions.RequestException as e:
            error_message = f"Paper search for '{query}' failed. Reason: {e}"
            status_callback(f"‚ùå {error_message}")
            return False, error_message

    final_error_message = f"Paper search for '{query}' failed after {MAX_RETRIES} retries."
    status_callback(f"‚ùå {final_error_message}")
    return False, final_error_message


def agent_youtube_extractor(query: str, status_callback=print) -> Tuple[bool, list or str]:
    status_callback(f"üì∫ Agent YouTube Extractor: Searching for '{query}'...")
    if not YOUTUBE_API_KEY:
        error_message = "YouTube search failed: YOUTUBE_API_KEY is not configured in your .env file."
        status_callback(f"‚ùå {error_message}")
        return False, error_message
    try:
        youtube = build('youtube', 'v3', developerKey=YOUTUBE_API_KEY)
        search_response = youtube.search().list(q=query, part='id,snippet', maxResults=MAX_VIDEOS_PER_QUERY, type='video').execute()
        videos, video_ids_processed = [], set()
        for item in search_response.get('items', []):
            video_id = item['id']['videoId']
            if video_id in video_ids_processed: continue
            video_ids_processed.add(video_id)
            title, description = item['snippet']['title'], item['snippet']['description']
            url, text_content = f"https://www.youtube.com/watch?v={video_id}", ""
            status_callback(f"  - Checking video: '{title[:60]}...'")
            try:
                transcript_list = YouTubeTranscriptApi.get_transcript(video_id)
                text_content = " ".join([d['text'] for d in transcript_list])
                status_callback(f"    ‚úÖ Found transcript.")
            except Exception as te:
                text_content = f"Title: {title}\nDescription: {description}"
                status_callback(f"    - No transcript available (Reason: {te}). Falling back to metadata.")
            videos.append({"source_type": "youtube", "title": title, "text": text_content, "url": url})
        if not videos:
            status_callback(f"üü° YouTube search for '{query}' was successful but returned no videos.")
        return True, videos
    except Exception as e:
        error_message = f"YouTube search for '{query}' failed. Reason: {e}"
        status_callback(f"‚ùå {error_message}")
        return False, error_message

# --- Synthesiser, Visualiser & Deep Researcher ---
def agent_synthesiser(context: str, themes: List[str], topic: str, sources: List[Dict[str, Any]], status_callback=print):
    status_callback("üìù Agent Synthesiser: Generating theme-driven outputs...")
    themes_for_note = "\n".join([f"{i+1}. {theme}" for i, theme in enumerate(themes)])
    note_prompt = f'You are a research analyst. Generate a "Discovery Note" in Markdown for the topic "{topic}".\nStructure your analysis EXPLICITLY around the following numbered key themes:\n{themes_for_note}\nIMPORTANT INSTRUCTIONS:\n- For each theme, synthesize insights from the provided sources.\n- When you cite a source, you MUST include a clickable Markdown link to its URL.\n- Conclude with a section for "Open Questions".\nContext:\n{context}'
    discovery_note = llm_completion(note_prompt, status_callback=status_callback)
    themes_for_map = ", ".join(f'"{t}"' for t in themes)
    mindmap_prompt = f"""You are a mind map generator. Your task is to create a hierarchical outline as a plain text indented list.
CRITICAL INSTRUCTIONS:
1.  The single root node MUST be the main research topic.
2.  The second-level nodes MUST be the provided key themes.
3.  The third-level nodes should be 3-5 key concepts derived from the context relevant to each theme.
4.  Do NOT include source titles or URLs. Keep all nodes concise.
MIND MAP DETAILS:
- **Main Topic:** "{topic}"
- **Key Themes:** [{themes_for_map}]
CONTEXT FOR SUB-TOPICS:
---
{context}
---
YOUR MIND MAP OUTLINE (indented list only):
"""
    mindmap_structure = llm_completion(mindmap_prompt, status_callback=status_callback)
    if sources:
        references_md = "\n\n---\n\n## Curated Sources\n\n"
        sources.sort(key=lambda s: (s.get('is_foundational', False), s.get('source_type', '')), reverse=True)
        for source in sources:
            cite_count = f" (Citations: {source.get('citations', 0)})" if source.get('source_type') == 'paper' else ''
            references_md += f"*   **[{source.get('source_type', 'N/A').upper()}] {source.get('title', 'No Title')}**{cite_count}\n    *   Link: <{source.get('url', '#')}>\n"
        discovery_note += references_md
    if mindmap_structure:
        mindmap_md = f"\n\n---\n\n## Mind Map Outline\n\n```\n{mindmap_structure}\n```\n"
        discovery_note += mindmap_md
    return discovery_note, mindmap_structure

def agent_visualiser(mindmap_structure: str, topic: str, output_folder: Path, status_callback=print):
    status_callback("üé® Agent Visualiser: Creating Mind Map image...")
    dot_path = GRAPHVIZ_DOT_EXECUTABLE
    if not dot_path or not shutil.which(dot_path):
        status_callback(f"‚ö†Ô∏è 'dot' executable not found in .env at '{dot_path}'. Searching system PATH...")
        dot_path = shutil.which("dot")
    if not dot_path:
        error_message = "‚ùå CRITICAL: Mind map generation failed. The 'dot' command from Graphviz could not be found."
        status_callback(error_message); print(error_message)
        return
    # ... (rest of the function is unchanged)
    def get_indent(line: str): return len(line) - len(line.lstrip(' \t'))
    def sanitize_node_name(name: str): return re.sub(r'^\d+[\.\)]\s*', '', name.strip().lstrip('*-‚Ä¢ ')).strip()
    lines = [line for line in mindmap_structure.strip().split('\n') if line.strip()]
    if not lines:
        status_callback("‚ö†Ô∏è Mind map structure was empty. Skipping image generation."); return
    try:
        dot = graphviz.Digraph(comment=topic, engine='dot', graph_attr={'rankdir': 'LR', 'splines': 'ortho', 'nodesep': '0.4'}, node_attr={'shape': 'box', 'style': 'rounded,filled', 'fontname': 'Helvetica'}, edge_attr={'arrowsize': '0.7'})
        parent_stack, indents = [], {get_indent(line) for line in lines}
        sorted_indents, indent_levels = sorted(list(indents)), {}
        for i, indent in enumerate(sorted_indents): indent_levels[indent] = i
        for i, line in enumerate(lines):
            node_name = sanitize_node_name(line)
            if not node_name: continue
            level = indent_levels.get(get_indent(line), 0)
            node_id = f"node_{i}"
            dot.node(node_id, label=node_name, fillcolor='skyblue' if level == 0 else ('lightgray' if level == 1 else 'white'))
            while len(parent_stack) > level: parent_stack.pop()
            if parent_stack: dot.edge(parent_stack[-1], node_id)
            parent_stack.append(node_id)
        dot_source_path = output_folder / "mindmap.dot"
        png_output_path = output_folder / "mindmap.png"
        dot.save(str(dot_source_path))
        command = [dot_path, "-Tpng", str(dot_source_path), "-o", str(png_output_path)]
        status_callback(f"  -> Executing command: {' '.join(command)}")
        result = subprocess.run(command, capture_output=True, text=True, check=False)
        if result.returncode == 0:
            status_callback(f"‚úÖ Mind Map saved to {png_output_path}")
        else:
            error_details = result.stderr or result.stdout or "No error output from Graphviz."
            status_callback(f"‚ùå Error rendering mind map with Graphviz: {error_details.strip()}")
        if dot_source_path.exists():
            os.remove(dot_source_path)
    except Exception as e:
        status_callback(f"‚ùå An unexpected Python exception occurred during mind map generation: {e}")

def agent_deep_researcher(topic: str, initial_synthesis_note: str, status_callback=print) -> str:
    """
    Takes the initial synthesis and performs a deeper, more comprehensive
    research pass to generate a final, detailed report.
    """
    status_callback("üßê Agent Deep Researcher: Initiating deep-dive analysis...")
    # ENHANCED PROMPT to fix citation issue
    prompt = f"""
You are a world-class research analyst producing a comprehensive report on the topic of **"{topic}"**.
You have been provided with an initial "Discovery Note" which contains a preliminary analysis and a list of sources. Your job is to expand this into a final, detailed report with proper citations.

**INITIAL CONTEXT (Discovery Note):**
---
{initial_synthesis_note}
---

**YOUR TASK (Follow these steps precisely):**

1.  **CREATE REFERENCE LIST:** First, at the very end of your response, create a markdown section titled `## Consolidated Reference List`. Review the "Curated Sources" section from the context above and create a **numbered list** of all the sources. For example:
    ```
    ## Consolidated Reference List
    1. [PAPER] Title of Paper A. URL: <http://...>
    2. [VIDEO] Title of Video B. URL: <http://...>
    ```

2.  **WRITE THE REPORT:** Next, generate the main report in Markdown. It must have sections for an Executive Summary, Introduction, a detailed Thematic Deep-Dive for each theme identified in the context, Practical Applications, Challenges, and a Future Outlook.

3.  **CITE YOUR SOURCES:** This is critical. As you write the report, you **MUST** provide inline citations for your claims by referencing the number from the "Consolidated Reference List" you created in step 1. Use the format `[1]`, `[2]`, etc. Every major claim or piece of data must have a citation. If you cannot find a source for a claim, do not make that claim.

**FINAL OUTPUT:**
Produce a single, cohesive Markdown document that contains the full report with inline citations, followed by the final numbered reference list.
"""
    status_callback("  -> Sending comprehensive prompt to LLM for deep synthesis...")
    final_report = llm_completion(prompt, status_callback=status_callback)
    status_callback("‚úÖ Deep Research Agent has completed the report.")
    return final_report


def build_context_from_sources(topic: str, all_sources: list) -> str:
    context = f"Topic: {topic}\n\n--- CONSOLIDATED SOURCES ---\n\n"
    for s in all_sources:
        context += f"Source Type: {s['source_type'].upper()}\nTitle: {s['title']}\nURL: {s.get('url', 'N/A')}\nContent: {s['text'][:1500].strip()}...\n\n---\n\n"
    return context

# --- Workflow Functions ---
# RESTORED from v3.1.0 to fix regression
def step1_fetch_foundational_sources(queries: Dict[str, Any], status_callback=print) -> dict:
    all_sources, failures = [], []
    if queries.get("highly_cited_query"):
        success, result = agent_paper_retriever(queries["highly_cited_query"], sort_by="citationCount", status_callback=status_callback)
        if success:
            all_sources.extend(result)
        else:
            failures.append(result)
    if queries.get("review_queries"):
        for query in queries["review_queries"]:
            success, result = agent_paper_retriever(query, sort_by="relevance", status_callback=status_callback)
            if success:
                all_sources.extend(result)
            else:
                failures.append(result)

    unique_sources = list({s['url']: s for s in all_sources if s.get('url')}.values())
    for source in unique_sources:
        source['is_foundational'] = True
    return {"sources": unique_sources, "failures": failures}

def step2_fetch_exploratory_sources(queries: Dict[str, List[str]], status_callback=print) -> dict:
    all_sources, failures = [], []
    if queries.get("scholar_queries"):
        for query in queries["scholar_queries"]:
            success, result = agent_paper_retriever(query, status_callback=status_callback)
            if success: all_sources.extend(result)
            else: failures.append(result)
    if queries.get("youtube_queries"):
        for query in queries["youtube_queries"]:
            success, result = agent_youtube_extractor(query, status_callback=status_callback)
            if success: all_sources.extend(result)
            else: failures.append(result)
    unique_sources = list({s['url']: s for s in all_sources if s.get('url')}.values())
    return {"sources": unique_sources, "failures": failures}

def step3_go_deeper(theme_query: str, status_callback=print):
    new_sources = []
    success_papers, papers_or_err = agent_paper_retriever(theme_query, status_callback=status_callback)
    if success_papers: new_sources.extend(papers_or_err)
    success_vids, vids_or_err = agent_youtube_extractor(theme_query, status_callback=status_callback)
    if success_vids: new_sources.extend(vids_or_err)
    return new_sources

def step4_run_synthesis(sources: List[Dict[str, Any]], themes: List[str], topic: str, status_callback=print):
    topic_folder_name = re.sub(r'[\\/*?:"<>|]', "", topic).replace(" ", "_")[:50]
    final_output_dir = OUTPUT_DIR / topic_folder_name
    final_output_dir.mkdir(exist_ok=True)
    if not sources or not themes:
        status_callback("‚ùå No sources or themes provided to synthesize. Aborting.")
        return None, None
    context = build_context_from_sources(topic, sources)
    discovery_note_md, mindmap_txt = agent_synthesiser(context, themes, topic, sources, status_callback=status_callback)
    if not discovery_note_md or not mindmap_txt:
        status_callback("‚ùå CRITICAL: Synthesis agent failed."); return None, None
    note_path = final_output_dir / "discovery_note.md"
    map_path = final_output_dir / "mindmap.png"
    with open(note_path, "w", encoding="utf-8") as f: f.write(discovery_note_md)
    status_callback(f"‚úÖ Discovery Note saved to {note_path}")
    agent_visualiser(mindmap_txt, topic, final_output_dir, status_callback=status_callback)
    if map_path.exists():
        return str(note_path), str(map_path)
    else:
        status_callback("‚ö†Ô∏è Synthesis finished, but the mind map image could not be created.")
        return str(note_path), None

def step5_run_deep_synthesis(topic: str, initial_synthesis_note: str, status_callback=print) -> str:
    """
    Orchestrates the final deep research step.
    """
    topic_folder_name = re.sub(r'[\\/*?:"<>|]', "", topic).replace(" ", "_")[:50]
    final_output_dir = OUTPUT_DIR / topic_folder_name
    final_output_dir.mkdir(exist_ok=True)

    if not initial_synthesis_note:
        status_callback("‚ùå Cannot run deep synthesis without an initial note. Aborting.")
        return None

    deep_report_md = agent_deep_researcher(topic, initial_synthesis_note, status_callback=status_callback)
    if not deep_report_md:
        status_callback("‚ùå CRITICAL: Deep research agent failed to produce a report.")
        return None

    report_path = final_output_dir / f"deep_research_report_{topic_folder_name}.md"
    with open(report_path, "w", encoding="utf-8") as f:
        f.write(deep_report_md)

    status_callback(f"‚úÖ Deep Research Report saved to {report_path}")
    return str(report_path)


def save_outputs_to_custom_dir(source_note_path: str, source_map_path: str, custom_dest_dir_str: str, status_callback=print):
    try:
        if not custom_dest_dir_str:
            status_callback("‚ùå Error: Destination directory cannot be empty.")
            return False, "Destination directory was not provided."
        dest_path = Path(_convert_windows_to_wsl_path(custom_dest_dir_str))
        dest_path.mkdir(parents=True, exist_ok=True)
        status_callback(f"  -> Ensured destination directory exists: {dest_path}")
        if source_note_path and Path(source_note_path).exists():
            shutil.copy(source_note_path, dest_path)
            status_callback(f"  -> ‚úÖ Copied discovery note to {dest_path}")
        else:
             status_callback(f"  -> ‚ö†Ô∏è Could not find source discovery note at {source_note_path} to copy.")
        if source_map_path and Path(source_map_path).exists():
            shutil.copy(source_map_path, dest_path)
            status_callback(f"  -> ‚úÖ Copied mind map to {dest_path}")
        else:
            status_callback(f"  -> ‚ö†Ô∏è Could not find source mind map at {source_map_path} to copy.")
        return True, f"Successfully saved a copy to {dest_path}"
    except Exception as e:
        error_msg = f"‚ùå Failed to save to custom directory: {e}"
        status_callback(error_msg)
        return False, error_msg


--- FILE: ./cortex_engine/task_engine.py ---
# ## File: cortex_engine/task_engine.py
# Version: 12.0.1 (Refinement Fix)
# Date: 2025-07-15
# Purpose: Core AI task execution engine.
#          - CRITICAL FIX (v12.0.1): Restored the 'refine_with_ai' method and its
#            associated prompt templates, which were erroneously removed during
#            previous refactoring. This resolves the AttributeError and completes
#            the restoration of the co-pilot's core functionality.

import docx
import io
import os
from typing import Dict, List, Any, Optional

from docx.shared import RGBColor
from llama_index.core import VectorStoreIndex, Settings
from .instruction_parser import CortexInstruction, INSTRUCTION_REGEX
from .collection_manager import WorkingCollectionManager

# --- PROMPT PERSONAS ---
PROMPTS = {
    "green": "You are a factual, concise AI assistant. Your tone should be professional, direct, and formal.",
    "orange": "You are an expert proposal writer. Your tone should be confident, persuasive, and benefits-oriented.",
    "red": "You are a world-class strategy consultant. Your tone should be authoritative, visionary, and inspiring."
}

# --- PROMPT TEMPLATES (RESTORED) ---
REFINE_PROMPT = """
{persona}
Your task is to refine and enhance the following raw text for a proposal section, using the provided context to add relevant details.
Do not invent facts. If the context is irrelevant, simply improve the existing text's clarity and style based on the guidance.
PROPOSAL SECTION HEADING: {section_heading}
SPECIFIC GUIDANCE: {sub_instruction}
RAW TEXT FROM USER:
---
{raw_text}
---
BACKGROUND CONTEXT FROM KNOWLEDGE BASE:
---
{context}
---
REFINED AND ENHANCED TEXT:
"""

GENERATE_KB_PROMPT = """
{persona}
Your task is to draft a compelling paragraph for the proposal section "{section_heading}", based ONLY on the provided context.
Use the specific guidance to focus your response. Synthesize information from multiple sources if available.
Do not use any information not present in the context.
SPECIFIC GUIDANCE: {sub_instruction}
BACKGROUND CONTEXT FROM KNOWLEDGE BASE:
---
{context}
---
DRAFT FOR "{section_heading}":
"""

GENERATE_RESOURCES_PROMPT = """
{persona}
Your task is to suggest a team of resources for a project. Use the full proposal draft so far to understand the project's scope and requirements. Use the additional background context from the knowledge base to inform your suggestions about specific technologies or methodologies mentioned.
PROPOSAL SECTION HEADING: {section_heading}
SPECIFIC GUIDANCE: {sub_instruction}
FULL PROPOSAL DRAFT SO FAR:
---
{proposal_context}
---
ADDITIONAL BACKGROUND CONTEXT FROM KNOWLEDGE BASE:
---
{kb_context}
---
Based on all the information above, please suggest the required team roles and their key responsibilities.
SUGGESTED RESOURCES:
"""

# --- Color mapping ---
COLOR_MAP = {
    "green": RGBColor(0x00, 0x80, 0x00),
    "orange": RGBColor(0xFF, 0x8C, 0x00),
    "red": RGBColor(0xB2, 0x22, 0x22),
}

class TaskExecutionEngine:
    def __init__(self, main_index: VectorStoreIndex, collection_manager: WorkingCollectionManager):
        self.main_index = main_index
        self.collection_manager = collection_manager

    def _get_retriever(self, similarity_top_k=5, doc_id_filter: Optional[List[str]] = None):
        if doc_id_filter:
            from llama_index.core.vector_stores import ExactMatchFilter, MetadataFilters
            filters = MetadataFilters(filters=[ExactMatchFilter(key="doc_id", value=doc_id) for doc_id in doc_id_filter])
            return self.main_index.as_retriever(
                similarity_top_k=similarity_top_k,
                filters=filters
            )
        return self.main_index.as_retriever(similarity_top_k=similarity_top_k)

    def _resolve_knowledge_sources(self, knowledge_sources: List[str]) -> Optional[List[str]]:
        if not knowledge_sources or "Main Cortex Knowledge Base" in knowledge_sources:
            return None
        all_doc_ids = set()
        for source_name in knowledge_sources:
            all_doc_ids.update(self.collection_manager.get_doc_ids_by_name(source_name))
        return list(all_doc_ids) if all_doc_ids else None

    # --- refine_with_ai METHOD (RESTORED) ---
    def refine_with_ai(self, section_heading: str, raw_text: str, creativity: str, sub_instruction: Optional[str]) -> str:
        if not raw_text.strip(): return ""
        search_query = f"Context for proposal section '{section_heading}'. User input: '{raw_text}'"
        retriever = self._get_retriever()
        retrieved_nodes = retriever.retrieve(search_query)
        context_str = "\n\n---\n\n".join([node.get_content() for node in retrieved_nodes])
        final_prompt = REFINE_PROMPT.format(
            persona=PROMPTS.get(creativity, PROMPTS['green']),
            section_heading=section_heading,
            sub_instruction=sub_instruction or "N/A",
            raw_text=raw_text,
            context=context_str or "No context found."
        )
        response = Settings.llm.complete(final_prompt)
        return response.text.strip()

    def generate_from_kb(self, instruction: CortexInstruction, creativity: str, knowledge_sources: List[str], session_state: dict) -> str:
        doc_id_filter = self._resolve_knowledge_sources(knowledge_sources)
        retriever = self._get_retriever(doc_id_filter=doc_id_filter)

        proposal_context = []
        parsed_instructions = session_state.get('parsed_instructions', [])
        section_content = session_state.get('section_content', {})
        for i, inst in enumerate(parsed_instructions):
            content_key = f"content_inst_{i}"
            content = section_content.get(content_key, {}).get('text', '')
            if content:
                proposal_context.append(f"Content from section '{inst.section_heading}':\n{content}\n")

        full_proposal_context = "\n---\n".join(proposal_context)

        search_query = f"FULL PROPOSAL CONTEXT SO FAR:\n{full_proposal_context}\n\nGenerate suggestions for proposal section '{instruction.section_heading}'."
        retrieved_nodes = retriever.retrieve(search_query)
        kb_context_str = "\n\n---\n\n".join([node.get_content() for node in retrieved_nodes]) or "No relevant information found."

        prompt_template = GENERATE_RESOURCES_PROMPT if instruction.task_type == "GENERATE_RESOURCES" else GENERATE_KB_PROMPT

        if instruction.task_type == "GENERATE_RESOURCES":
            final_prompt = prompt_template.format(
                persona=PROMPTS.get(creativity, PROMPTS['green']),
                section_heading=instruction.section_heading,
                sub_instruction=instruction.sub_instruction or "N/A",
                proposal_context=full_proposal_context or "No prior proposal content available.",
                kb_context=kb_context_str
            )
        else: # Covers all other GENERATE tasks
            final_prompt = prompt_template.format(
                persona=PROMPTS.get(creativity, PROMPTS['green']),
                section_heading=instruction.section_heading,
                sub_instruction=instruction.sub_instruction or "N/A",
                context=kb_context_str
            )

        response = Settings.llm.complete(final_prompt)
        return response.text.strip()

    def retrieve_from_kb(self, instruction: CortexInstruction, knowledge_sources: List[str], session_state: dict) -> str:
        doc_id_filter = self._resolve_knowledge_sources(knowledge_sources)
        retriever = self._get_retriever(similarity_top_k=5, doc_id_filter=doc_id_filter)

        search_query = f"Find the most relevant case studies or project examples for the proposal section '{instruction.section_heading}'. Focus on project outcomes, challenges, and solutions."
        retrieved_nodes = retriever.retrieve(search_query)

        if not retrieved_nodes: return "[No relevant case studies found.]"

        content = [f"**Source:** {os.path.basename(node.metadata.get('doc_posix_path', 'N/A'))}\n**Summary:** {node.metadata.get('summary', 'N/A')}\n\n**Excerpt:**\n{node.get_content().strip()}" for node in retrieved_nodes if node.score > 0.7]

        return "\n\n---\n\n".join(content) if content else "[No relevant case studies found with sufficient confidence.]"

    def assemble_document(self, instructions: List[CortexInstruction], section_content: dict, doc_template_bytes: bytes) -> io.BytesIO:
        doc = docx.Document(io.BytesIO(doc_template_bytes))

        # Find all paragraphs that contain an instruction tag
        instruction_paragraphs = []
        for p in doc.paragraphs:
            if INSTRUCTION_REGEX.search(p.text.strip()):
                instruction_paragraphs.append(p)

        for i, p in enumerate(instruction_paragraphs):
            # Ensure we don't go out of bounds if the number of instructions and paragraphs mismatches
            if i >= len(instructions):
                break

            content_key = f"content_inst_{i}"
            content_data = section_content.get(content_key, {'text': '', 'creativity': 'green'})
            generated_text = content_data.get('text', '')
            creativity_level = content_data.get('creativity')

            # Clear the placeholder text (e.g., "[PROMPT_HUMAN]") from the paragraph
            p.text = ""

            if generated_text:
                run = p.add_run()
                font_color = COLOR_MAP.get(creativity_level)
                if font_color:
                    run.font.color.rgb = font_color

                # Add the generated text, preserving line breaks
                lines = generated_text.split('\n')
                for j, line in enumerate(lines):
                    run.add_text(line)
                    if j < len(lines) - 1:
                        run.add_break()

        final_bio = io.BytesIO()
        doc.save(final_bio)
        final_bio.seek(0)
        return final_bio


--- FILE: ./cortex_engine/utils.py ---
# ## File: cortex_engine/utils.py
# Version: 1.0.0 (Initial Creation)
# Date: 2025-07-13
# Purpose: A module for common, low-dependency utility functions shared across the engine.
#          - FEATURE (v1.0.0): Created to house the `get_file_hash` function,
#            breaking a circular dependency between `ingest_cortex` and `collection_manager`.

import hashlib

def get_file_hash(filepath: str) -> str:
    """
    Generates a SHA256 hash for a given file.

    Args:
        filepath: The path to the file.

    Returns:
        The hex digest of the file's SHA256 hash.
    """
    sha256_hash = hashlib.sha256()
    with open(filepath, "rb") as f:
        # Read and update hash in chunks of 4K
        for byte_block in iter(lambda: f.read(4096), b""):
            sha256_hash.update(byte_block)
    return sha256_hash.hexdigest()


--- FILE: ./external_research/graphRAG_and_use_of_graph_networks_to_create_embed/deep_research_report_graphRAG_and_use_of_graph_networks_to_create_embed.md ---
# GraphRAG: Enhancing Retrieval Augmented Generation with Graph Networks

## Executive Summary

Retrieval Augmented Generation (RAG) significantly improves Large Language Models (LLMs) by incorporating external knowledge retrieval.  GraphRAG takes this a step further by leveraging knowledge graphs and graph neural networks (GNNs) to represent and reason about relationships within the knowledge base. This report explores the core components of graphRAG‚Äîknowledge graph embedding, RAG methodologies, and GNNs‚Äîhighlighting their synergistic potential for improved retrieval accuracy, explainability, and handling of complex relationships. While offering significant advantages, challenges remain in scalability, explainability, and handling data imperfections. Future research should focus on addressing these limitations to unlock graphRAG's full potential across diverse domains.


## Introduction

Retrieval Augmented Generation (RAG) enhances Large Language Models (LLMs) by integrating external knowledge retrieval into their generation process [7, 14].  This improves accuracy, especially when dealing with factual information and reduces hallucinations.  GraphRAG extends RAG by using knowledge graphs and GNNs to represent and query relationships within the knowledge base, surpassing the capabilities of traditional keyword-based or vector-based retrieval.  This report delves into the core components of graphRAG, analyzing their strengths, limitations, and future directions.


## Thematic Deep-Dive

### 1. Knowledge Graph Embedding (KGE)

KGE methods represent entities and their relationships as low-dimensional vectors, facilitating efficient semantic similarity calculations and inference [1, 8, 12]. This allows for the capture of relational information crucial for accurate retrieval, as demonstrated in patent retrieval using TransE and Sentence-BERT embeddings [1].  The integration of GNNs further enhances KGE by enabling the learning of multi-level representations and the handling of long-tail entities through the incorporation of attribute information [8, 12, 13]. This is particularly important in applications like recommender systems where leveraging rich knowledge graph information significantly boosts performance [9].  Hypergraph neural networks offer an effective solution for embedding entities with limited structural information [8].


### 2. Retrieval Augmented Generation (RAG)

RAG systems enhance LLMs by retrieving relevant information from an external knowledge base before generating a response [7, 14].  This approach has shown success in various domains, including healthcare, where it improves the accuracy of LLM responses to complex medical questions [7, 14].  The use of graphRAG in answering medical exam questions, as exemplified by an evidence-based GraphRAG pipeline using LLMs tailored for answering USMLE questions, highlights improved interpretability and traceability by explicitly using a knowledge graph and a vector store [14].   Furthermore, advancements in embedding models, such as APEX-Embedding-7B, utilize entity relationship maps and contrastive sampling to enhance the accuracy of document retrieval for RAG tasks [15].


### 3. Graph Neural Networks (GNNs)

GNNs are adept at learning representations from graph-structured data [10, 11, 12, 13]. Their integration with KGE significantly boosts embedding quality and link prediction capabilities [12, 13].  GNNs can capture higher-order relationships and multi-level information, offering richer semantic representations than simpler embedding methods [12, 13].  Beyond knowledge graph applications, GNNs are used effectively in diverse tasks, such as image retrieval by modeling local semantic correlations [11] and dynamic heterogeneous graph modeling for tasks like citation prediction [10].


## Practical Applications

GraphRAG finds practical applications in various sectors:

* **Healthcare:** Answering complex medical questions accurately and interpretably [7, 14].
* **Patent Retrieval:** Improving the efficiency and accuracy of patent searching by capturing relationships between patents, inventors, and citations [1].
* **Recommender Systems:** Delivering personalized recommendations by incorporating rich knowledge graph information [9].
* **Question Answering:** Providing more accurate and explainable answers to complex questions by leveraging the relationship information stored in a knowledge graph [14].
* **Scientific Discovery:** Facilitating knowledge discovery by analyzing and relating scientific papers and concepts through their underlying relationships.


## Challenges

Despite its potential, graphRAG faces several challenges:

* **Scalability:** Handling extremely large knowledge graphs and high-volume queries efficiently remains a significant hurdle [Discovery Note, Open Questions].
* **Explainability and Interpretability:** Generating easily understandable explanations for the generated outputs requires further research, especially critical in high-stakes applications [Discovery Note, Open Questions].
* **Knowledge Graph Quality:** The accuracy of graphRAG heavily relies on the quality, completeness, and lack of bias in the underlying knowledge graph [Discovery Note, Open Questions].
* **Efficient Inference:**  Developing efficient inference methods for complex graph-based retrieval and reasoning is crucial for real-time applications [Discovery Note, Open Questions].
* **Handling Uncertainty:** Incorporating mechanisms to manage uncertainty in the knowledge graph or retrieved information is essential for creating robust systems [Discovery Note, Open Questions].
* **Comparison with Traditional RAG:** A comprehensive comparison of graphRAG and traditional RAG is needed to definitively establish the advantages of this approach across diverse domains and benchmark tasks [Discovery Note, Open Questions].


## Future Outlook

Future research should focus on:

* **Developing scalable graphRAG architectures:** Explore distributed computing and approximate inference techniques to handle massive knowledge graphs.
* **Improving explainability:** Develop methods that provide transparent and easily understandable explanations for generated outputs.
* **Addressing knowledge graph quality issues:** Develop techniques to deal with incomplete, noisy, and biased knowledge graphs.
* **Exploring novel GNN architectures:** Investigate advanced GNN architectures to improve relationship representation and reasoning.
* **Developing efficient inference methods:** Research techniques to speed up inference for large graphs and reduce computational costs.
* **Incorporating uncertainty quantification:** Develop mechanisms to quantify and manage uncertainty in the knowledge graph and retrieved information.
* **Comparative analysis:** Conduct thorough comparative analyses of graphRAG and traditional RAG approaches across different domains.


## Consolidated Reference List

1. [PAPER] Enhancing patent retrieval using text and knowledge graph embeddings: a technical note. URL: <https://www.semanticscholar.org/paper/de140ee4c800066fd980f09443deafe152e21549>
2. [YOUTUBE] Graph RAG: Improving RAG with Knowledge Graphs. URL: <https://www.youtube.com/watch?v=vX3A96_F3FU>
3. [YOUTUBE] What is a Knowledge Graph?. URL: <https://www.youtube.com/watch?v=y7sXDpffzQQ>
4. [YOUTUBE] RECON: Relation Extraction using Knowledge Graph Context in a Graph Neural Network. URL: <https://www.youtube.com/watch?v=ZKwYitIkSOE>
5. [YOUTUBE] Graph Neural Networks - a perspective from the ground up. URL: <https://www.youtube.com/watch?v=GXhBEj1ZtE8>
6. [YOUTUBE] GraphRAG vs. Traditional RAG: Higher Accuracy & Insight with LLM. URL: <https://www.youtube.com/watch?v=Aw7iQjKAX2k>
7. [PAPER] Development and Testing of Retrieval Augmented Generation in Large Language Models - A Case Study Report. URL: <https://www.semanticscholar.org/paper/7423e5c903fb2befaf471cae64e2530f7c1d0404>
8. [PAPER] Knowledge graph embedding with entity attributes using hypergraph neural networks. URL: <https://www.semanticscholar.org/paper/fef404dae79eadcc07e7422a3674880e2b1b6bb5>
9. [PAPER] Enhancing Recommender Systems Performance using Knowledge Graph Embedding with Graph Neural Networks. URL: <https://www.semanticscholar.org/paper/c9ea9ea3220c28c73a20fbd47b0e99adce822911>
10. [PAPER] Modeling Dynamic Heterogeneous Graph and Node Importance for Future Citation Prediction. URL: <https://www.semanticscholar.org/paper/5c425d910cdf4246061ef6c685b2012f7b96c193>
11. [PAPER] Local Semantic Correlation Modeling Over Graph Neural Networks for Deep Feature Embedding and Image Retrieval. URL: <https://www.semanticscholar.org/paper/f374c34fa95d7e09f0b1e4aa9ad94e56a7a3c213>
12. [PAPER] Knowledge Graph: A Survey. URL: <https://www.semanticscholar.org/paper/453d77c447561be30ecc2ec1a52549ba12e9c3db>
13. [PAPER] Learning multilevel representations for knowledge graph embedding using graph neural networks. URL: <https://www.semanticscholar.org/paper/2f4285846db2dbbcd736737f5789dd70239bd8c5>
14. [PAPER] Investigations on using Evidence-Based GraphRag Pipeline using LLM Tailored for Answering USMLE Medical Exam Questions. URL: <https://www.semanticscholar.org/paper/573721af3692834841c3e82303a3bfabb98423b9>
15. [PAPER] Improving Embedding Accuracy for Document Retrieval Using Entity Relationship Maps and Model-Aware Contrastive Sampling. URL: <https://www.semanticscholar.org/paper/6726ac1e514ae64142d88a9809cfe3dd37e48771>




--- FILE: ./external_research/graphRAG_and_use_of_graph_networks_to_create_embed/discovery_note.md ---
# Discovery Note: graphRAG and the Use of Graph Networks for Embedding Models

This note explores the intersection of knowledge graph embedding, Retrieval Augmented Generation (RAG), and graph neural networks (GNNs) in the context of graphRAG, focusing on its ability to store and retrieve relationships.  The analysis is structured around three key themes:

## 1. Knowledge Graph Embedding

Knowledge graph embedding (KGE) techniques aim to represent entities and their relationships in a low-dimensional vector space, allowing for efficient semantic similarity calculations and inference.  Several sources highlight the importance of KGE in various applications.

* **Patent Retrieval:** The paper ["Enhancing patent retrieval using text and knowledge graph embeddings: a technical note"](https://www.semanticscholar.org/paper/de140ee4c800066fd980f09443deafe152e21549) demonstrates the use of KGE (specifically TransE) to embed patent citation and inventor information, complementing text embeddings (Sentence-BERT) for improved patent retrieval.  This shows how KGE can capture relational information crucial for accurate retrieval.
* **Recommender Systems:** The paper ["Enhancing Recommender Systems Performance using Knowledge Graph Embedding with Graph Neural Networks"](https://www.semanticscholar.org/paper/c9ea9ea3220c28c73a20fbd47b0e99adce822911) highlights the use of KGE integrated with GNNs to enhance recommender systems by leveraging rich knowledge graph information. This underscores the power of combining KGE with GNNs for improved performance in applications beyond simple link prediction.
* **Addressing Long-Tail Entities:** The study ["Knowledge graph embedding with entity attributes using hypergraph neural networks"](https://www.semanticscholar.org/paper/fef404dae79eadcc07e7422a3674880e2b1b6bb5) addresses the challenge of embedding long-tail entities (those with limited structural information) by incorporating attribute information using hypergraph neural networks. This demonstrates the need for robust KGE techniques that handle diverse entity characteristics.
* **Multilevel Representations:**  The paper ["Learning multilevel representations for knowledge graph embedding using graph neural networks"](https://www.semanticscholar.org/paper/2f4285846db2dbbcd736737f5789dd70239bd8c5) focuses on learning multi-level representations of entities and relationships within a knowledge graph using graph neural networks.  This suggests a pathway for capturing richer semantic relationships than simpler embedding methods allow.

Overall, KGE, especially when enhanced with GNNs, provides a robust method to encode complex relationships for improved retrieval and inference within RAG systems.


## 2. Retrieval Augmented Generation (RAG)

RAG enhances Large Language Models (LLMs) by augmenting their generation capabilities with external knowledge retrieval.  Several studies highlight the effectiveness of RAG in various domains.

* **Healthcare:** The paper ["Development and Testing of Retrieval Augmented Generation in Large Language Models - A Case Study Report"](https://www.semanticscholar.org/paper/7423e5c903fb2befaf471cae64e2530f7c1d0404) demonstrates the application of RAG in a healthcare setting, improving the accuracy of LLM responses to preoperative medicine questions. This showcases RAG's practical use in high-stakes domains.
* **Medical Exam Questions:**  The paper ["Investigations on using Evidence-Based GraphRag Pipeline using LLM Tailored for Answering USMLE Medical Exam Questions"](https://www.semanticscholar.org/paper/573721af3692834841c3e82303a3bfabb98423b9) specifically uses a GraphRAG framework (combining a knowledge graph with a vector store) for answering USMLE medical exam questions, emphasizing improved interpretability and traceability.  This directly addresses the core concept of graphRAG.
* **Improved Document Retrieval:** The paper ["Improving Embedding Accuracy for Document Retrieval Using Entity Relationship Maps and Model-Aware Contrastive Sampling"](https://www.semanticscholar.org/paper/6726ac1e514ae64142d88a9809cfe3dd37e48771) presents APEX-Embedding-7B, demonstrating improved accuracy in document retrieval for RAG using entity relationship maps and contrastive sampling. This highlights advances in embedding models tailored for RAG tasks.

These studies highlight RAG's ability to improve LLM accuracy, interpretability and efficiency, particularly when combined with graph-based methods.


## 3. Graph Neural Networks (GNNs)

GNNs are a powerful tool for learning representations from graph-structured data. Their application in conjunction with KGE significantly enhances the capabilities of embedding models.

* **Knowledge Graph Embedding:** Multiple sources show the integration of GNNs with KGE for improved link prediction and embedding quality.  The papers ["Knowledge graph embedding with entity attributes using hypergraph neural networks"](https://www.semanticscholar.org/paper/fef404dae79eadcc07e7422a3674880e2b1b6bb5) and ["Learning multilevel representations for knowledge graph embedding using graph neural networks"](https://www.semanticscholar.org/paper/2f4285846db2dbbcd736737f5789dd70239bd8c5) exemplify this, demonstrating that GNNs can capture higher-order relationships and multilevel information which is otherwise lost in simpler methods.
* **Image Retrieval:**  The paper ["Local Semantic Correlation Modeling Over Graph Neural Networks for Deep Feature Embedding and Image Retrieval"](https://www.semanticscholar.org/paper/f374c34fa95d7e09f0b1e4aa9ad94e56a7a3c213) illustrates the effectiveness of GNNs in image retrieval by modeling local correlation structures in the feature space.  While not directly related to RAG, it shows the broader applicability of GNNs for embedding tasks.
* **Dynamic Heterogeneous Graphs:** The paper ["Modeling Dynamic Heterogeneous Graph and Node Importance for Future Citation Prediction"](https://www.semanticscholar.org/paper/5c425d910cdf4246061ef6c685b2012f7b96c193) leverages GNNs to model dynamic heterogeneous academic networks for citation prediction, showcasing its ability to handle complex, evolving graph structures.  This demonstrates the robustness of GNNs in dealing with real-world data complexities.


The combination of GNNs with KGE provides a powerful approach for creating rich, nuanced embeddings that can accurately represent and reason about relationships within the knowledge graph, forming the foundation of effective graphRAG systems.


## Open Questions

1. **Scalability of GraphRAG:** How can graphRAG systems be scaled to handle extremely large knowledge graphs and high-volume query requests?  Current implementations may face challenges in processing and storing massive datasets.
2. **Explainability and Interpretability:** While some graphRAG approaches focus on improved interpretability, further research is needed to develop techniques that offer more transparent and easily understandable explanations for generated outputs, especially crucial in high-stakes applications.
3. **Knowledge Graph Quality:** The performance of graphRAG is highly dependent on the quality and completeness of the underlying knowledge graph. How can we address challenges related to incomplete, noisy, or biased knowledge graphs?
4. **Comparison to Traditional RAG:** A more comprehensive comparative analysis of graphRAG and traditional RAG systems across various domains and benchmarks is needed to better understand the benefits and limitations of each approach.
5. **Efficient Inference:**  Developing efficient inference methods for complex graph-based retrieval and reasoning is essential for real-time applications. Current methods may be computationally expensive for large graphs.
6. **Handling Uncertainty:** How can graphRAG models handle uncertainty in the knowledge graph or in the retrieved information?  Incorporating mechanisms for quantifying and managing uncertainty is crucial for building robust and trustworthy systems.

Addressing these open questions will significantly advance the field of graphRAG and unlock its full potential across diverse application domains.


---

## Curated Sources

*   **[PAPER] Enhancing patent retrieval using text and knowledge graph embeddings: a technical note** (Citations: 16)
    *   Link: <https://www.semanticscholar.org/paper/de140ee4c800066fd980f09443deafe152e21549>
*   **[YOUTUBE] Graph RAG: Improving RAG with Knowledge Graphs**
    *   Link: <https://www.youtube.com/watch?v=vX3A96_F3FU>
*   **[YOUTUBE] What is a Knowledge Graph?**
    *   Link: <https://www.youtube.com/watch?v=y7sXDpffzQQ>
*   **[YOUTUBE] RECON:  Relation Extraction using Knowledge Graph Context in a Graph Neural Network**
    *   Link: <https://www.youtube.com/watch?v=ZKwYitIkSOE>
*   **[YOUTUBE] Graph Neural Networks - a perspective from the ground up**
    *   Link: <https://www.youtube.com/watch?v=GXhBEj1ZtE8>
*   **[YOUTUBE] GraphRAG vs. Traditional RAG: Higher Accuracy &amp; Insight with LLM**
    *   Link: <https://www.youtube.com/watch?v=Aw7iQjKAX2k>
*   **[PAPER] Development and Testing of Retrieval Augmented Generation in Large Language Models - A Case Study Report** (Citations: 19)
    *   Link: <https://www.semanticscholar.org/paper/7423e5c903fb2befaf471cae64e2530f7c1d0404>
*   **[PAPER] Knowledge graph embedding with entity attributes using hypergraph neural networks** (Citations: 3)
    *   Link: <https://www.semanticscholar.org/paper/fef404dae79eadcc07e7422a3674880e2b1b6bb5>
*   **[PAPER] Modeling Dynamic Heterogeneous Graph and Node Importance for Future Citation Prediction** (Citations: 11)
    *   Link: <https://www.semanticscholar.org/paper/5c425d910cdf4246061ef6c685b2012f7b96c193>
*   **[PAPER] Enhancing Recommender Systems Performance using Knowledge Graph Embedding with Graph Neural Networks** (Citations: 0)
    *   Link: <https://www.semanticscholar.org/paper/c9ea9ea3220c28c73a20fbd47b0e99adce822911>
*   **[PAPER] Local Semantic Correlation Modeling Over Graph Neural Networks for Deep Feature Embedding and Image Retrieval** (Citations: 17)
    *   Link: <https://www.semanticscholar.org/paper/f374c34fa95d7e09f0b1e4aa9ad94e56a7a3c213>
*   **[PAPER] Knowledge Graph: A Survey** (Citations: 3)
    *   Link: <https://www.semanticscholar.org/paper/453d77c447561be30ecc2ec1a52549ba12e9c3db>
*   **[PAPER] Learning multilevel representations for knowledge graph embedding using graph neural networks** (Citations: 0)
    *   Link: <https://www.semanticscholar.org/paper/2f4285846db2dbbcd736737f5789dd70239bd8c5>
*   **[PAPER] Investigations on using Evidence-Based GraphRag Pipeline using LLM Tailored for Answering USMLE Medical Exam Questions** (Citations: 0)
    *   Link: <https://www.semanticscholar.org/paper/573721af3692834841c3e82303a3bfabb98423b9>
*   **[PAPER] Improving Embedding Accuracy for Document Retrieval Using Entity Relationship Maps and Model-Aware Contrastive Sampling** (Citations: 0)
    *   Link: <https://www.semanticscholar.org/paper/6726ac1e514ae64142d88a9809cfe3dd37e48771>


---

## Mind Map Outline

```
graphRAG and use of graph networks to create embedding models with ability to store and retrieve relationships
    Knowledge Graph Embedding
        Entity Representation
        Relationship Modeling
        Knowledge Graph Construction
        Link Prediction
        Scalability and Efficiency
    Retrieval Augmented Generation (RAG)
        Contextual Retrieval
        Document Chunking
        Embedding Methods
        Vector Databases
        Answer Synthesis
    Graph Neural Networks
        Graph Convolutional Networks (GCNs)
        Graph Attention Networks (GATs)
        Hypergraph Neural Networks
        Node Embeddings
        Relationship Embeddings


```



--- FILE: ./pages/1_AI_Assisted_Research.py ---
# ## File: pages/1_AI_Assisted_Research.py
# Version: 4.0.0 (UI Bug Fixes)
# Date: 2025-07-22
# Purpose: A Streamlit UI for the multi-agent research and synthesis engine.
#          - FIX (v4.0.0): The header for each step is now managed by a central
#            dictionary lookup, resolving the cosmetic bug where the final
#            step incorrectly displayed the title of the first step.
#          - FIX (v3.5.0): Resolved a TypeError by removing an
#            unexpected keyword argument ('topic') from the call to
#            `step1_fetch_foundational_sources`, aligning the UI with the
#            backend function's signature.

import streamlit as st
import sys
from pathlib import Path

# Add the project root to the Python path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from cortex_engine.synthesise import (
    agent_foundational_query_crafter,
    agent_exploratory_query_crafter,
    step1_fetch_foundational_sources,
    step2_fetch_exploratory_sources,
    agent_thematic_analyser,
    step3_go_deeper,
    step4_run_synthesis,
    step5_run_deep_synthesis,
    build_context_from_sources,
    save_outputs_to_custom_dir
)

st.set_page_config(page_title="Cortex AI Research Assistant", layout="wide")

# --- Initialize Session State ---
def initialize_session_state():
    """
    Initializes session state keys ONLY if they are not already present.
    This is critical for preserving state across reruns.
    """
    defaults = {
        "research_step": "start",
        "research_topic": "",
        "research_log": [],
        "foundational_queries": {},
        "foundational_sources": [],
        "curated_foundational_sources": [],
        "exploratory_queries": {},
        "exploratory_sources": [],
        "curated_exploratory_sources": [],
        "research_themes": [],
        "research_final_results": None, # Will hold (note_path, map_path)
        "deep_research_result": None, # Will hold path to the deep research note
        "research_output_path": "",
        "fetch_failures": []
    }
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value

initialize_session_state()


# --- UI Layout & Callbacks ---
st.title("ü§ñ 1. AI Assisted Research")
st.caption("Using a **Double Diamond** approach to find, analyze, and synthesize external knowledge.")

# FIX: Centralized header management to fix UI step numbering bug
STEP_HEADERS = {
    "start": "Step 1: Define Research Topic",
    "foundational_curation": "Step 2: Curate Foundational Sources",
    "exploratory_queries": "Step 3: Refine Exploratory Queries",
    "exploratory_curation": "Step 4: Curate Exploratory Sources",
    "theme_curation": "Step 5: Refine Themes & Synthesize Initial Report",
    "synthesis_complete": "Step 6: Initial Synthesis Complete",
    "deep_research_complete": "‚úÖ Research Complete: Final Report Generated"
}
current_step = st.session_state.research_step
st.header(STEP_HEADERS.get(current_step, "AI Research"))


log_expander = st.expander("Show Live Log", expanded=True)

def execute_and_log(step_function, *args, **kwargs):
    """
    Correctly wraps a backend function call to inject a logging callback
    and display live updates in the UI.
    """
    log_placeholder = log_expander.empty()
    st.session_state.research_log.append(f"--- Running: {step_function.__name__} ---")

    def ui_log_callback(message):
        st.session_state.research_log.append(message)
        log_placeholder.code("\n".join(st.session_state.research_log), language="log")

    kwargs['status_callback'] = ui_log_callback
    result = step_function(*args, **kwargs)

    with log_expander:
        st.code("\n".join(st.session_state.research_log), language="log")
    return result

# --- Reusable UI Components ---
def display_source_list(sources, selection_key, title):
    st.subheader(title)
    st.markdown(f"Found {len(sources)} sources. Select the most relevant ones to include in the final synthesis.")

    if not sources and not st.session_state.fetch_failures:
        st.info("No sources were found in this step.")
        return

    if st.session_state.fetch_failures:
        with st.expander("‚ö†Ô∏è View Search Failures", expanded=False):
            for failure in st.session_state.fetch_failures:
                st.error(failure)

    if not sources:
        return

    if selection_key not in st.session_state or not st.session_state[selection_key]:
        st.session_state[selection_key] = sources.copy()

    with st.container(border=True):
        col1, col2 = st.columns(2)
        if col1.button(f"Select All ({len(sources)})", use_container_width=True, key=f"sel_all_{selection_key}"):
            st.session_state[selection_key] = sources.copy(); st.rerun()
        if col2.button("Deselect All", use_container_width=True, key=f"desel_all_{selection_key}"):
            st.session_state[selection_key] = []; st.rerun()
    st.divider()

    selected_urls = {src['url'] for src in st.session_state[selection_key]}

    def toggle_source_selection(source_obj):
        current_selection = st.session_state.get(selection_key, [])
        if source_obj['url'] in {s['url'] for s in current_selection}:
            st.session_state[selection_key] = [s for s in current_selection if s['url'] != source_obj['url']]
        else:
            current_selection.append(source_obj)
            st.session_state[selection_key] = current_selection

    for i, source in enumerate(sources):
        is_selected = source['url'] in selected_urls
        cite_count = f" (Citations: {source.get('citations', 0)})" if 'citations' in source else ''
        label = f"**[{source['source_type'].upper()}]** {source['title']}{cite_count}"
        st.checkbox(label, value=is_selected, key=f"{selection_key}_{i}", on_change=toggle_source_selection, args=(source,))


# --- Main Workflow Controller ---
if current_step == "start":
    st.markdown("This tool automates research using a two-phase 'Double Diamond' approach. First, it finds foundational, highly-cited papers. Second, it explores the topic more broadly to build thematic understanding.")
    st.divider()
    research_topic = st.text_input(
        "**Enter your research topic:**",
        placeholder="e.g., Best practices for AI project management",
        key="research_topic_input"
    )
    if st.button("üíé Start Diamond 1: Find Foundational Sources", disabled=not research_topic, type="primary"):
        st.session_state.research_topic = research_topic
        st.session_state.research_log = [f"--- Starting New Research on '{research_topic}' ---"]
        with st.spinner("AI is generating queries for foundational papers..."):
            st.session_state.foundational_queries = execute_and_log(agent_foundational_query_crafter, topic=st.session_state.research_topic)
        with st.spinner("AI is fetching highly-cited and review papers... This may take a moment."):
            # Call to the (now restored) robust fetcher function
            fetch_results = execute_and_log(
                step1_fetch_foundational_sources,
                queries=st.session_state.foundational_queries
            )
            st.session_state.foundational_sources = fetch_results.get("sources", [])
            st.session_state.fetch_failures = fetch_results.get("failures", [])
        st.session_state.research_step = "foundational_curation"
        st.rerun()

elif current_step == "foundational_curation":
    display_source_list(st.session_state.foundational_sources, 'curated_foundational_sources', "Foundational Papers (Highly-Cited & Reviews)")

    st.divider()
    num_selected = len(st.session_state.get('curated_foundational_sources', []))
    st.info(f"You have selected **{num_selected}** foundational source(s). These will be locked in and included in the final analysis.")
    if st.button("üíé Start Diamond 2: Explore Thematically", type="primary"):
        st.session_state.fetch_failures = [] # Clear failures before next step
        with st.spinner("AI is generating queries for broader, thematic exploration..."):
            st.session_state.exploratory_queries = execute_and_log(agent_exploratory_query_crafter, topic=st.session_state.research_topic)
        st.session_state.research_step = "exploratory_queries"
        st.rerun()

elif current_step == "exploratory_queries":
    st.markdown("The AI has generated broader queries. Edit, add, or delete them before fetching additional sources.")

    edited_queries = {"scholar_queries": [], "youtube_queries": []}
    col1, col2 = st.columns(2)
    with col1:
        st.subheader("üìö Academic Queries")
        queries = st.session_state.exploratory_queries.get("scholar_queries", [""])
        for i, q in enumerate(queries):
            q_input = st.text_input(f"Academic Query {i+1}", value=q, key=f"sq_{i}", label_visibility="collapsed")
            if q_input: edited_queries["scholar_queries"].append(q_input)
        if st.button("‚ûï Add Academic Query"): st.session_state.exploratory_queries["scholar_queries"].append(""); st.rerun()
    with col2:
        st.subheader("üì∫ Video Queries")
        queries = st.session_state.exploratory_queries.get("youtube_queries", [""])
        for i, q in enumerate(queries):
            q_input = st.text_input(f"YouTube Query {i+1}", value=q, key=f"yq_{i}", label_visibility="collapsed")
            if q_input: edited_queries["youtube_queries"].append(q_input)
        if st.button("‚ûï Add Video Query"): st.session_state.exploratory_queries["youtube_queries"].append(""); st.rerun()

    st.session_state.exploratory_queries = edited_queries
    st.divider()
    if st.button("‚ñ∂Ô∏è Fetch Exploratory Sources", type="primary"):
        with st.spinner("AI is fetching additional papers and videos... This may take a moment."):
            fetch_results = execute_and_log(step2_fetch_exploratory_sources, queries=st.session_state.exploratory_queries)
            st.session_state.exploratory_sources = fetch_results.get("sources", [])
            st.session_state.fetch_failures.extend(fetch_results.get("failures", []))
        st.session_state.research_step = "exploratory_curation"
        st.rerun()

elif current_step == "exploratory_curation":
    display_source_list(st.session_state.exploratory_sources, 'curated_exploratory_sources', "Exploratory Sources (Papers & Videos)")

    st.divider()
    num_selected_foundational = len(st.session_state.get('curated_foundational_sources', []))
    num_selected_exploratory = len(st.session_state.get('curated_exploratory_sources', []))
    st.info(f"You have selected **{num_selected_foundational}** foundational and **{num_selected_exploratory}** exploratory sources.")

    if st.button("‚ñ∂Ô∏è Identify Themes from All Sources", type="primary"):
        if not st.session_state.get('curated_foundational_sources') and not st.session_state.get('curated_exploratory_sources'):
            st.error("You must select at least one source to continue."); st.stop()

        with st.spinner("AI is performing thematic analysis on all curated sources..."):
            all_curated_sources = st.session_state.get('curated_foundational_sources', []) + st.session_state.get('curated_exploratory_sources', [])
            context = build_context_from_sources(st.session_state.research_topic, all_curated_sources)
            st.session_state.research_themes = execute_and_log(agent_thematic_analyser, context=context, existing_themes=None)
        st.session_state.research_step = "theme_curation"
        st.rerun()

elif current_step == "theme_curation":
    st.markdown("Review, re-rank, edit, or delete the themes. Use 'Go Deeper' to find more sources for a specific theme, then generate the initial report.")

    if not st.session_state.research_themes or "Could not determine" in st.session_state.research_themes[0]:
        st.error("Could not identify themes. The content may be too diverse or insufficient.")
    else:
        with st.container(border=True):
            for i, theme in enumerate(st.session_state.research_themes):
                col1, col2, col3, col4, col5 = st.columns([0.7, 0.1, 0.05, 0.05, 0.05])
                new_theme = col1.text_input(f"Theme {i+1}", value=theme, key=f"theme_edit_{i}", label_visibility="collapsed")
                if new_theme != theme: st.session_state.research_themes[i] = new_theme; st.rerun()
                if col2.button("üîé Go Deeper", key=f"deep_{i}"):
                    with st.spinner(f"Searching for more sources on '{theme}'..."):
                        new_sources = execute_and_log(step3_go_deeper, theme_query=theme)
                        if new_sources:
                            existing_urls = {src['url'] for src in st.session_state.exploratory_sources}
                            unique_new = [s for s in new_sources if s['url'] not in existing_urls]
                            if unique_new:
                                st.session_state.exploratory_sources.extend(unique_new)
                                st.toast(f"‚úÖ Added {len(unique_new)} new source(s)! Returning to source curation.")
                                st.session_state.research_step = "exploratory_curation"; st.rerun()
                if col3.button("‚¨ÜÔ∏è", key=f"up_{i}", disabled=(i==0)): st.session_state.research_themes.insert(i-1, st.session_state.research_themes.pop(i)); st.rerun()
                if col4.button("‚¨áÔ∏è", key=f"down_{i}", disabled=(i==len(st.session_state.research_themes)-1)): st.session_state.research_themes.insert(i+1, st.session_state.research_themes.pop(i)); st.rerun()
                if col5.button("üóëÔ∏è", key=f"del_{i}"): st.session_state.research_themes.pop(i); st.rerun()

        if st.button("‚ûï Add New Theme"): st.session_state.research_themes.append("New theme - please edit"); st.rerun()

    st.divider()
    if st.button("‚ñ∂Ô∏è Generate Initial Report", type="primary"):
        final_themes = [theme for theme in st.session_state.research_themes if theme.strip()]
        all_final_sources = st.session_state.get('curated_foundational_sources', []) + st.session_state.get('curated_exploratory_sources', [])
        with st.spinner("AI is generating the initial Discovery Note and Mind Map..."):
            st.session_state.research_final_results = execute_and_log(
                step4_run_synthesis,
                sources=all_final_sources,
                themes=final_themes,
                topic=st.session_state.research_topic
            )
        st.session_state.research_step = "synthesis_complete"
        st.rerun()

elif current_step == "synthesis_complete":
    st.success("The initial `discovery_note.md` and `mindmap.png` have been generated. You can now ingest this note or proceed to the final deep research step.")

    if st.session_state.research_final_results and st.session_state.research_final_results[0]:
        note_path, map_path = st.session_state.research_final_results
        try:
            with open(note_path, 'r', encoding='utf-8') as f: discovery_note = f.read()
            col1, col2 = st.columns(2)
            with col1:
                st.subheader("üìù Discovery Note")
                st.markdown(discovery_note, unsafe_allow_html=True)
                st.download_button("Download Note (.md)", discovery_note, f"{Path(note_path).name}")
            if map_path and Path(map_path).exists():
                 with open(map_path, 'rb') as f: map_image = f.read()
                 with col2:
                    st.subheader("üé® Mind Map")
                    st.image(map_image)
                    st.download_button("Download Mind Map (.png)", map_image, f"{Path(map_path).name}", "image/png")
        except Exception as e:
            st.error(f"Error displaying results: {e}")

        st.divider()
        st.header("üöÄ Final Step: Generate Deep Research Report")
        st.info("This optional final step instructs the AI to take all the information gathered so far (including the themes and mind map) and conduct a final, deeper round of analysis and synthesis to produce a comprehensive research report.")
        if st.button("üß† Generate Deep Research Report", type="primary", use_container_width=True):
            with st.spinner("The Deep Research Agent is now working. This is an intensive process and may take several minutes..."):
                discovery_note_content = ""
                try:
                    with open(st.session_state.research_final_results[0], 'r', encoding='utf-8') as f:
                        discovery_note_content = f.read()
                except Exception:
                    st.error("Could not read initial discovery note to begin deep research.")
                    st.stop()

                final_report_path = execute_and_log(
                    step5_run_deep_synthesis,
                    topic=st.session_state.research_topic,
                    initial_synthesis_note=discovery_note_content
                )
                st.session_state.deep_research_result = final_report_path
            st.session_state.research_step = "deep_research_complete"
            st.rerun()
    else:
        st.error("Synthesis failed to produce output files.")

elif current_step == "deep_research_complete":
    st.balloons()
    st.success("The final, comprehensive research report has been generated and saved to the `external_research` folder.")

    if st.session_state.deep_research_result and Path(st.session_state.deep_research_result).exists():
        report_path = st.session_state.deep_research_result
        try:
            with open(report_path, 'r', encoding='utf-8') as f:
                deep_research_note = f.read()

            st.subheader("üìú Deep Research Report")
            st.markdown(deep_research_note, unsafe_allow_html=True)
            st.download_button(
                "‚¨áÔ∏è Download Deep Research Report (.md)",
                deep_research_note,
                file_name=Path(report_path).name
            )
        except Exception as e:
            st.error(f"Error displaying final report: {e}")
    else:
        st.error("Could not find or display the generated deep research report.")

if current_step != "start":
    st.divider()
    if st.button("‚Ü©Ô∏è Start New Research Topic"):
        keys_to_delete = [key for key in st.session_state.keys() if key.startswith('research_') or key.startswith('deep_')]
        for key in keys_to_delete:
            del st.session_state[key]
        initialize_session_state()
        st.rerun()


--- FILE: ./pages/2_Knowledge_Ingest.py ---
# ## File: pages/2_Knowledge_Ingest.py
# Version: 38.0.0 (Enable Image Scanning)
# Date: 2025-07-22
# Purpose: GUI for knowledge base ingestion.
#          - FEATURE (v38.0.0): Implemented Sprint 21 UI changes. Image file
#            extensions have been removed from the unsupported list, allowing
#            them to be discovered by the file scanner for VLM processing.
#          - FEATURE (v38.0.0): Added "Image/Diagram" to the list of document
#            types for metadata review.

import streamlit as st
import os
import json
import subprocess
import re
import sys
import shutil
from pathlib import Path
from fnmatch import fnmatch
from collections import defaultdict
from datetime import datetime
from typing import List

import fitz
import docx

# --- Project Setup ---
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from cortex_engine.config import STAGING_INGESTION_FILE, INGESTED_FILES_LOG, DEFAULT_EXCLUSION_PATTERNS_STR
from cortex_engine.config_manager import ConfigManager
from cortex_engine.ingest_cortex import RichMetadata
from cortex_engine.collection_manager import WorkingCollectionManager

st.set_page_config(layout="wide", page_title="Knowledge Ingest")

# --- Constants & State ---
REVIEW_PAGE_SIZE = 10
# SPRINT 21: Removed image files from the unsupported list. They are now processed by the backend.
UNSUPPORTED_EXTENSIONS = {
    # Multimedia
    '.mp4', '.mov', '.avi', '.wmv', '.mkv', '.mp3', '.wav', '.aac',
    # Other
    '.tmp', '.lnk'
}


DOC_TYPE_OPTIONS = RichMetadata.model_fields['document_type'].annotation.__args__
PROPOSAL_OUTCOME_OPTIONS = RichMetadata.model_fields['proposal_outcome'].annotation.__args__

def initialize_state(force_reset: bool = False):
    config_manager = ConfigManager()
    config = config_manager.get_config()

    if force_reset:
        keys_to_reset = list(st.session_state.keys())
        for key in keys_to_reset:
            del st.session_state[key]

    st.session_state.knowledge_source_path = config.get("knowledge_source_path", "")
    st.session_state.db_path = config.get("ai_database_path", "")

    defaults = {
        "ingestion_stage": "config", "dir_selections": {},
        "files_to_review": [], "staged_files": [], "file_selections": {},
        "edited_staged_files": [], "review_page": 0, "ingestion_process": None,
        "log_messages": [], "filter_exclude_common": True, "filter_prefer_docx": True,
        "filter_deduplicate": True, "enable_pattern_exclusion": False,
        "exclude_patterns_input": "", "show_confirm_clear_log": False,
        "show_confirm_delete_kb": False, "last_ingested_doc_ids": []
    }
    for key, val in defaults.items():
        if key not in st.session_state: st.session_state[key] = val

    if "directory_scan_path" not in st.session_state or not st.session_state.directory_scan_path:
        st.session_state.directory_scan_path = config.get("knowledge_source_path", "")

def convert_windows_to_wsl_path(path_str: str) -> str:
    path_str = (path_str or "").strip()
    if not path_str or path_str.startswith('/mnt/'): return path_str
    normalized_path = path_str.replace('\\', '/')
    match = re.match(r'^([a-zA-Z]):/(.*)', normalized_path)
    if match:
        drive_letter, rest = match.groups(); return f"/mnt/{drive_letter.lower()}/{rest}"
    return normalized_path

def delete_knowledge_base(db_path: str):
    wsl_db_path = convert_windows_to_wsl_path(db_path)
    chroma_db_dir = Path(wsl_db_path) / "knowledge_hub_db"
    try:
        if chroma_db_dir.exists() and chroma_db_dir.is_dir():
            shutil.rmtree(chroma_db_dir)
            st.success(f"‚úÖ Successfully deleted the Knowledge Base at: {chroma_db_dir}")
        else:
            st.warning("Knowledge Base directory not found.")
    except Exception as e:
        st.error(f"Failed to delete the Knowledge Base directory: {e}")
    st.session_state.show_confirm_delete_kb = False

@st.cache_data
def get_full_file_content(file_path_str: str) -> str:
    file_path = Path(file_path_str)
    try:
        if file_path.suffix.lower() == '.pdf':
            with fitz.open(file_path) as doc: text = "\n".join(page.get_text("text") for page in doc.pages())
        elif file_path.suffix.lower() == '.docx':
            doc = docx.Document(file_path); text = "\n".join(p.text for p in doc.paragraphs)
        # SPRINT 21: Add a preview handler for images (show a message, since text preview is not applicable)
        elif file_path.suffix.lower() in ['.png', '.jpg', '.jpeg']:
            return "[Image file preview not available. The content for ingestion is generated by the Vision AI model.]"
        else:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f: text = f.read()
        return text.strip() if text.strip() else "[No preview available or empty file]"
    except Exception as e: return f"[Error generating preview: {e}]"

def load_staged_files():
    st.session_state.staged_files = []
    staging_path = Path(STAGING_INGESTION_FILE)
    if staging_path.exists():
        try:
            with open(staging_path, 'r') as f: st.session_state.staged_files = json.load(f)
        except (json.JSONDecodeError, IOError) as e:
            st.error(f"Error reading staging file: {e}"); st.session_state.staged_files = []

def scan_for_files(selected_dirs: List[str]):
    wsl_db_path = convert_windows_to_wsl_path(st.session_state.db_path)
    chroma_db_dir = Path(wsl_db_path) / "knowledge_hub_db"
    ingested_log_path = chroma_db_dir / INGESTED_FILES_LOG

    ingested_files = {}
    if ingested_log_path.exists():
        with open(ingested_log_path, 'r') as f:
            try: ingested_files = json.load(f)
            except json.JSONDecodeError: pass

    all_files = []
    for dir_path in selected_dirs:
        wsl_dir_path = convert_windows_to_wsl_path(dir_path)
        all_files.extend([p for p in Path(wsl_dir_path).rglob('*') if p.is_file()])

    candidate_files = [f.as_posix() for f in all_files if f.as_posix() not in ingested_files and f.suffix.lower() not in UNSUPPORTED_EXTENSIONS]

    if st.session_state.filter_exclude_common:
        exclude_keywords = [
            "working", "temp", "archive", "ignore", "backup", "node_modules",
            ".git", "exclude", "draft", "invoice", "timesheet", "contract", "receipt"
        ]
        candidate_files = [f for f in candidate_files if not any(k in part.lower() for k in exclude_keywords for part in Path(f).parts)]

    if st.session_state.enable_pattern_exclusion:
        patterns = [p.strip() for p in st.session_state.exclude_patterns_input.split('\n') if p.strip()]
        candidate_files = [f for f in candidate_files if not any(fnmatch(Path(f).name, p) for p in patterns)]

    if st.session_state.filter_prefer_docx:
        docx_stems = {Path(f).stem for f in candidate_files if f.lower().endswith('.docx')}
        candidate_files = [f for f in candidate_files if not (f.lower().endswith('.pdf') and Path(f).stem in docx_stems)]

    if st.session_state.filter_deduplicate:
        grouped_files = defaultdict(list)
        for f_path in candidate_files:
            pattern = r'[\s_-]*(v\d+(\.\d+)*|draft|\d{8}|final|revised)[\s_-]*'
            normalized = re.sub(pattern, "", Path(f_path).stem, flags=re.IGNORECASE).strip()
            grouped_files[normalized].append(f_path)
        candidate_files = [max(file_list, key=lambda f: Path(f).stat().st_mtime) if len(file_list) > 1 else file_list[0] for _, file_list in grouped_files.items()]

    st.session_state.files_to_review = sorted(candidate_files)
    st.session_state.file_selections = {fp: True for fp in st.session_state.files_to_review}
    st.session_state.review_page = 0
    st.session_state.ingestion_stage = "pre_analysis"

def clear_ingestion_log_file():
    wsl_db_path = convert_windows_to_wsl_path(st.session_state.db_path)
    chroma_db_dir = Path(wsl_db_path) / "knowledge_hub_db"
    ingested_log_path = chroma_db_dir / INGESTED_FILES_LOG
    try:
        chroma_db_dir.mkdir(parents=True, exist_ok=True)
        if ingested_log_path.exists():
            ingested_log_path.unlink()
            st.success(f"Successfully cleared the ingestion log: {ingested_log_path}")
        else:
            st.info("Ingestion log not found (it may have already been cleared).")
    except Exception as e:
        st.error(f"Failed to clear the ingestion log: {e}")
    st.session_state.show_confirm_clear_log = False

def render_config_and_scan_ui():
    st.header("Ingest New Documents")
    st.info("Set your paths, navigate folders, and select directories to scan.")

    def reset_scan_path():
        st.session_state.directory_scan_path = st.session_state.knowledge_source_path
        st.session_state.dir_selections = {}

    st.text_input("1. Root Source Documents Path", key="knowledge_source_path", on_change=reset_scan_path)
    st.text_input("2. Database Storage Path (Destination)", key="db_path")
    st.markdown("---")
    st.markdown("**3. Select Directories to Scan**")

    root_display_path = st.session_state.knowledge_source_path
    root_wsl_path = convert_windows_to_wsl_path(root_display_path)
    is_knowledge_path_valid = os.path.isdir(root_wsl_path)

    if is_knowledge_path_valid:
        current_display_path = st.session_state.directory_scan_path
        st.text_input("Current Directory:", current_display_path, disabled=True)
        current_scan_path_wsl = Path(convert_windows_to_wsl_path(current_display_path))
        try:
            subdirs = sorted([d.name for d in os.scandir(current_scan_path_wsl) if d.is_dir()], key=str.lower)
            c1, c2, c3 = st.columns(3)
            if c1.button("Select All Visible", use_container_width=True):
                for d in subdirs: st.session_state.dir_selections[str(Path(current_display_path) / d)] = True
                st.rerun()
            if c2.button("Deselect All Visible", use_container_width=True):
                for d in subdirs: st.session_state.dir_selections[str(Path(current_display_path) / d)] = False
                st.rerun()
            if current_scan_path_wsl != Path(root_wsl_path):
                if c3.button("‚¨ÜÔ∏è Go Up One Level", use_container_width=True):
                    st.session_state.directory_scan_path = str(Path(current_display_path).parent)
                    st.rerun()
            st.markdown("---")
            if subdirs:
                cols = st.columns(3)
                for i, dirname in enumerate(subdirs):
                    col = cols[i % 3]
                    with col:
                        full_display_path = str(Path(current_display_path) / dirname)
                        is_selected = st.session_state.dir_selections.get(full_display_path, False)
                        with st.container(border=True):
                            c1, c2 = st.columns([0.85, 0.15])
                            with c1:
                                st.session_state.dir_selections[full_display_path] = st.checkbox(dirname, value=is_selected, key=f"cb_{full_display_path}")
                            with c2:
                                if st.button("‚û°Ô∏è", key=f"nav_{full_display_path}", help=f"Navigate into {dirname}", use_container_width=True):
                                    st.session_state.directory_scan_path = full_display_path
                                    st.rerun()
            else:
                st.write("No subdirectories found in the current directory.")
        except Exception as e:
            st.warning(f"Could not read directory: {e}")
    else:
        st.warning("Please provide a valid root source path to enable navigation.")

    st.markdown("---")
    st.markdown("**4. Advanced Options**")
    with st.expander("Filtering & Pattern-Based Exclusion"):
        st.info("The 'Smart Filtering' options below provide robust, default exclusions. Use 'Pattern-Based' for more specific needs.")
        col1, col2 = st.columns(2)
        with col1:
            st.write("**Smart Filtering**"); st.checkbox("Exclude common temp/archive/business folders", key="filter_exclude_common"); st.checkbox("Prefer .docx over .pdf", key="filter_prefer_docx"); st.checkbox("Deduplicate by latest version", key="filter_deduplicate")
        with col2:
            st.write("**Pattern-Based Exclusion**"); st.checkbox("Enable pattern-based exclusion", key="enable_pattern_exclusion", on_change=lambda: setattr(st.session_state, 'exclude_patterns_input', DEFAULT_EXCLUSION_PATTERNS_STR if st.session_state.enable_pattern_exclusion else ""))
            if st.session_state.enable_pattern_exclusion: st.text_area("File Patterns (one per line)", key="exclude_patterns_input", height=150)

    st.markdown("---")
    is_db_path_valid = os.path.isdir(os.path.dirname(convert_windows_to_wsl_path(st.session_state.db_path)))
    selected_to_scan = [path for path, selected in st.session_state.dir_selections.items() if selected]

    if st.button(f"üîé Scan {len(selected_to_scan)} Selected Director(y/ies) for New Files", type="primary", use_container_width=True, disabled=not selected_to_scan):
        if is_knowledge_path_valid and is_db_path_valid:
            config_manager = ConfigManager(); config_manager.update_config({"knowledge_source_path": st.session_state.knowledge_source_path, "ai_database_path": st.session_state.db_path})
            with st.spinner("Applying filters and scanning for documents and images..."):
                scan_for_files(selected_to_scan)
            st.rerun()
        else:
            if not is_knowledge_path_valid: st.error(f"Root Source Path is not valid.")
            if not is_db_path_valid: st.error(f"DB Path's parent is not valid.")

    with st.expander("‚öôÔ∏è Maintenance & Danger Zone"):
        st.warning("These are powerful actions that can result in data loss. Please proceed with caution.")
        st.subheader("Clear Ingestion Log")
        st.info("This action allows all files to be scanned and re-ingested. This is useful if you want to rebuild the knowledge base from scratch with new settings.")
        if st.button("Clear Ingestion Log..."):
            st.session_state.show_confirm_clear_log = True
        if st.session_state.get("show_confirm_clear_log"):
            st.warning(f"This will delete the **{INGESTED_FILES_LOG}** file. The system will then see all source files as new on the next scan. Are you sure?")
            c1, c2 = st.columns(2)
            if c1.button("YES, Clear the Log", use_container_width=True, type="primary"):
                clear_ingestion_log_file()
                st.rerun()
            if c2.button("Cancel", use_container_width=True):
                st.session_state.show_confirm_clear_log = False
                st.rerun()
        st.divider()
        st.subheader("Delete Entire Knowledge Base")
        st.error("‚ö†Ô∏è **DANGER:** This is the most destructive action.")
        if st.button("Permanently Delete Knowledge Base...", type="primary"):
            st.session_state.show_confirm_delete_kb = True
        if st.session_state.get("show_confirm_delete_kb"):
            st.warning(f"This will permanently delete the entire **knowledge_hub_db** directory. This action cannot be undone.")
            c1, c2 = st.columns(2)
            if c1.button("YES, DELETE EVERYTHING", use_container_width=True):
                delete_knowledge_base(st.session_state.db_path)
                st.rerun()
            if c2.button("Cancel Deletion", use_container_width=True):
                st.session_state.show_confirm_delete_kb = False
                st.rerun()

def render_pre_analysis_ui():
    st.header("Pre-Analysis Review")
    files_to_review = st.session_state.get("files_to_review", [])
    total_files = len(files_to_review)

    if not files_to_review:
        st.success("Scan complete. No new documents found based on your criteria.")
        if st.button("‚¨ÖÔ∏è Back to Configuration"): initialize_state(force_reset=True); st.rerun()
        return

    def update_selection(f_path, is_selected):
        st.session_state.file_selections[f_path] = is_selected

    with st.container(border=True):
        sc1, sc2 = st.columns(2)
        def sort_by_name(): st.session_state.files_to_review.sort(key=lambda f: Path(f).name)
        def sort_by_date(): st.session_state.files_to_review.sort(key=lambda f: Path(f).stat().st_mtime, reverse=True)
        sc1.button("Sort by Name (A-Z)", on_click=sort_by_name, use_container_width=True)
        sc2.button("Sort by Date (Newest First)", on_click=sort_by_date, use_container_width=True)

    page = st.session_state.review_page
    start_idx, end_idx = page * REVIEW_PAGE_SIZE, (page + 1) * REVIEW_PAGE_SIZE
    paginated_files = files_to_review[start_idx:end_idx]
    total_pages = -(-total_files // REVIEW_PAGE_SIZE) or 1

    num_selected = sum(1 for f in st.session_state.file_selections.values() if f)
    st.info(f"Found **{total_files}** documents and images. Currently selecting **{num_selected}** for processing. Displaying page {page + 1} of {total_pages}.")

    c1, c2, c3, c4 = st.columns(4)
    if c1.button("Select All on Page", use_container_width=True): st.session_state.file_selections.update({f: True for f in paginated_files}); st.rerun()
    if c2.button("Deselect All on Page", use_container_width=True): st.session_state.file_selections.update({f: False for f in paginated_files}); st.rerun()
    if c3.button("Select All (All Pages)", use_container_width=True): st.session_state.file_selections.update({f: True for f in files_to_review}); st.rerun()
    if c4.button("Deselect All (All Pages)", use_container_width=True): st.session_state.file_selections.update({f: False for f in files_to_review}); st.rerun()

    st.markdown("---")

    for fp in paginated_files:
        mod_time = datetime.fromtimestamp(Path(fp).stat().st_mtime)
        label = f"**{Path(fp).name}** (`{mod_time.strftime('%Y-%m-%d %H:%M:%S')}`)"
        is_selected = st.session_state.file_selections.get(fp, False)
        new_selection = st.checkbox(label, value=is_selected, key=f"cb_{fp}")
        if new_selection != is_selected:
            update_selection(fp, new_selection)
            st.rerun()
        with st.expander("Show Preview"):
             st.text_area("Preview", get_full_file_content(fp), height=200, disabled=True, key=f"preview_{fp}")

    st.divider()
    nav_cols = st.columns([1, 1, 5])
    if page > 0: nav_cols[0].button("‚¨ÖÔ∏è Previous", on_click=lambda: st.session_state.update(review_page=page - 1), use_container_width=True)
    if end_idx < total_files: nav_cols[1].button("Next ‚û°Ô∏è", on_click=lambda: st.session_state.update(review_page=page + 1), use_container_width=True)
    nav_cols[2].write(f"Page {page + 1} of {total_pages}")

    st.divider()
    globally_selected = [f for f, s in st.session_state.file_selections.items() if s]
    globally_ignored = [f for f, s in st.session_state.file_selections.items() if not s]
    proc_c1, proc_c2 = st.columns(2)
    if proc_c1.button("‚¨ÖÔ∏è Back to Configuration", use_container_width=True): initialize_state(force_reset=True); st.rerun()
    if proc_c2.button(f"Process {len(globally_selected)} files & Ignore {len(globally_ignored)}", type="primary", use_container_width=True, disabled=not globally_selected):
        wsl_db_path = convert_windows_to_wsl_path(st.session_state.db_path)
        if globally_ignored:
            chroma_db_dir = Path(wsl_db_path) / "knowledge_hub_db"
            chroma_db_dir.mkdir(parents=True, exist_ok=True)
            ingested_log_path = chroma_db_dir / INGESTED_FILES_LOG
            ingested_log = {}
            if ingested_log_path.exists():
                try:
                    with open(ingested_log_path, 'r') as f: ingested_log = json.load(f)
                except json.JSONDecodeError: pass
            for fp in globally_ignored: ingested_log[fp] = "user_excluded"
            with open(ingested_log_path, 'w') as f: json.dump(ingested_log, f, indent=4)
        st.session_state.log_messages = []; st.session_state.ingestion_stage = "analysis_running"
        command = [sys.executable, "-m", "cortex_engine.ingest_cortex", "--analyze-only", "--db-path", wsl_db_path, "--include", *globally_selected]
        st.session_state.ingestion_process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1, universal_newlines=True)
        st.rerun()

def render_log_and_review_ui(stage_title: str, on_complete_stage: str):
    st.header(stage_title)
    progress_bar = st.progress(0, text="Starting process...")
    log_container = st.container(height=400, border=True)

    if st.session_state.ingestion_process:
        with log_container:
            log_placeholder = st.empty()
            log_placeholder.code("\n".join(st.session_state.log_messages), language="log")

            for line in iter(st.session_state.ingestion_process.stdout.readline, ''):
                line = line.strip()
                if line.startswith("CORTEX_PROGRESS::"):
                    try:
                        _, progress_part, filename_part = line.split("::", 2)
                        current, total = map(int, progress_part.split('/'))
                        progress_bar.progress(current / total, text=f"Analyzing: {current}/{total} - {filename_part}")
                    except (ValueError, IndexError):
                        st.session_state.log_messages.append(line)
                else:
                    st.session_state.log_messages.append(line)
                log_placeholder.code("\n".join(st.session_state.log_messages), language="log")

        st.session_state.ingestion_process.wait()
        st.session_state.ingestion_process = None
        progress_bar.progress(1.0, text="Analysis Complete!")
        st.success("Process finished.")

        if on_complete_stage == "metadata_review":
            load_staged_files()

        st.session_state.ingestion_stage = on_complete_stage
        st.rerun()

def render_completion_screen():
    st.success("‚úÖ Finalization complete! Your knowledge base is up to date.")
    if st.session_state.get('last_ingested_doc_ids'):
        with st.form("new_collection_from_ingest"):
            st.markdown("---")
            st.subheader("Organize Your New Knowledge")
            st.info(f"You've just added {len(st.session_state.last_ingested_doc_ids)} new documents. Save them as a new collection for easy access later.")
            collection_name = st.text_input("New Collection Name", placeholder="e.g., Project Phoenix Discovery")
            if st.form_submit_button("Create Collection", type="primary"):
                if collection_name:
                    collection_mgr = WorkingCollectionManager()
                    if collection_name in collection_mgr.get_collection_names():
                        st.error(f"Collection '{collection_name}' already exists.")
                    else:
                        collection_mgr.add_docs_by_id_to_collection(collection_name, st.session_state.last_ingested_doc_ids)
                        st.success(f"Successfully created collection '{collection_name}'!")
                        st.session_state.last_ingested_doc_ids = []
                else:
                    st.warning("Please provide a name for the collection.")
    st.markdown("---")
    if st.button("‚¨ÖÔ∏è Start a New Ingestion"):
        initialize_state(force_reset=True)
        st.rerun()

def render_metadata_review_ui():
    st.header("Review AI-Generated Metadata")

    if 'edited_staged_files' not in st.session_state or not st.session_state.edited_staged_files:
        initial_files = st.session_state.get('staged_files', [])
        for doc in initial_files:
            is_error = doc.get('rich_metadata', {}).get('summary', '').startswith("ERROR:")
            doc['exclude_from_final'] = is_error
        st.session_state.edited_staged_files = initial_files
        st.session_state.review_page = 0

    edited_files = st.session_state.edited_staged_files
    if not edited_files:
        st.success("Analysis complete, but no documents were staged for review.")
        st.info("Check `logs/ingestion.log` for details.")
        if st.button("‚¨ÖÔ∏è Back to Configuration"): initialize_state(force_reset=True); st.rerun()
        return

    st.info(f"Please review and approve the metadata for the **{len(edited_files)}** document(s) below.")

    page = st.session_state.review_page
    start_idx, end_idx = page * REVIEW_PAGE_SIZE, (page + 1) * REVIEW_PAGE_SIZE
    paginated_files = edited_files[start_idx:end_idx]
    total_pages = -(-len(edited_files) // REVIEW_PAGE_SIZE) or 1

    def update_edited_state(index, field, value):
        if field == 'include': st.session_state.edited_staged_files[index]['exclude_from_final'] = not value
        elif field == 'thematic_tags': st.session_state.edited_staged_files[index]['rich_metadata'][field] = [tag.strip() for tag in value.split(',') if tag.strip()]
        else: st.session_state.edited_staged_files[index]['rich_metadata'][field] = value

    for i, doc in enumerate(paginated_files):
        absolute_index = start_idx + i
        rich_meta = doc.get('rich_metadata', {})
        is_included = not doc.get('exclude_from_final', False)
        checkbox_label = f"**{doc.get('file_name', 'N/A')}** - {rich_meta.get('summary', 'No summary available.')}"

        new_include_val = st.checkbox(checkbox_label, value=is_included, key=f"include_{absolute_index}")
        if new_include_val != is_included:
            update_edited_state(absolute_index, 'include', new_include_val); st.rerun()

        with st.expander("Edit Metadata & Preview"):
            try: doc_type_index = DOC_TYPE_OPTIONS.index(rich_meta.get('document_type'))
            except (ValueError, TypeError): doc_type_index = len(DOC_TYPE_OPTIONS) - 1
            try: outcome_index = PROPOSAL_OUTCOME_OPTIONS.index(rich_meta.get('proposal_outcome'))
            except (ValueError, TypeError): outcome_index = len(PROPOSAL_OUTCOME_OPTIONS) - 1

            st.selectbox("Document Type", options=DOC_TYPE_OPTIONS, index=doc_type_index, key=f"dt_{absolute_index}", on_change=lambda idx=absolute_index: update_edited_state(idx, 'document_type', st.session_state[f"dt_{idx}"]))
            st.selectbox("Proposal Outcome", options=PROPOSAL_OUTCOME_OPTIONS, index=outcome_index, key=f"oc_{absolute_index}", on_change=lambda idx=absolute_index: update_edited_state(idx, 'proposal_outcome', st.session_state[f"oc_{idx}"]))
            st.text_area("Summary", value=rich_meta.get('summary', ''), key=f"sm_{absolute_index}", height=100, on_change=lambda idx=absolute_index: update_edited_state(idx, 'summary', st.session_state[f"sm_{idx}"]))
            st.text_input("Thematic Tags (comma-separated)", value=", ".join(rich_meta.get('thematic_tags', [])), key=f"tg_{absolute_index}", on_change=lambda idx=absolute_index: update_edited_state(idx, 'thematic_tags', st.session_state[f"tg_{idx}"]))
            st.divider()
            st.text_area("File Content Preview", get_full_file_content(doc['doc_posix_path']), height=200, disabled=True, key=f"preview_{doc['doc_posix_path']}")

    st.divider()
    nav_cols = st.columns([1, 1, 5])
    if page > 0: nav_cols[0].button("‚¨ÖÔ∏è Previous", on_click=lambda: st.session_state.update(review_page=page - 1), use_container_width=True)
    if end_idx < len(edited_files): nav_cols[1].button("Next ‚û°Ô∏è", on_click=lambda: st.session_state.update(review_page=page + 1), use_container_width=True)
    nav_cols[2].write(f"Page {page + 1} of {total_pages}")

    st.divider()
    action_cols = st.columns(2)
    if action_cols[0].button("‚¨ÖÔ∏è Cancel and Go Back", use_container_width=True): initialize_state(force_reset=True); st.rerun()
    if action_cols[1].button("‚úÖ Finalize Approved Documents", use_container_width=True, type="primary"):
        final_files_to_process = st.session_state.edited_staged_files
        doc_ids_to_ingest = [doc['doc_id'] for doc in final_files_to_process if not doc.get('exclude_from_final')]
        st.session_state.last_ingested_doc_ids = doc_ids_to_ingest
        with open(STAGING_INGESTION_FILE, 'w') as f: json.dump(final_files_to_process, f, indent=2)

        wsl_db_path = convert_windows_to_wsl_path(st.session_state.db_path)
        if not wsl_db_path or not Path(wsl_db_path).exists():
             st.error(f"Database path is invalid or does not exist: {wsl_db_path}"); st.stop()

        st.session_state.log_messages = ["Finalizing ingestion..."]
        st.session_state.ingestion_stage = "finalizing"
        command = [sys.executable, "-m", "cortex_engine.ingest_cortex", "--finalize-from-staging", "--db-path", wsl_db_path]
        st.session_state.ingestion_process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True, bufsize=1, universal_newlines=True)
        st.rerun()

# --- Main App Logic ---
initialize_state()
st.title("2. Knowledge Ingest")
st.caption(f"Manage the knowledge base by ingesting new documents. App Version: v38.0.0 (Enable Image Scanning)")
st.markdown("---")
stage = st.session_state.get("ingestion_stage", "config")
if stage == "config": render_config_and_scan_ui()
elif stage == "pre_analysis": render_pre_analysis_ui()
elif stage == "analysis_running": render_log_and_review_ui("Live Analysis Log", "metadata_review")
elif stage == "metadata_review": render_metadata_review_ui()
elif stage == "finalizing": render_log_and_review_ui("Live Finalization Log", "config_done")
elif stage == "config_done": render_completion_screen()


--- FILE: ./pages/3_Knowledge_Search.py ---
# ## File: pages/3_Knowledge_Search.py
# Version: 20.0.0 (Native Filter Fix)
# Date: 2025-07-22
# Purpose: A UI for searching the knowledge base, managing collections,
#          and deleting documents from the KB.
#          - CRITICAL FIX (v20.0.0): To permanently resolve the recurring
#            `AttributeError`, the filter logic has been re-architected to
#            bypass the `llama-index` filter translation layer. The code now
#            manually constructs the native ChromaDB 'where' clause dictionary
#            and passes it via `vector_store_kwargs`. This is a more robust
#            method that avoids the adapter bug and ensures complex AND/OR
#            filter combinations work as expected.

import streamlit as st
from llama_index.core import (
    VectorStoreIndex,
    StorageContext,
    load_index_from_storage,
)
from llama_index.core.settings import Settings
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.core.vector_stores import ExactMatchFilter, MetadataFilters
from llama_index.llms.ollama import Ollama
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from collections import OrderedDict
import chromadb
from chromadb.config import Settings as ChromaSettings
import os
import sys
from pathlib import Path
import logging
import re
import json
from datetime import datetime

# Add the project root to the Python path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from cortex_engine.collection_manager import WorkingCollectionManager
from cortex_engine.session_state import initialize_app_session_state
from cortex_engine.config import COLLECTION_NAME, INGESTED_FILES_LOG, EMBED_MODEL

# --- App Config ---
APP_VERSION = "v20.0.0 (Native Filter Fix)"
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Constants ---
DOC_TYPE_OPTIONS = [
    "Any", "Project Plan", "Technical Documentation", "Proposal/Quote", "Case Study / Trophy",
    "Final Report", "Draft Report", "Presentation", "Contract/SOW",
    "Meeting Minutes", "Financial Report", "Research Paper", "Email Correspondence", "Other"
]
PROPOSAL_OUTCOME_OPTIONS = ["Any", "Won", "Lost", "Pending", "N/A"]
RESULTS_PAGE_SIZE = 5

# --- Helper Functions ---
def convert_windows_to_wsl_path(win_path):
    if not isinstance(win_path, str) or win_path.startswith('/mnt/'): return win_path
    path_str = str(win_path).replace('\\', '/')
    match = re.match(r"^([a-zA-Z]):/(.*)", path_str)
    if match: drive, rest = match.groups(); return f"/mnt/{drive.lower()}/{rest}"
    return win_path

def heal_and_get_collection_doc_ids(collection_name):
    collection_mgr = WorkingCollectionManager()
    st.session_state.collections = collection_mgr.collections
    collection_obj = st.session_state.collections.get(collection_name, {})
    return set(collection_obj.get("doc_ids", []))

def delete_document_from_kb(doc_id: str, file_path: str, vector_collection, db_path: str):
    try:
        if not vector_collection: st.error("Vector collection not available. Cannot perform deletion."); return False
        nodes_to_delete = vector_collection.get(where={"doc_id": doc_id})
        node_ids_to_delete = nodes_to_delete.get('ids', [])
        if not node_ids_to_delete: st.warning(f"No vector data found for document ID {doc_id}. It may have already been removed.")
        else: vector_collection.delete(ids=node_ids_to_delete); st.toast(f"Deleted {len(node_ids_to_delete)} vector chunks from the knowledge base.")
        chroma_db_path = os.path.join(db_path, "knowledge_hub_db")
        processed_log_path = os.path.join(chroma_db_path, INGESTED_FILES_LOG)
        if os.path.exists(processed_log_path):
            with open(processed_log_path, 'r') as f: log_data = json.load(f)
            key_to_delete = next((key for key in log_data if Path(key).as_posix() == Path(file_path).as_posix()), None)
            if key_to_delete: del log_data[key_to_delete]; st.toast("Removed document from the processed files log.")
            with open(processed_log_path, 'w') as f: json.dump(log_data, f, indent=4)
        collection_mgr = WorkingCollectionManager()
        for coll_name in collection_mgr.get_collection_names(): collection_mgr.remove_from_collection(coll_name, [doc_id])
        st.toast("Removed document from all working collections.")
        st.success(f"Successfully pruned document '{Path(file_path).name}' from the system.")
        return True
    except Exception as e: st.error(f"An error occurred during deletion: {e}"); return False

# --- Core Loading and State Management ---
@st.cache_resource(ttl=3600)
def load_base_index(db_path, model_provider, api_key=None):
    if not db_path or not db_path.strip(): st.warning("Database path is not configured."); return None, None
    wsl_db_path = convert_windows_to_wsl_path(db_path)
    chroma_db_path = os.path.join(wsl_db_path, "knowledge_hub_db")
    if not os.path.isdir(chroma_db_path): st.warning(f"üß† Knowledge base directory not found at '{chroma_db_path}'."); return None, None
    try:
        Settings.llm = Ollama(model="mistral", request_timeout=120.0)
        Settings.embed_model = HuggingFaceEmbedding(model_name=EMBED_MODEL)
        db_settings = ChromaSettings(anonymized_telemetry=False)
        db = chromadb.PersistentClient(path=chroma_db_path, settings=db_settings)
        chroma_collection = db.get_or_create_collection(COLLECTION_NAME)
        vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
        storage_context = StorageContext.from_defaults(vector_store=vector_store, persist_dir=chroma_db_path)
        index = load_index_from_storage(storage_context)
        st.success(f"‚úÖ Knowledge base loaded successfully from '{chroma_db_path}'.")
        return index, chroma_collection
    except Exception as e:
        st.error(f"Backend initialization failed: {e}")
        logging.error(f"Error loading query engine from {chroma_db_path}: {e}", exc_info=True)
        return None, None

def initialize_search_state():
    if 'search_sort_key' not in st.session_state: st.session_state.search_sort_key = 'score'
    if 'search_sort_asc' not in st.session_state: st.session_state.search_sort_asc = False
    if 'search_page' not in st.session_state: st.session_state.search_page = 0
    if 'doc_type_filter' not in st.session_state: st.session_state.doc_type_filter = "Any"
    if 'outcome_filter' not in st.session_state: st.session_state.outcome_filter = "Any"
    if 'filter_operator' not in st.session_state: st.session_state.filter_operator = "AND"
    if 'query_text' not in st.session_state: st.session_state.query_text = ""
    if 'search_results' not in st.session_state: st.session_state.search_results = []

def reset_search_state():
    st.session_state.search_sort_key = 'score'
    st.session_state.search_sort_asc = False
    st.session_state.search_page = 0
    st.session_state.doc_type_filter = "Any"
    st.session_state.outcome_filter = "Any"
    st.session_state.filter_operator = "AND"
    st.session_state.query_text = ""
    st.session_state.search_results = []


# --- UI Rendering ---

def render_sidebar():
    collection_mgr = WorkingCollectionManager()
    with st.sidebar:
        st.header("‚öôÔ∏è Configuration")
        st.text_input("Database Storage Path", key="db_path_input", disabled=True)
        st.divider()
        st.header("üìö Working Collections")
        st.info("First, select a collection. Then, use the 'Add' buttons on search results to add documents to it.")
        collection_names = collection_mgr.get_collection_names()
        try:
            current_index = collection_names.index(st.session_state.selected_collection)
        except (ValueError, AttributeError):
            current_index = 0
            st.session_state.selected_collection = collection_names[0] if collection_names else "default"
        st.selectbox("Active Collection", options=collection_names, key="selected_collection", index=current_index)
        with st.form("new_collection_form"):
            new_name = st.text_input("Create New Collection")
            if st.form_submit_button("Create"):
                name = new_name.strip()
                if name:
                    if collection_mgr.create_collection(name): st.session_state.collection_to_select = name; st.rerun()
                    else: st.warning("Collection already exists.")
                else: st.warning("Name cannot be empty.")

def render_main_content(base_index, vector_collection):
    st.header("üîé 3. Knowledge Search")
    if not base_index:
        st.markdown("---"); st.info("Backend not loaded. Please check configuration and ensure ingestion has been run."); return

    # --- Search Input UI ---
    st.text_input("Free-Text Search (optional)", key="query_text", placeholder="e.g., project management for AI")
    st.markdown("**Metadata Filters (optional)**")
    col1, col2, col3 = st.columns([2, 2, 1])
    with col1: st.selectbox("Document Type", options=DOC_TYPE_OPTIONS, key="doc_type_filter")
    with col2: st.selectbox("Proposal Outcome", options=PROPOSAL_OUTCOME_OPTIONS, key="outcome_filter")
    num_active_filters = (1 if st.session_state.doc_type_filter != "Any" else 0) + (1 if st.session_state.outcome_filter != "Any" else 0)
    with col3: st.radio("Operator", ["AND", "OR"], key="filter_operator", horizontal=True, disabled=(num_active_filters < 2))
    st.radio("Search In:", ["Entire Knowledge Base", "Active Collection"], key="search_scope", horizontal=True)

    # --- Search Execution Logic ---
    search_button_disabled = not st.session_state.query_text.strip() and num_active_filters == 0
    if st.button("Search Knowledge Base", type="primary", disabled=search_button_disabled):
        query = st.session_state.query_text.strip()

        # --- START: v20.0.0 NATIVE CHROMA FILTER LOGIC ---
        where_clause = {}
        all_conditions = []

        # 1. Build list of metadata conditions from UI
        ui_metadata_conditions = []
        if st.session_state.doc_type_filter != "Any":
            ui_metadata_conditions.append({"document_type": st.session_state.doc_type_filter})
        if st.session_state.outcome_filter != "Any":
            ui_metadata_conditions.append({"proposal_outcome": st.session_state.outcome_filter})

        # 2. Group the UI conditions based on the AND/OR operator
        if len(ui_metadata_conditions) > 1:
            operator_key = f"${st.session_state.filter_operator.lower()}"
            all_conditions.append({operator_key: ui_metadata_conditions})
        elif ui_metadata_conditions:
            all_conditions.extend(ui_metadata_conditions)

        # 3. Build the collection scope condition (always an OR group)
        if st.session_state.search_scope == "Active Collection":
            collection_doc_ids = list(heal_and_get_collection_doc_ids(st.session_state.selected_collection))
            if collection_doc_ids:
                scope_conditions = [{"doc_id": doc_id} for doc_id in collection_doc_ids]
                all_conditions.append({"$or": scope_conditions})
            else:
                st.warning(f"The '{st.session_state.selected_collection}' collection is empty. Searching entire knowledge base instead.")

        # 4. Combine all conditions into a final '$and' group if needed
        if len(all_conditions) > 1:
            where_clause = {"$and": all_conditions}
        elif len(all_conditions) == 1:
            where_clause = all_conditions[0]
        # else: where_clause remains {}

        retriever_kwargs = {"vector_store_kwargs": {"where": where_clause}} if where_clause else {}
        # --- END: v20.0.0 NATIVE CHROMA FILTER LOGIC ---

        with st.spinner("Searching..."):
            retriever = base_index.as_retriever(similarity_top_k=50, **retriever_kwargs)
            search_text = query if query else " "
            response_nodes = retriever.retrieve(search_text)
            unique_results = list(OrderedDict((node.metadata.get('doc_id'), node) for node in response_nodes).values())
            st.session_state.search_results = unique_results
            st.session_state.search_page = 0
            st.session_state.search_sort_key = 'score'
            st.session_state.search_sort_asc = False
            st.rerun()

    # --- Results Viewer ---
    if st.session_state.get("search_results"):
        st.divider()
        st.subheader("Search Results")
        results = st.session_state.search_results

        # --- Sorting Controls ---
        sort_col1, sort_col2, sort_col3 = st.columns(3)
        if sort_col1.button("Sort by Relevance (Score)", use_container_width=True): st.session_state.search_sort_key = 'score'; st.session_state.search_sort_asc = False; st.rerun()
        if sort_col2.button("Sort by Filename (A-Z)", use_container_width=True): st.session_state.search_sort_key = 'filename'; st.session_state.search_sort_asc = True; st.rerun()
        if sort_col3.button("Sort by Date (Newest First)", use_container_width=True): st.session_state.search_sort_key = 'date'; st.session_state.search_sort_asc = False; st.rerun()

        # --- Sorting Logic ---
        sort_key = st.session_state.search_sort_key; sort_asc = st.session_state.search_sort_asc
        if sort_key == 'score': results.sort(key=lambda n: n.score or 0, reverse=not sort_asc)
        elif sort_key == 'filename': results.sort(key=lambda n: n.metadata.get('file_name', '').lower(), reverse=not sort_asc)
        elif sort_key == 'date': results.sort(key=lambda n: datetime.fromisoformat(n.metadata.get('last_modified_date', '1970-01-01T00:00:00')) if n.metadata.get('last_modified_date') else datetime.min, reverse=not sort_asc)

        # --- Pagination Logic ---
        total_results = len(results); total_pages = -(-total_results // RESULTS_PAGE_SIZE) or 1
        current_page = st.session_state.search_page
        start_idx = current_page * RESULTS_PAGE_SIZE; end_idx = start_idx + RESULTS_PAGE_SIZE
        paginated_results = results[start_idx:end_idx]
        st.info(f"Showing {len(paginated_results)} of {total_results} results. Page {current_page + 1} of {total_pages}.")

        active_collection_ids = heal_and_get_collection_doc_ids(st.session_state.selected_collection)
        collection_mgr = WorkingCollectionManager()
        if not paginated_results: st.info("No relevant documents found for your query and filters.")

        for node in paginated_results:
            doc_id = node.metadata.get('doc_id')
            file_name = node.metadata.get('file_name', 'Unknown Document')
            is_in_collection = doc_id in active_collection_ids
            col_main, col_actions = st.columns([0.8, 0.2])

            with col_main:
                meta_display = f"**Type:** {node.metadata.get('document_type', 'N/A')} | **Outcome:** {node.metadata.get('proposal_outcome', 'N/A')} | **Score:** {node.score:.2f}"
                with st.expander(f"**{file_name}**"):
                    st.markdown(meta_display)
                    st.text_area("Preview", node.get_content(), height=250, disabled=True, key=f"preview_{doc_id}")
                    st.caption(f"Document ID: {doc_id}")
                    st.divider()
                    if st.checkbox("Show Maintenance Actions", key=f"show_maint_{doc_id}"):
                        st.warning(f"This will permanently delete **{file_name}** from the knowledge base. This action cannot be undone.")
                        if st.button("DELETE PERMANENTLY", key=f"del_btn_{doc_id}", type="primary"):
                            wsl_db_path = convert_windows_to_wsl_path(st.session_state.db_path_input)
                            if delete_document_from_kb(doc_id, node.metadata.get('doc_posix_path', ''), vector_collection, wsl_db_path):
                                st.session_state.search_results = []; st.rerun()
            with col_actions:
                if is_in_collection:
                    if st.button("Remove", key=f"rem_{doc_id}", use_container_width=True):
                        collection_mgr.remove_from_collection(st.session_state.selected_collection, [doc_id]); st.rerun()
                else:
                    if st.button("Add to Collection", key=f"add_{doc_id}", type="secondary", use_container_width=True):
                        collection_mgr.add_docs_by_id_to_collection(st.session_state.selected_collection, [doc_id]); st.rerun()

        # --- Pagination Controls ---
        st.divider()
        page_col1, page_col2, page_col3 = st.columns([1, 6, 1])
        if current_page > 0:
            if page_col1.button("‚¨ÖÔ∏è Previous Page", use_container_width=True): st.session_state.search_page -= 1; st.rerun()
        if end_idx < total_results:
            if page_col3.button("Next Page ‚û°Ô∏è", use_container_width=True): st.session_state.search_page += 1; st.rerun()
        page_col2.write("")

    if st.button("Reset Search / Clear Filters"):
        reset_search_state()
        st.rerun()

# --- Main App Logic ---
st.set_page_config(layout="wide", page_title="Cortex Knowledge Search")
st.title("üß† Project Cortex Suite")
st.caption(f"App Version: {APP_VERSION}")

if "collection_to_select" in st.session_state:
    st.session_state.selected_collection = st.session_state.collection_to_select
    del st.session_state.collection_to_select

initialize_app_session_state()
initialize_search_state()

base_index, vector_collection = load_base_index(st.session_state.db_path_input, "Local")

render_sidebar()
render_main_content(base_index, vector_collection)


--- FILE: ./pages/4_Collection_Management.py ---
# ## File: pages/4_Collection_Management.py
# Version: 6.1.0 (Document Removal Feature)
# Date: 2025-07-16
# Purpose: A UI for managing Working Collections.
#          - FEATURE (v6.1.0): Added the ability for a user to remove a
#            single document from a collection directly from the view expander.

import streamlit as st
import sys
from pathlib import Path
import os
import re
import chromadb
from chromadb.config import Settings as ChromaSettings
from collections import OrderedDict
from datetime import datetime

project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from cortex_engine.collection_manager import WorkingCollectionManager
from cortex_engine.session_state import initialize_app_session_state
from cortex_engine.config import COLLECTION_NAME
from cortex_engine.config_manager import ConfigManager

def convert_windows_to_wsl_path(win_path):
    if not isinstance(win_path, str) or win_path.startswith('/mnt/'): return win_path
    path_str = str(win_path).replace('\\', '/')
    match = re.match(r"^([a-zA-Z]):/(.*)", path_str)
    if match: return f"/mnt/{match.groups()[0].lower()}/{match.groups()[1]}"
    return win_path

@st.cache_resource
def init_chroma_client(db_path):
    wsl_db_path = convert_windows_to_wsl_path(db_path)
    chroma_db_path = os.path.join(wsl_db_path, "knowledge_hub_db")
    if not db_path or not os.path.isdir(chroma_db_path):
        st.error(f"Database path not found: '{chroma_db_path}'. Please configure it in Knowledge Ingest.")
        return None
    try:
        db_settings = ChromaSettings(anonymized_telemetry=False)
        return chromadb.PersistentClient(path=chroma_db_path, settings=db_settings)
    except Exception as e:
        st.error(f"Failed to connect to ChromaDB: {e}")
        return None

st.set_page_config(layout="wide", page_title="Cortex Collection Management")
st.title("üìö 4. Collection Management")
st.caption("View, sort, merge, and manage your curated Working Collections.")

initialize_app_session_state()

config = ConfigManager().get_config()
db_path = config.get("ai_database_path", "")
st.text_input("Knowledge Hub DB Path", value=db_path, disabled=True, help="Set this on the Knowledge Ingest page.")

if not db_path:
    st.warning("Please set a valid AI Database Path on the 'Knowledge Ingest' page first.")
    st.stop()

chroma_client = init_chroma_client(db_path)
if not chroma_client: st.stop()

try:
    vector_collection = chroma_client.get_collection(name=COLLECTION_NAME)
except Exception as e:
    st.error(f"Could not connect to collection '{COLLECTION_NAME}'. Please run an ingestion process first. Error: {e}")
    st.stop()

if 'collection_sort_key' not in st.session_state: st.session_state.collection_sort_key = 'modified_at'
if 'collection_sort_order_asc' not in st.session_state: st.session_state.collection_sort_order_asc = False

def set_sort_order(key):
    if st.session_state.collection_sort_key == key: st.session_state.collection_sort_order_asc = not st.session_state.collection_sort_order_asc
    else: st.session_state.collection_sort_key = key; st.session_state.collection_sort_order_asc = True if key == 'name' else False

collection_mgr = WorkingCollectionManager()
collections_list = list(collection_mgr.collections.values())

is_reverse = not st.session_state.collection_sort_order_asc
sort_key = st.session_state.collection_sort_key
if sort_key == 'name': collections_list.sort(key=lambda x: x.get('name', '').lower(), reverse=is_reverse)
else: collections_list.sort(key=lambda x: x.get('modified_at', '1970-01-01T00:00:00Z'), reverse=is_reverse)

st.header("Your Collections")

c1, c2, c3 = st.columns([4, 1, 2])
with c1:
    name_sort_icon = "üîº" if st.session_state.collection_sort_order_asc else "üîΩ"
    if st.button(f"Collection Name {name_sort_icon if sort_key == 'name' else ''}", use_container_width=True): set_sort_order('name'); st.rerun()
with c2:
    st.markdown("<p style='text-align: center; font-weight: bold;'>Documents</p>", unsafe_allow_html=True)
with c3:
    mod_sort_icon = "üîº" if st.session_state.collection_sort_order_asc else "üîΩ"
    if st.button(f"Last Modified {mod_sort_icon if sort_key == 'modified_at' else ''}", use_container_width=True): set_sort_order('modified_at'); st.rerun()
st.markdown("---")

if not collections_list:
    st.info("No collections found. Create one on the 'Knowledge Search' page."); st.stop()

for collection in collections_list:
    name = collection['name']
    doc_ids = collection.get('doc_ids', [])
    modified_iso = collection.get('modified_at', 'N/A')
    try: modified_dt = datetime.fromisoformat(modified_iso.replace('Z', '+00:00')); modified_str = modified_dt.strftime('%Y-%m-%d %H:%M')
    except (ValueError, TypeError): modified_str = "N/A"

    with st.container(border=True):
        # --- Main Collection Row ---
        col1, col2, col3, col4, col5 = st.columns([4, 1, 2, 1, 1])
        with col1: st.markdown(f"**{name}**")
        with col2: st.markdown(f"<p style='text-align: center;'>{len(doc_ids)}</p>", unsafe_allow_html=True)
        with col3: st.write(modified_str)
        with col4:
            if st.button("üìÑ View", key=f"view_{name}", use_container_width=True):
                st.session_state[f"view_visible_{name}"] = not st.session_state.get(f"view_visible_{name}", False)
        with col5:
            if st.button("‚öôÔ∏è Manage", key=f"manage_{name}", use_container_width=True, disabled=(name=="default")):
                st.session_state[f"manage_visible_{name}"] = not st.session_state.get(f"manage_visible_{name}", False)

        # --- Expander for Viewing Documents ---
        if st.session_state.get(f"view_visible_{name}", False):
            with st.container():
                if not doc_ids:
                    st.info("This collection is empty.")
                else:
                    with st.spinner("Fetching document details..."):
                        results = vector_collection.get(where={"doc_id": {"$in": doc_ids}}, include=["metadatas"])
                        unique_docs = {meta['doc_id']: meta for meta in results.get('metadatas', []) if 'doc_id' in meta}

                        if not unique_docs:
                            st.warning("Could not retrieve details for documents in this collection. They may have been deleted from the Knowledge Base.")

                        for doc_id, meta in unique_docs.items():
                            doc_col, action_col = st.columns([0.9, 0.1])
                            with doc_col:
                                st.markdown(f"- **{meta.get('file_name', 'N/A')}** (`{meta.get('doc_posix_path', 'N/A')}`)")
                            with action_col:
                                if st.button("‚ùå", key=f"remove_{doc_id}_from_{name}", help="Remove from this collection"):
                                    collection_mgr.remove_from_collection(name, [doc_id])
                                    st.toast(f"Removed '{meta.get('file_name', 'document')}' from '{name}'.")
                                    st.rerun()

        # --- Expander for Management Actions ---
        if st.session_state.get(f"manage_visible_{name}", False) and name != "default":
            with st.container(border=True):
                st.markdown("**Combine Collections**")
                merge_options = [c for c in collection_mgr.get_collection_names() if c != name]
                dest_collection = st.selectbox("Merge this collection into:", merge_options, key=f"merge_dest_{name}", index=None, placeholder="Select a destination...")
                if dest_collection:
                    st.warning(f"This will add all documents from **{name}** to **{dest_collection}** and then permanently delete **{name}**.")
                    if st.checkbox("I understand and want to proceed.", key=f"merge_confirm_{name}"):
                        if st.button("Merge and Delete Collection", key=f"merge_btn_{name}"):
                            with st.spinner(f"Merging '{name}' into '{dest_collection}'..."):
                                if collection_mgr.merge_collections(name, dest_collection):
                                    st.success("Merge successful!"); st.rerun()
                                else:
                                    st.error("Merge failed. Please check logs.")
                st.divider()

                st.markdown("**Export Collection**")
                output_dir = st.text_input("Destination Directory", key=f"export_path_{name}", placeholder="e.g., C:\\Users\\YourUser\\Desktop\\Export")
                if st.button("Export Files", key=f"export_btn_{name}"):
                    if output_dir:
                        with st.spinner("Exporting files..."):
                            copied, failed = collection_mgr.export_collection_files(name, output_dir, vector_collection)
                            st.success(f"‚úÖ Export complete! Copied {len(copied)} files.")
                            if failed: st.warning(f"Could not copy {len(failed)} files: {', '.join(failed)}")
                    else: st.error("Please provide a destination directory.")
                st.divider()

                st.markdown("**Rename Collection**")
                new_name_input = st.text_input("New name", key=f"rename_input_{name}")
                if st.button("Rename", key=f"rename_btn_{name}"):
                    if new_name_input and new_name_input.strip() not in collection_mgr.collections:
                        collection_mgr.rename_collection(name, new_name_input.strip())
                        st.success(f"Renamed '{name}' to '{new_name_input.strip()}'."); st.rerun()
                    else: st.error("New name cannot be empty or already exist.")
                st.divider()

                st.markdown("**Delete Collection**")
                st.warning(f"This action is permanent and cannot be undone.")
                if st.checkbox(f"I understand, permanently delete '{name}'.", key=f"delete_confirm_{name}"):
                    if st.button("DELETE PERMANENTLY", type="primary", key=f"delete_btn_{name}"):
                        collection_mgr.delete_collection(name)
                        st.success(f"Successfully deleted collection '{name}'."); st.rerun()


--- FILE: ./pages/5_Proposal_Step_1_Prep.py ---
# ## File: pages/5_Proposal_Step_1.py
# Version: 27.0.0 (Corrected Placeholder Parsing)
# Date: 2025-07-15
# Purpose: An interactive UI to tag document placeholders.
#          - CRITICAL FIX (v27.0.0): The parsing logic has been entirely rewritten
#            to correctly handle placeholder paragraphs. The editor now identifies
#            a heading, and then presents EVERY subsequent paragraph (including empty
#            ones) as a taggable item, resolving the "missing sections" bug.
#          - FIX (v27.0.0): The template processing logic was updated to match the
#            new parsing logic, ensuring correct replacement of all items.

import streamlit as st
import sys
from pathlib import Path
import io
import docx
from docx.text.paragraph import Paragraph
from docx.table import Table
import json
import os

# --- Project Setup ---
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from cortex_engine.instruction_parser import iter_block_items

# --- Configuration ---
st.set_page_config(layout="wide", page_title="Cortex Template Editor")

MAPS_DIR = project_root / "template_maps"
MAPS_DIR.mkdir(exist_ok=True)

CORTEX_ACTIONS = {
    "Ignore this block": "[IGNORE]",
    "Insert Content from a File": "[INSERT_FROM_FILE]",
    "Human Input (Manual Text)": "[PROMPT_HUMAN]",
    "AI: Generate from KB only": "[GENERATE_FROM_KB_ONLY]",
    "AI: Generate from Proposal only": "[GENERATE_FROM_PROPOSAL_ONLY]",
    "AI: Generate from both KB and Proposal": "[GENERATE_FROM_KB_AND_PROPOSAL]",
    "AI: Retrieve Case Studies from KB": "[RETRIEVE_FROM_KB]",
    "AI: Generate Resources": "[GENERATE_RESOURCES]",
}


# --- Core Functions ---

def initialize_state():
    """Initializes all session state keys needed for the editor."""
    keys = ['template_doc_bytes', 'processed_doc_bytes', 'template_elements', 'action_map', 'template_filename', 'inserted_content']
    for key in keys:
        if key not in st.session_state:
            st.session_state[key] = None if 'bytes' in key else [] if 'elements' in key else {} if 'map' in key or 'content' in key else ""

def parse_document(doc_bytes):
    """
    (RE-ARCHITECTED) Iterates through all document blocks (paragraphs and tables).
    It identifies headings to establish the current section context, and then
    presents all subsequent paragraphs (including empty ones) and table cells as taggable items.
    """
    doc = docx.Document(io.BytesIO(doc_bytes))
    elements_for_ui = []
    action_map = {}
    current_heading = "Document Header (No Section Found Yet)"

    for i, block in enumerate(iter_block_items(doc)):
        block_id = f"block_{i}"

        if isinstance(block, Paragraph):
            p = block
            text = p.text.strip()
            # A heading is defined by style or by a specific text convention.
            is_heading = p.style.name.startswith('Heading') or text.lstrip(' "*‚Ä¢').startswith('Section:')

            if is_heading:
                current_heading = text if text else "Unnamed Heading"
                continue  # Skip adding headings to the UI, they just provide context.

            # Any paragraph that IS NOT a heading is a taggable placeholder.
            # This includes empty paragraphs which are crucial for defining insertion points.
            ui_content = text if text else "[Empty paragraph available for tagging]"
            elements_for_ui.append({
                "id": block_id,
                "content": ui_content,
                "heading": current_heading,
                "type": "Paragraph"
            })
            action_map[block_id] = "[IGNORE]"

        elif isinstance(block, Table):
            for r_idx, row in enumerate(block.rows):
                for c_idx, cell in enumerate(row.cells):
                    cell_id = f"{block_id}_r{r_idx}_c{c_idx}"
                    content = cell.text.strip()
                    # Show any cell that has content or is an empty response cell in a Q&A table.
                    if content or (c_idx > 0 and row.cells[c_idx-1].text.strip()):
                        ui_content = content if content else f"(Response for: '{row.cells[c_idx-1].text.strip()[:40]}...')"
                        elements_for_ui.append({
                            "id": cell_id,
                            "content": ui_content,
                            "heading": f"{current_heading} - Table Row {r_idx+1}",
                            "type": "Table Cell"
                        })
                        action_map[cell_id] = "[IGNORE]"

    return elements_for_ui, action_map

def process_template():
    """
    (CORRECTED) Assembles the final template by iterating through the document
    and replacing content based on the action map. This logic mirrors `parse_document`.
    """
    if not st.session_state.template_doc_bytes:
        st.error("No template loaded.")
        return

    doc = docx.Document(io.BytesIO(st.session_state.template_doc_bytes))

    for i, block in enumerate(iter_block_items(doc)):
        block_id = f"block_{i}"

        if isinstance(block, Paragraph):
            p = block
            text = p.text.strip()
            is_heading = p.style.name.startswith('Heading') or text.lstrip(' "*‚Ä¢').startswith('Section:')

            if is_heading:
                continue # Never modify headings

            # This paragraph is a taggable placeholder. Check if an action applies.
            action = st.session_state.action_map.get(block_id)
            if action and action != "[IGNORE]":
                p.clear()  # Clear all runs (content and formatting)
                if action == "[INSERT_FROM_FILE]":
                    content_to_insert = st.session_state.inserted_content.get(block_id, f"[{action} - ERROR: NO FILE UPLOADED]")
                    p.add_run(content_to_insert)
                else:
                    p.add_run(action)

        elif isinstance(block, Table):
            for r_idx, row in enumerate(block.rows):
                for c_idx, cell in enumerate(row.cells):
                    cell_id = f"{block_id}_r{r_idx}_c{c_idx}"
                    action = st.session_state.action_map.get(cell_id)
                    if action and action != "[IGNORE]":
                        cell.text = ""  # Clear cell content
                        if action == "[INSERT_FROM_FILE]":
                            content_to_insert = st.session_state.inserted_content.get(cell_id, f"[{action} - ERROR: NO FILE UPLOADED]")
                            cell.text = content_to_insert
                        else:
                            cell.text = action
    bio = io.BytesIO()
    doc.save(bio)
    st.session_state.processed_doc_bytes = bio.getvalue()
    st.success("‚úÖ Template processed successfully!")


# --- UI Callbacks ---

def handle_upload():
    uploaded_file = st.session_state.get('template_uploader')
    if uploaded_file:
        initialize_state()
        st.session_state.template_doc_bytes = uploaded_file.getvalue()
        st.session_state.template_filename = uploaded_file.name
        with st.spinner("Parsing document to identify all taggable content..."):
            elements, actions = parse_document(st.session_state.template_doc_bytes)
        st.session_state.template_elements = elements
        st.session_state.action_map = actions

def handle_file_insertion(element_id):
    """Reads content from uploaded file and stores it in session state."""
    uploader_key = f"uploader_{element_id}"
    uploaded_file = st.session_state.get(uploader_key)
    if uploaded_file:
        try:
            if uploaded_file.name.endswith('.txt'):
                content = uploaded_file.getvalue().decode('utf-8')
            else:  # .docx
                doc = docx.Document(io.BytesIO(uploaded_file.getvalue()))
                content = "\n".join([p.text for p in doc.paragraphs])
            st.session_state.inserted_content[element_id] = content
        except Exception as e:
            st.error(f"Failed to read file: {e}")

def save_progress():
    if not st.session_state.template_filename:
        st.error("Cannot save without an uploaded file.")
        return
    save_data = {
        "source_template_filename": st.session_state.template_filename,
        "action_map": st.session_state.action_map,
        "inserted_content": st.session_state.inserted_content,
    }
    save_path = MAPS_DIR / f"{Path(st.session_state.template_filename).stem}_mapping.json"
    try:
        with open(save_path, 'w', encoding='utf-8') as f: json.dump(save_data, f, indent=4)
        st.success(f"‚úÖ Progress saved to `{save_path}`")
    except Exception as e: st.error(f"Failed to save progress: {e}")

def load_progress(map_file_path):
    try:
        with open(map_file_path, 'r', encoding='utf-8') as f: load_data = json.load(f)
        if load_data.get("source_template_filename") != st.session_state.template_filename:
            st.warning("Warning: This mapping file appears to be for a different source document.")
        st.session_state.action_map = load_data.get("action_map", {})
        st.session_state.inserted_content = load_data.get("inserted_content", {})
        st.success(f"‚úÖ Progress loaded from `{Path(map_file_path).name}`!")
    except Exception as e: st.error(f"Failed to load progress file: {e}")

# --- Streamlit UI Layout ---
st.title("üìù 6. Template Editor")
st.caption("Version 27.0.0 - Corrected Placeholder Parsing")

initialize_state()

with st.expander("Start Here: Upload Your Template", expanded=True):
    st.info("Upload any `.docx` file. The editor will show all content, allowing you to replace any part with a Cortex instruction or insert boilerplate from a file.")
    st.file_uploader("Upload document", type="docx", key="template_uploader", on_change=handle_upload)
    st.header("Load Saved Progress")
    saved_maps = [f for f in os.listdir(MAPS_DIR) if f.endswith('_mapping.json')]
    if saved_maps:
        selected_map_file = st.selectbox("Select progress file", options=[""] + saved_maps, index=0, help="To load progress, you must first upload the matching template file.")
        if selected_map_file and st.session_state.template_doc_bytes:
            if st.button("Load Selected Progress"): load_progress(MAPS_DIR / selected_map_file)

if st.session_state.template_elements:
    st.divider()
    st.header("Assign Cortex Actions to Document Content")
    st.markdown("For any content you want to keep as-is, leave the action as **'Ignore this block'**. To insert an instruction, select an action from the dropdown to **replace** the original text.")
    st.button("üíæ Save Progress", on_click=save_progress)

    for element in st.session_state.template_elements:
        elem_id = element['id']
        heading = element['heading']
        content = element['content']

        st.markdown("---")
        st.markdown(f"**Found {element['type']} under Section:** `{heading}`")
        st.info(f"**Original Text:**\n\n```\n{content}\n```")

        action_keys = list(CORTEX_ACTIONS.keys())
        current_action_tag = st.session_state.action_map.get(elem_id, "[IGNORE]")

        try:
            current_text_key = next(k for k, v in CORTEX_ACTIONS.items() if v == current_action_tag)
            default_index = action_keys.index(current_text_key)
        except (StopIteration, ValueError): default_index = 0

        selected_text = st.selectbox("Action for this block:", options=action_keys, index=default_index, key=f"action_{elem_id}")
        st.session_state.action_map[elem_id] = CORTEX_ACTIONS[selected_text]

        if st.session_state.action_map[elem_id] == "[INSERT_FROM_FILE]":
            st.file_uploader(
                "Upload boilerplate (.txt or .docx)",
                type=['txt', 'docx'],
                key=f"uploader_{elem_id}",
                on_change=handle_file_insertion,
                args=(elem_id,)
            )
            if elem_id in st.session_state.inserted_content:
                st.success("Boilerplate file loaded and ready for insertion.")

    st.divider()
    st.header("Generate Final Tagged Template")
    st.info("This creates a new `.docx` file where any content you tagged is replaced with the chosen Cortex instruction or boilerplate text.")
    st.button("‚öôÔ∏è Process and Generate Tagged Template", on_click=process_template, type="primary", use_container_width=True)

    if st.session_state.processed_doc_bytes:
        new_filename = f"CORTEX_TPL_{st.session_state.template_filename}"
        st.download_button(label=f"‚¨áÔ∏è Download '{new_filename}'", data=st.session_state.processed_doc_bytes, file_name=new_filename)
else:
    st.markdown("---")
    st.warning("Upload a document to begin.")


--- FILE: ./pages/6_Proposal_Step_2_Make.py ---
# ## File: pages/6_Proposal_Step_2_Make.py
# Version: 2.2.0 (Add Delete Confirmation)
# Date: 2025-07-15
# Purpose: A central hub for creating, loading, and managing proposals.
#          - FEATURE (v2.2.0): Added a confirmation step before deleting a
#            proposal to prevent accidental data loss.

import streamlit as st
from pathlib import Path
import sys
import pandas as pd

# Add the project root to the Python path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from cortex_engine.proposal_manager import ProposalManager

st.set_page_config(layout="wide", page_title="Cortex Proposal Management")

# Initialize the manager
prop_mgr = ProposalManager()

# --- Initialize Session State for this page ---
if 'confirming_delete_proposal_id' not in st.session_state:
    st.session_state.confirming_delete_proposal_id = None

st.title("üóÇÔ∏è 6. Proposal Step 2 Make")
st.caption("Create a new proposal or load an existing one to continue working.")

# --- Section: Create New Proposal ---
with st.expander("üöÄ Create a New Proposal", expanded=True):
    with st.form("new_proposal_form"):
        new_proposal_name = st.text_input("Proposal Name")
        submitted = st.form_submit_button("Create and Start")

        if submitted:
            if new_proposal_name:
                with st.spinner("Creating new proposal..."):
                    proposal_id = prop_mgr.create_proposal(new_proposal_name)

                    # Set the current proposal ID in session state for other pages
                    st.session_state['current_proposal_id'] = proposal_id

                    # Clear any potentially lingering state from other proposals
                    keys_to_clear = ['parsed_instructions', 'doc_template', 'section_content', 'generated_doc_bytes', 'original_filename', 'last_uploaded_file']
                    for key in keys_to_clear:
                        if key in st.session_state:
                            del st.session_state[key]

                    # Switch to the co-pilot page
                    st.switch_page("pages/Proposal_Copilot.py")
            else:
                st.error("Please enter a name for the proposal.")

st.divider()

# --- Section: Load Existing Proposals ---
st.header("üìÇ Existing Proposals")

proposals_list = prop_mgr.list_proposals()

if not proposals_list:
    st.info("No existing proposals found. Create one above to get started.")
else:
    # Use st.columns to create a header
    c1, c2, c3, c4, c5 = st.columns([3, 2, 2, 1, 1])
    c1.write("**Proposal Name**")
    c2.write("**Last Modified**")
    c3.write("**Status**")

    st.markdown("---")

    for p in proposals_list:
        with st.container(border=True): # Use a container to group each proposal row and its confirmation
            col1, col2, col3, col4, col5 = st.columns([3, 2, 2, 1, 1])
            with col1:
                st.markdown(f"**{p['name']}**")
                st.caption(f"ID: {p['id']}")
            with col2:
                st.write(p['last_modified'].strftime('%Y-%m-%d %H:%M'))
            with col3:
                # Use a selectbox to allow status changes
                new_status = st.selectbox(
                    "Status",
                    ["Drafting", "Review", "Approved", "Archived"],
                    index=["Drafting", "Review", "Approved", "Archived"].index(p['status']),
                    key=f"status_{p['id']}",
                    label_visibility="collapsed"
                )
                if new_status != p['status']:
                    prop_mgr.update_proposal_status(p['id'], new_status)
                    st.toast(f"Updated '{p['name']}' status to {new_status}")
                    st.rerun()
            with col4:
                if st.button("Load", key=f"load_{p['id']}", use_container_width=True):
                     with st.spinner(f"Loading '{p['name']}'..."):
                        st.session_state['current_proposal_id'] = p['id']
                        st.switch_page("pages/Proposal_Copilot.py")
            with col5:
                if st.button("‚ùå", key=f"del_{p['id']}", use_container_width=True):
                    # Set the proposal to be confirmed for deletion
                    st.session_state.confirming_delete_proposal_id = p['id']
                    st.rerun()

            # --- Deletion Confirmation UI ---
            if st.session_state.confirming_delete_proposal_id == p['id']:
                st.warning(f"**Are you sure you want to permanently delete the proposal '{p['name']}'?** This action cannot be undone.")
                confirm_cols = st.columns(2)
                with confirm_cols[0]:
                    if st.button("YES, DELETE PERMANENTLY", key=f"confirm_del_{p['id']}", use_container_width=True, type="primary"):
                        prop_mgr.delete_proposal(p['id'])
                        st.session_state.confirming_delete_proposal_id = None
                        st.rerun()
                with confirm_cols[1]:
                    if st.button("Cancel", key=f"cancel_del_{p['id']}", use_container_width=True):
                        st.session_state.confirming_delete_proposal_id = None
                        st.rerun()


--- FILE: ./pages/Proposal_Copilot.py ---
# ## File: Proposal_Copilot.py
# Version: 27.0.0 (Save/Load State Fix)
# Date: 2025-07-15
# Purpose: Core UI for drafting proposals.
#          - CRITICAL FIX (v27.0.0): Fixed a fatal bug where non-serializable
#            parser instructions were being saved to JSON, causing the app to
#            break on reload. Instructions are now always regenerated from the
#            template file upon loading a proposal.

import streamlit as st
import os
import io
import docx
import chromadb
import sys
import re
from pathlib import Path

# Add the project root to the Python path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from llama_index.core import Settings, VectorStoreIndex
from llama_index.vector_stores.chroma import ChromaVectorStore
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.ollama import Ollama
from chromadb.config import Settings as ChromaSettings

from cortex_engine.config import EMBED_MODEL, LLM_MODEL, COLLECTION_NAME
from cortex_engine.instruction_parser import CortexInstruction, parse_template_for_instructions
from cortex_engine.task_engine import TaskExecutionEngine
from cortex_engine.collection_manager import WorkingCollectionManager
from cortex_engine.proposal_manager import ProposalManager
from cortex_engine.config_manager import ConfigManager

# --- CONFIGURATION ---
DEFAULT_DB_PATH = "/mnt/f/ai_databases"
RESPONDING_ORGS = ["Deakin", "Escient", "Longboardfella", "Consortium"]
GENERATIVE_TASKS = ["GENERATE_FROM_KB", "GENERATE_RESOURCES", "GENERATE_FROM_KB_AND_PROPOSAL", "GENERATE_FROM_PROPOSAL_ONLY"]
RETRIEVAL_TASKS = ["RETRIEVE_FROM_KB"]
REFINEMENT_ONLY_TASKS = ["PROMPT_HUMAN"]

# --- Helper Functions ---
def convert_windows_to_wsl_path(win_path):
    """Converts a Windows path (e.g., F:\...) to a WSL path (e.g., /mnt/f/...)."""
    if not isinstance(win_path, str) or win_path.startswith('/mnt/'):
        return win_path
    path_str = str(win_path).replace('\\', '/')
    match = re.match(r"^([a-zA-Z]):/(.*)", path_str)
    if match:
        drive, rest = match.groups()
        return f"/mnt/{drive.lower()}/{rest}"
    return win_path

# --- Core Functions & State Management ---

def initialize_session_state():
    """Initializes session state with default values if keys are missing."""
    defaults = {
        "responding_org": None,
        "knowledge_sources": ["Main Cortex Knowledge Base"],
        "parsed_instructions": [],
        "section_content": {},
        "doc_template": None,
        "generated_doc_bytes": None,
        "original_filename": "",
        "loaded_proposal_id": None
    }
    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value

@st.cache_resource
def load_system(_db_path):
    with st.spinner(f"Connecting to Cortex KB at '{_db_path}'..."):
        chroma_db_path = os.path.join(_db_path, "knowledge_hub_db")
        if not os.path.isdir(chroma_db_path): # Use isdir for better checking
            st.error(f"ChromaDB path not found. Please run an ingestion process. Path checked: '{chroma_db_path}'"); st.stop()
        try:
            Settings.llm = Ollama(model=LLM_MODEL, request_timeout=300.0)
            Settings.embed_model = HuggingFaceEmbedding(model_name=EMBED_MODEL, device="cuda")
            db_settings = ChromaSettings(anonymized_telemetry=False)
            db = chromadb.PersistentClient(path=chroma_db_path, settings=db_settings)
            chroma_collection = db.get_or_create_collection(COLLECTION_NAME)
            index = VectorStoreIndex.from_vector_store(ChromaVectorStore(chroma_collection=chroma_collection))
            collection_manager = WorkingCollectionManager()
        except Exception as e: st.error(f"Failed to initialize system: {e}"); st.stop()
    return index, collection_manager

def load_proposal_into_session(proposal_id, collection_manager):
    """Loads a specific proposal's data into the session state."""
    prop_mgr = ProposalManager()
    data = prop_mgr.load_proposal(proposal_id)
    if data:
        session_state_data = data['state'].get('session_state', {})
        valid_collections = collection_manager.get_collection_names()
        saved_sources = session_state_data.get('knowledge_sources', ["Main Cortex Knowledge Base"])
        healed_sources = [src for src in saved_sources if src in valid_collections or src == "Main Cortex Knowledge Base"]
        st.session_state.knowledge_sources = healed_sources if healed_sources else ["Main Cortex Knowledge Base"]

        # Load all other JSON-safe data into the session
        st.session_state.responding_org = session_state_data.get('responding_org')
        st.session_state.section_content = session_state_data.get('section_content', {})
        st.session_state.original_filename = session_state_data.get('original_filename', "")

        # Handle template bytes and ALWAYS re-parse instructions
        if 'template_bytes' in data and data['template_bytes']:
            st.session_state.doc_template = docx.Document(io.BytesIO(data['template_bytes']))
            st.session_state.parsed_instructions = parse_template_for_instructions(st.session_state.doc_template)
        else:
            # If no template is loaded, ensure instructions and template object are cleared
            st.session_state.doc_template = None
            st.session_state.parsed_instructions = []

        if 'generated_doc_bytes' in data:
            st.session_state.generated_doc_bytes = data['generated_doc_bytes']

        st.session_state['proposal_name'] = data['state'].get('name', 'Untitled')
        st.toast(f"Loaded proposal: {st.session_state['proposal_name']}")
    else:
        st.error("Failed to load proposal."); st.switch_page("pages/6_Proposal_Step_2_Make.py")

# --- Action Handlers ---

def handle_ai_action(engine: TaskExecutionEngine, instruction: CortexInstruction, content_key: str):
    creativity = st.session_state.section_content.get(content_key, {}).get('creativity', 'green')
    raw_text = st.session_state.section_content.get(content_key, {}).get('text', '')
    has_content = raw_text.strip()
    generated_text = ""

    if has_content and (instruction.task_type in REFINEMENT_ONLY_TASKS or instruction.task_type in GENERATIVE_TASKS):
        with st.spinner("AI is refining..."):
            generated_text = engine.refine_with_ai(
                section_heading=instruction.section_heading,
                raw_text=raw_text,
                creativity=creativity,
                sub_instruction=instruction.sub_instruction
            )
    elif not has_content and instruction.task_type in GENERATIVE_TASKS:
        with st.spinner("AI is generating a new draft..."):
            generated_text = engine.generate_from_kb(
                instruction=instruction,
                creativity=creativity,
                knowledge_sources=st.session_state.knowledge_sources,
                session_state=st.session_state
            )
    elif instruction.task_type in RETRIEVAL_TASKS:
        with st.spinner("Finding relevant case studies..."):
            generated_text = engine.retrieve_from_kb(
                instruction=instruction,
                knowledge_sources=st.session_state.knowledge_sources,
                session_state=st.session_state
            )
            creativity = 'green'
    else:
        st.warning("Please provide some text to refine, or use a generative task."); return

    st.session_state.section_content[content_key] = {'text': generated_text, 'creativity': creativity}
    st.rerun()

# --- Main Application ---
prop_mgr = ProposalManager()

if 'current_proposal_id' not in st.session_state:
    st.warning("No proposal selected. Please go to the Proposal Manager."); st.stop()

initialize_session_state()

config_mgr = ConfigManager()
raw_db_path = config_mgr.get_config().get("ai_database_path", DEFAULT_DB_PATH)
wsl_db_path = convert_windows_to_wsl_path(raw_db_path)
index, collection_mgr = load_system(wsl_db_path)

if st.session_state.loaded_proposal_id != st.session_state.current_proposal_id:
    with st.spinner("Loading proposal state..."):
        load_proposal_into_session(st.session_state.current_proposal_id, collection_mgr)
        st.session_state.loaded_proposal_id = st.session_state.current_proposal_id

st.title(f"ü§ñ Co-pilot: {st.session_state.get('proposal_name', 'Untitled Proposal')}")

with st.sidebar:
    if st.button("üíæ Save Progress & Draft", type="primary", use_container_width=True):
         with st.spinner("Saving..."):
            # CRITICAL FIX: Do NOT save 'parsed_instructions' to the JSON state file.
            keys_to_save = ['responding_org', 'knowledge_sources', 'section_content', 'original_filename']
            session_data_to_save = {k: st.session_state.get(k) for k in keys_to_save}
            template_bytes = None
            if st.session_state.get('doc_template'):
                bio = io.BytesIO(); st.session_state.doc_template.save(bio); template_bytes = bio.getvalue()
            prop_mgr.save_proposal(st.session_state.current_proposal_id, session_data_to_save, template_bytes, st.session_state.get('generated_doc_bytes'))
            st.toast("‚úÖ Progress Saved!")

    if st.button("‚¨ÖÔ∏è Back to Proposal Management"): st.switch_page("pages/6_Proposal_Step_2_Make.py")
    st.divider()
    st.header("1. Proposal Context")
    st.selectbox("Responding Organisation", options=RESPONDING_ORGS, key="responding_org")
    st.multiselect("Knowledge Source(s)", options=["Main Cortex Knowledge Base"] + collection_mgr.get_collection_names(), key="knowledge_sources")

if not st.session_state.get('responding_org'):
    st.info("‚¨ÖÔ∏è Please select a Responding Organisation in the sidebar to begin."); st.stop()

if not st.session_state.get('doc_template'):
    st.header("2. Upload Proposal Template")
    uploaded_file = st.file_uploader("Upload your .docx template with Cortex instructions.", type="docx")
    if uploaded_file:
        st.session_state.original_filename = uploaded_file.name
        doc_stream = io.BytesIO(uploaded_file.getvalue()); st.session_state.doc_template = docx.Document(doc_stream)
        with st.spinner("Parsing template for instructions..."):
            st.session_state.parsed_instructions = parse_template_for_instructions(st.session_state.doc_template)
        st.rerun()
else:
    st.success(f"Template '{st.session_state.get('original_filename', 'template.docx')}' is loaded.")

if st.session_state.get('parsed_instructions'):
    st.divider()
    st.header("3. Co-pilot Drafting")
    engine = TaskExecutionEngine(main_index=index, collection_manager=collection_mgr)

    for i, inst in enumerate(st.session_state.parsed_instructions):
        content_key = f"content_inst_{i}"
        if content_key not in st.session_state.section_content:
             st.session_state.section_content[content_key] = {'text': '', 'creativity': 'green'}

        expander_title = f"Section: {inst.section_heading}  (Instruction: `{inst.instruction_raw}`)"
        with st.expander(expander_title, expanded=True):
            current_text_value = st.session_state.section_content[content_key].get('text', '')
            current_text = st.text_area(
                "Content Draft",
                value=current_text_value,
                key=f"text_area_{content_key}",
                height=150
            )
            if current_text != current_text_value:
                st.session_state.section_content[content_key]['text'] = current_text
                st.rerun()

            has_content = current_text.strip()
            button_label, is_actionable = "...", False
            if inst.task_type in GENERATIVE_TASKS:
                button_label, is_actionable = ("‚úçÔ∏è Refine" if has_content else "ü§ñ Generate"), True
            elif inst.task_type in REFINEMENT_ONLY_TASKS:
                button_label, is_actionable = "‚úçÔ∏è Refine", bool(has_content)
            elif inst.task_type in RETRIEVAL_TASKS:
                button_label, is_actionable = "üîé Find", True

            show_creativity_controls = inst.task_type in GENERATIVE_TASKS or inst.task_type in REFINEMENT_ONLY_TASKS

            if show_creativity_controls:
                control_cols = st.columns([3, 1])
                with control_cols[0]:
                    creativity_map = {"Factual": "green", "Persuasive": "orange", "Visionary": "red"}
                    color_to_label = {v: k for k, v in creativity_map.items()}
                    current_color = st.session_state.section_content[content_key].get('creativity', 'green')
                    current_label = color_to_label.get(current_color, "Factual")
                    options = list(creativity_map.keys())
                    try: current_index = options.index(current_label)
                    except ValueError: current_index = 0
                    selected_creativity_label = st.radio(
                        "AI Creativity:", options=options, index=current_index,
                        key=f"creative_radio_{content_key}", horizontal=True,
                    )
                    st.session_state.section_content[content_key]['creativity'] = creativity_map[selected_creativity_label]
                with control_cols[1]:
                    st.markdown("<div> </div>", unsafe_allow_html=True)
                    st.button(
                        button_label, key=f"btn_action_{i}", on_click=handle_ai_action,
                        args=(engine, inst, content_key), use_container_width=True,
                        type="primary", disabled=not is_actionable
                    )
            elif is_actionable:
                _, btn_col = st.columns([4, 1])
                with btn_col:
                    st.button(
                        button_label, key=f"btn_action_{i}", on_click=handle_ai_action,
                        args=(engine, inst, content_key), use_container_width=True, type="primary"
                    )

    st.divider()
    st.header("4. Assemble & Download")
    if st.button("üöÄ Assemble Proposal", type="primary", use_container_width=True):
        if not st.session_state.get('doc_template'):
            st.error("Template not found.")
        else:
            with st.spinner("Assembling final document with color-coding..."):
                template_bio = io.BytesIO(); st.session_state.doc_template.save(template_bio)
                final_doc_bytes = engine.assemble_document(
                    st.session_state.parsed_instructions,
                    st.session_state.section_content,
                    template_bio.getvalue()
                )
                st.session_state.generated_doc_bytes = final_doc_bytes
                st.success("‚úÖ Proposal Assembled!"); st.balloons()

    if st.session_state.generated_doc_bytes:
        dl_filename = f"DRAFT_{st.session_state.get('proposal_name', 'proposal').replace(' ', '_')}.docx"
        st.download_button("üì• Download Generated Proposal", st.session_state.generated_doc_bytes, file_name=dl_filename)


--- FILE: ./scripts/__init__.py ---



--- FILE: ./scripts/cortex_inspector.py ---
# ## File: cortex_inspector.py
# Version: 3.2.0 (ChromaDB Settings Fix)
# Date: 2025-07-13
# Purpose: A diagnostic tool to inspect the contents of the Cortex knowledge base.
#          - CRITICAL FIX (v3.2.0): Standardized the ChromaDB client initialization
#            with explicit settings to prevent conflicts with other modules.
#          - FEATURE: Added `--inspect-doc` to view rich metadata for a single file.
#          - FIX: Added sys.path modification to allow the script to find cortex_engine.

import pickle
import argparse
import networkx as nx
import chromadb
from chromadb.config import Settings as ChromaSettings # <-- FIX: Import settings
from pathlib import Path
import os
import re
import sys
import json

# Add the project root to the Python path
# This is necessary for the script to find the 'cortex_engine' module
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from cortex_engine import config

def convert_windows_to_wsl_path(win_path):
    if not isinstance(win_path, str) or win_path.startswith('/mnt/'): return win_path
    path=win_path.replace('\\','/'); match=re.match(r"^([a-zA-Z]):/(.*)", path)
    if match: drive,rest=match.groups(); return f"/mnt/{drive.lower()}/{rest}"
    return path

def print_header(title: str):
    print("\n" + "="*80); print(f" {title.upper()} ".center(80, "=")); print("="*80)

def inspect_document(graph: nx.DiGraph, collection: chromadb.Collection, doc_path: str):
    posix_path = Path(doc_path).as_posix()
    print_header(f"Inspecting Document: {posix_path}")

    if not graph.has_node(posix_path):
        print(f"\n‚ùå ERROR: Document not found in the Knowledge Graph.")
        print(f"   Please ensure the path is correct and was included in the last ingestion.")
        return

    # --- Print Graph Metadata ---
    graph_metadata = graph.nodes[posix_path]
    print("\n--- üîé Metadata from Knowledge Graph Node ---")
    for key, value in sorted(graph_metadata.items()):
        if key == 'thematic_tags' and isinstance(value, list):
            print(f"  - {key}:")
            for tag in value:
                print(f"    - {tag}")
        else:
            print(f"  - {key}: {value}")

    # --- Print Vector Store Metadata ---
    print("\n--- üîé Metadata from a Vector Store Chunk ---")
    try:
        # Fetch one chunk from the vector store associated with this file
        vector_chunks = collection.get(where={"file_path": posix_path}, limit=1, include=["metadatas"])
        if not vector_chunks or not vector_chunks['ids']:
            print("\n-> No associated chunks found in the vector store for this file.")
        else:
            vector_metadata = vector_chunks['metadatas'][0]
            print("(Showing one example chunk to verify metadata was propagated correctly)")
            for key, value in sorted(vector_metadata.items()):
                 if key == 'thematic_tags' and isinstance(value, list):
                    print(f"  - {key}:")
                    for tag in value:
                        print(f"    - {tag}")
                 else:
                    print(f"  - {key}: {value}")
    except Exception as e:
        print(f"\n‚ùå ERROR: Could not query the vector store. Reason: {e}")


def inspect_stats(graph: nx.DiGraph, collection: chromadb.Collection, db_path: str):
    graph_file_path = os.path.join(db_path, "knowledge_cortex.gpickle")
    image_store_path = os.path.join(db_path, "knowledge_hub_db", "images")
    print_header("Knowledge Base Health Check")
    print("\n--- Knowledge Graph ---")
    print(f"Graph loaded from: {graph_file_path}"); print(f"Total Nodes: {graph.number_of_nodes()}"); print(f"Total Edges: {graph.number_of_edges()}")
    node_types = {}
    for _, data in graph.nodes(data=True): ntype = data.get('node_type', 'Unknown'); node_types[ntype] = node_types.get(ntype, 0) + 1
    print("Node Distribution:"); [print(f"  - {ntype}: {count}") for ntype, count in sorted(node_types.items())]
    print("\n--- Vector Store ---"); print(f"ChromaDB Collection '{collection.name}'"); print(f"Total Embeddings: {collection.count()}")
    print("\n--- Image Store ---")
    if os.path.exists(image_store_path):
        image_count = len([name for name in os.listdir(image_store_path) if os.path.isfile(os.path.join(image_store_path, name))])
        print(f"Image store path: {image_store_path}"); print(f"Total Images Found: {image_count}")
    else: print(f"Image store path not found at: {image_store_path}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="A diagnostic tool for Project Cortex's knowledge base.")
    parser.add_argument("--db-path", required=True, type=str, help="The root directory where the database is stored.")
    parser.add_argument("--stats", action="store_true", help="Display overall stats of the graph and vector store.")
    parser.add_argument("--inspect-doc", type=str, metavar="PATH", help="Inspect all stored metadata for a specific document path.")

    args = parser.parse_args()
    wsl_db_path = convert_windows_to_wsl_path(args.db_path)

    graph_file_path = os.path.join(wsl_db_path, "knowledge_cortex.gpickle")
    chroma_db_path = os.path.join(wsl_db_path, "knowledge_hub_db")

    graph, collection = None, None

    try:
        with open(graph_file_path, "rb") as f: graph = pickle.load(f)
    except FileNotFoundError:
        graph = nx.DiGraph()
        if not args.inspect_doc:
             print(f"‚ùå ERROR: Graph file not found at '{graph_file_path}'. Please run an ingestion first."); sys.exit(1)

    # --- FIX: Use consistent ChromaSettings ---
    db_settings = ChromaSettings(anonymized_telemetry=False)
    chroma_client = chromadb.PersistentClient(path=chroma_db_path, settings=db_settings)
    try:
        collection = chroma_client.get_collection(name=config.COLLECTION_NAME)
    except Exception as e:
        print(f"‚ùå ERROR: Could not connect to ChromaDB collection. Have you run an ingestion? Reason: {e}"); sys.exit(1)

    if args.stats:
        inspect_stats(graph, collection, wsl_db_path)
    elif args.inspect_doc:
        wsl_doc_path = convert_windows_to_wsl_path(args.inspect_doc)
        inspect_document(graph, collection, wsl_doc_path)
    else:
        parser.print_help()


--- FILE: requirements.txt ---
# requirements.txt
#
# FINAL - Python 3.11 with GraphRAG Support
# Fixed numpy version conflict between spaCy and llama-index/chromadb
# Added entity extraction dependencies

# --- Core Application Framework ---
streamlit==1.36.0

# --- LlamaIndex Family (v0.10.x Stable Branch for Python 3.11) ---
llama-index==0.10.50
llama-index-core==0.10.50
llama-index-llms-ollama==0.1.3
llama-index-embeddings-huggingface==0.2.0
llama-index-vector-stores-chroma==0.1.7
llama-index-readers-file==0.1.22
llama-index-llms-openai==0.1.24
llama-index-embeddings-openai==0.1.11
llama-index-llms-gemini==0.1.10

# --- Machine Learning & AI Models ---
# CRITICAL: Pin numpy to <2.0.0 for compatibility
numpy<2.0.0,>=1.22.5
torch==2.3.1
torchvision==0.18.1
transformers==4.41.2
sentence-transformers==2.7.0
ollama==0.2.1
outlines==1.0.3
openai==1.35.3
google-generativeai

# --- Data Storage & Handling ---
chromadb==0.5.3
networkx==3.5
pydantic==2.7.4
pandas==2.2.2

# --- Document Parsing ---
PyMuPDF==1.26.1
python-docx==1.2.0
openpyxl==3.1.4
python-pptx==0.6.23
docx2txt==0.8

# --- Entity Extraction & NLP ---
# Pin spaCy to a version compatible with numpy<2.0.0
spacy>=3.5.0,<3.8.0
# Download the model separately with: python -m spacy download en_core_web_sm

# --- Utilities ---
tqdm==4.67.1
python-dotenv==1.0.1
requests==2.32.3

# --- AI Research Assistant Dependencies ---
beautifulsoup4==4.12.3
google-api-python-client==2.134.0
youtube-transcript-api==0.6.2
graphviz==0.20.3