# Adaptive Embedding Selection Guide

## Overview

Cortex Suite now features **fully adaptive embedding selection** that automatically chooses the best embedding model for ANY hardware configuration - from laptop GPUs to data center servers, Docker containers to bare metal.

**Zero configuration required** - it just works! âœ¨

## How It Works

The system automatically detects:
1. **GPU presence** (NVIDIA CUDA support)
2. **VRAM available** (2GB, 6GB, 8GB, 16GB, 40GB+ tiers)
3. **Dependencies installed** (qwen-vl-utils for multimodal support)
4. **Environment** (Docker, WSL, bare metal)

Based on this, it selects the optimal embedding approach:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           ADAPTIVE EMBEDDING DECISION TREE                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                    Hardware Detection
                            â”‚
                            â–¼
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚   NVIDIA GPU Found?   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           Yesâ”‚        â”‚No
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼                              â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Check VRAM & Deps   â”‚       â”‚   BGE-base       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚   (CPU-friendly) â”‚
                    â”‚                  â”‚   768D, text-onlyâ”‚
                    â–¼                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ qwen-vl-utils installed?  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
    Yesâ”‚             â”‚No
       â–¼              â–¼
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ VRAM?   â”‚    â”‚  NV-Embed-v2 â”‚
 â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜    â”‚ (GPU-optimized)â”‚
      â”‚         â”‚ 4096D, text-onlyâ”‚
      â–¼         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚  6GB+: Qwen3-VL-2B/8B       â”‚
 â”‚  (Multimodal: text+image+video)â”‚
 â”‚  2048D/4096D                â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Automatic Selection Matrix

| Hardware | Dependencies | Selected Approach | Model | Dimensions | Multimodal |
|----------|--------------|-------------------|-------|------------|------------|
| **No GPU** | - | BGE-base | BAAI/bge-base-en-v1.5 | 768 | âŒ |
| **NVIDIA 2-6GB** | - | NV-Embed-v2 | nvidia/NV-Embed-v2 | 4096 | âŒ |
| **NVIDIA 6-10GB** | qwen-vl-utils âœ… | Qwen3-VL-2B | Qwen/Qwen3-VL-Embedding-2B | 2048 | âœ… |
| **NVIDIA 10-16GB** | qwen-vl-utils âœ… | Qwen3-VL-2B | Qwen/Qwen3-VL-Embedding-2B | 2048 | âœ… |
| **NVIDIA 16-24GB** | qwen-vl-utils âœ… | Qwen3-VL-8B | Qwen/Qwen3-VL-Embedding-8B | 4096 | âœ… |
| **NVIDIA 24-40GB** | qwen-vl-utils âœ… | Qwen3-VL-8B | Qwen/Qwen3-VL-Embedding-8B | 4096 | âœ… |
| **NVIDIA 40GB+** | qwen-vl-utils âœ… | Qwen3-VL-8B | Qwen/Qwen3-VL-Embedding-8B | 4096 | âœ… |

## Real-World Examples

### Example 1: Your Laptop (RTX 4060 8GB)
```
Hardware: RTX 4060 Laptop GPU (8GB VRAM)
Dependencies: qwen-vl-utils installed âœ…
â†’ Auto-selects: Qwen3-VL-Embedding-2B
â†’ Multimodal: Yes (text + images + video)
â†’ Dimensions: 2048
```

### Example 2: Docker on Server (RTX 4090 24GB)
```
Hardware: RTX 4090 (24GB VRAM)
Dependencies: qwen-vl-utils in container âœ…
â†’ Auto-selects: Qwen3-VL-Embedding-8B
â†’ Multimodal: Yes
â†’ Dimensions: 4096
â†’ Reranker: Qwen3-VL-Reranker-2B
```

### Example 3: Cloud VM (No GPU)
```
Hardware: CPU only
Dependencies: N/A
â†’ Auto-selects: BGE-base
â†’ Multimodal: No (text-only)
â†’ Dimensions: 768
```

### Example 4: Older GPU (GTX 1660 6GB)
```
Hardware: GTX 1660 (6GB VRAM)
Dependencies: qwen-vl-utils not installed âŒ
â†’ Auto-selects: NV-Embed-v2
â†’ Multimodal: No (text-only)
â†’ Dimensions: 4096
```

## Manual Overrides

You can override the automatic selection if needed:

### Force Specific Model
```bash
# Use a specific model regardless of hardware
export CORTEX_EMBED_MODEL="nvidia/NV-Embed-v2"
streamlit run Cortex_Suite.py
```

### Force Qwen3-VL On
```bash
# Force Qwen3-VL even if auto-selection would choose something else
export QWEN3_VL_ENABLED=true
export QWEN3_VL_MODEL_SIZE=2B  # or 8B or auto
streamlit run Cortex_Suite.py
```

### Force Qwen3-VL Off
```bash
# Disable Qwen3-VL even if hardware supports it
export QWEN3_VL_ENABLED=false
streamlit run Cortex_Suite.py
```

## Docker Compatibility

The adaptive selection is **fully Docker-aware**:

### Auto-Detection in Docker
```yaml
# docker-compose.yml
services:
  cortex:
    image: cortex-suite:latest
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    # NO CONFIGURATION NEEDED!
    # System auto-detects GPU and selects optimal model
```

### Manual Configuration in Docker
```yaml
# docker-compose.yml
services:
  cortex:
    image: cortex-suite:latest
    environment:
      # Optional: Force specific configuration
      - QWEN3_VL_ENABLED=true
      - QWEN3_VL_MODEL_SIZE=auto
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
```

## Verification Commands

### Check What's Auto-Selected
```bash
python3 -c "
from cortex_engine.config import get_embedding_strategy
strategy = get_embedding_strategy()
print(f'Approach: {strategy[\"approach\"]}')
print(f'Model: {strategy[\"model\"]}')
print(f'Dimensions: {strategy[\"dimensions\"]}')
print(f'Multimodal: {strategy[\"multimodal\"]}')
print(f'Reason: {strategy[\"reason\"]}')
"
```

### Check GPU Detection
```bash
python3 -c "
from cortex_engine.utils.smart_model_selector import detect_nvidia_gpu
has_gpu, info = detect_nvidia_gpu()
print(f'GPU: {has_gpu}')
if has_gpu:
    print(f'Name: {info[\"device_name\"]}')
    print(f'VRAM: {info.get(\"memory_total_gb\", 0):.1f}GB')
"
```

## Migration from Manual Configuration

### Old Way (Manual)
```bash
# Required manual configuration
export QWEN3_VL_ENABLED=true
export QWEN3_VL_MODEL_SIZE=2B
export HF_HUB_OFFLINE=0
source .env.qwen3vl
streamlit run Cortex_Suite.py
```

### New Way (Automatic)
```bash
# Just start - it auto-detects everything!
streamlit run Cortex_Suite.py
```

**Your existing `.env.qwen3vl` file still works for manual override if needed!**

## Multi-Machine Flexibility

The adaptive approach makes it trivial to run on different machines:

### Same Codebase, Different Hardware

**Laptop (8GB):**
```bash
git clone https://github.com/youruser/cortex
cd cortex
streamlit run Cortex_Suite.py
# â†’ Auto-selects Qwen3-VL-2B
```

**Workstation (24GB):**
```bash
git clone https://github.com/youruser/cortex
cd cortex
streamlit run Cortex_Suite.py
# â†’ Auto-selects Qwen3-VL-8B
```

**Server (CPU only):**
```bash
git clone https://github.com/youruser/cortex
cd cortex
streamlit run Cortex_Suite.py
# â†’ Auto-selects BGE-base
```

**No configuration changes needed!**

## Performance Characteristics

| Approach | VRAM | Speed | Quality | Multimodal | Use Case |
|----------|------|-------|---------|------------|----------|
| **Qwen3-VL-8B** | 16GB | Fast | Excellent | âœ… | High-end workstations |
| **Qwen3-VL-2B** | 5GB | Fast | Very Good | âœ… | Mid-range GPUs |
| **NV-Embed-v2** | 1.2GB | Very Fast | Excellent | âŒ | Text-only, GPU-optimized |
| **BGE-base** | 0.5GB | Medium | Good | âŒ | CPU systems, fallback |

## Troubleshooting

### Issue: Wrong Model Selected

**Check what was selected:**
```bash
python3 -c "from cortex_engine.config import get_embedding_strategy; print(get_embedding_strategy())"
```

**Force a different selection:**
```bash
export CORTEX_EMBED_MODEL="your-preferred-model"
# or
export QWEN3_VL_ENABLED=true  # for multimodal
```

### Issue: Qwen3-VL Not Auto-Selected

**Likely causes:**
1. qwen-vl-utils not installed: `pip install qwen-vl-utils`
2. VRAM < 6GB (get NV-Embed-v2 instead)
3. Manual override set: `unset QWEN3_VL_ENABLED`

### Issue: GPU Not Detected

**Check PyTorch CUDA:**
```bash
python3 -c "import torch; print(f'CUDA: {torch.cuda.is_available()}')"
```

**If False, reinstall PyTorch with CUDA:**
```bash
pip install torch --index-url https://download.pytorch.org/whl/cu121
```

## Architecture Benefits

### âœ… Zero Configuration
- Works out of the box on any hardware
- No manual model selection needed
- No environment variable setup required

### âœ… Intelligent Fallbacks
- Gracefully handles missing dependencies
- Automatically downgrades if VRAM insufficient
- CPU fallback if no GPU available

### âœ… Docker-Friendly
- Auto-detects Docker environment
- Works with GPU pass-through
- No special Docker configuration needed

### âœ… Portable
- Same codebase works on laptop, workstation, server
- Adapts to available resources automatically
- Easy deployment across different hardware

### âœ… Override-Friendly
- Manual overrides still work for power users
- Environment variables respected
- Backward compatible with old configs

## Summary

The adaptive embedding selection makes Cortex Suite truly **hardware-agnostic**:

- **On your laptop**: Uses Qwen3-VL-2B for multimodal search
- **On a server**: Uses Qwen3-VL-8B for maximum quality
- **In Docker**: Auto-detects GPU and configures appropriately
- **On CPU-only**: Falls back to BGE-base gracefully

**Just clone and run - it works everywhere!** ğŸš€

---

**Next Steps:**
1. Remove any manual configuration (optional, still works)
2. Just start Cortex: `streamlit run Cortex_Suite.py`
3. Check what was selected: See verification commands above
4. Enjoy optimal performance on any hardware!
